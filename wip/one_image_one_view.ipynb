{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One pass\n",
    "%matplotlib inline\n",
    "import os\n",
    "from typing import Optional, Tuple, List, Union, Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from tqdm import trange\n",
    "import random\n",
    "\n",
    "# For repeatability\n",
    "seed = 2024\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Load data\n",
    "data = np.load('tiny_nerf_data.npz')\n",
    "\n",
    "testimg_idx = 99\n",
    "n_training = 100\n",
    "\n",
    "# Gather as torch tensors\n",
    "images = torch.from_numpy(data['images'][:n_training])#.to(device)\n",
    "poses = torch.from_numpy(data['poses'])#.to(device)\n",
    "focal = torch.from_numpy(data['focal'])#.to(device)\n",
    "testimg = torch.from_numpy(data['images'][testimg_idx])#.to(device)\n",
    "testpose = torch.from_numpy(data['poses'][testimg_idx])#.to(device)\n",
    "\n",
    "testimg, testpose = images[testimg_idx], poses[testimg_idx]\n",
    "height, width = images.shape[1:3]\n",
    "near, far = 2., 6.\n",
    "\n",
    "def get_rays(\n",
    "\theight: int,\n",
    "\twidth: int,\n",
    "\tfocal_length: float,\n",
    "\tc2w: torch.Tensor\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tFind origin and direction of rays through every pixel and camera origin.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Apply pinhole camera model to gather directions at each pixel\n",
    "\ti, j = torch.meshgrid(\n",
    "\t\t\ttorch.arange(width, dtype=torch.float32).to(c2w),\n",
    "\t\t\ttorch.arange(height, dtype=torch.float32).to(c2w),\n",
    "\t\t\tindexing='ij')\n",
    "\ti, j = i.transpose(-1, -2), j.transpose(-1, -2)\n",
    "\tdirections = torch.stack([(i - width * .5) / focal_length,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t-(j - height * .5) / focal_length,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t-torch.ones_like(i)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t ], dim=-1)\n",
    "\n",
    "\t# Apply camera pose to directions\n",
    "\trays_d = torch.sum(directions[..., None, :] * c2w[:3, :3], dim=-1)\n",
    "\n",
    "\t# Origin is same for all directions (the optical center)\n",
    "\trays_o = c2w[:3, -1].expand(rays_d.shape)\n",
    "\treturn rays_o, rays_d\n",
    "\n",
    "\n",
    "def sample_stratified(\n",
    "\trays_o: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\tnear: float,\n",
    "\tfar: float,\n",
    "\tn_samples: int,\n",
    "\tperturb: Optional[bool] = True,\n",
    "\tinverse_depth: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tSample along ray from regularly-spaced bins.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Grab samples for space integration along ray\n",
    "\tt_vals = torch.linspace(0., 1., n_samples, device=rays_o.device)\n",
    "\tif not inverse_depth:\n",
    "\t\t# Sample linearly between `near` and `far`\n",
    "\t\tz_vals = near * (1.-t_vals) + far * (t_vals)\n",
    "\telse:\n",
    "\t\t# Sample linearly in inverse depth (disparity)\n",
    "\t\tz_vals = 1./(1./near * (1.-t_vals) + 1./far * (t_vals))\n",
    "\n",
    "\t# Draw uniform samples from bins along ray\n",
    "\tif perturb:\n",
    "\t\tmids = .5 * (z_vals[1:] + z_vals[:-1])\n",
    "\t\tupper = torch.concat([mids, z_vals[-1:]], dim=-1)\n",
    "\t\tlower = torch.concat([z_vals[:1], mids], dim=-1)\n",
    "\t\tt_rand = torch.rand([n_samples], device=z_vals.device)\n",
    "\t\tz_vals = lower + (upper - lower) * t_rand\n",
    "\tz_vals = z_vals.expand(list(rays_o.shape[:-1]) + [n_samples])\n",
    "\n",
    "\t# Apply scale from `rays_d` and offset from `rays_o` to samples\n",
    "\t# pts: (width, height, n_samples, 3)\n",
    "\tpts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals[..., :, None]\n",
    "\treturn pts, z_vals\n",
    "\n",
    "\n",
    "class NeRF(nn.Module):\n",
    "\tr\"\"\"\n",
    "\tNeural radiance fields module.\n",
    "\t\"\"\"\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\td_input: int = 3,\n",
    "\t\tn_layers: int = 8,\n",
    "\t\td_filter: int = 256,\n",
    "\t\tskip: Tuple[int] = (4,),\n",
    "\t\td_viewdirs: Optional[int] = None\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.d_input = d_input\n",
    "\t\tself.skip = skip\n",
    "\t\tself.act = nn.functional.relu\n",
    "\t\tself.d_viewdirs = d_viewdirs\n",
    "\n",
    "\t\t# Create model layers\n",
    "\t\tself.layers = nn.ModuleList(\n",
    "\t\t\t[nn.Linear(self.d_input, d_filter)] +\n",
    "\t\t\t[nn.Linear(d_filter + self.d_input, d_filter) if i in skip \\\n",
    "\t\t\t else nn.Linear(d_filter, d_filter) for i in range(n_layers - 1)]\n",
    "\t\t)\n",
    "\n",
    "\t\t# Bottleneck layers\n",
    "\t\tif self.d_viewdirs is not None:\n",
    "\t\t\t# If using viewdirs, split alpha and RGB\n",
    "\t\t\tself.alpha_out = nn.Linear(d_filter, 1)\n",
    "\t\t\tself.rgb_filters = nn.Linear(d_filter, d_filter)\n",
    "\t\t\tself.branch = nn.Linear(d_filter + self.d_viewdirs, d_filter // 2)\n",
    "\t\t\tself.output = nn.Linear(d_filter // 2, 3)\n",
    "\t\telse:\n",
    "\t\t\t# If no viewdirs, use simpler output\n",
    "\t\t\tself.output = nn.Linear(d_filter, 4)\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tx: torch.Tensor,\n",
    "\t\tviewdirs: Optional[torch.Tensor] = None\n",
    "\t) -> torch.Tensor:\n",
    "\t\tr\"\"\"\n",
    "\t\tForward pass with optional view direction.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# Cannot use viewdirs if instantiated with d_viewdirs = None\n",
    "\t\tif self.d_viewdirs is None and viewdirs is not None:\n",
    "\t\t\traise ValueError('Cannot input x_direction if d_viewdirs was not given.')\n",
    "\n",
    "\t\t# Apply forward pass up to bottleneck\n",
    "\t\tx_input = x\n",
    "\t\tfor i, layer in enumerate(self.layers):\n",
    "\t\t\tx = self.act(layer(x))\n",
    "\t\t\tif i in self.skip:\n",
    "\t\t\t\tx = torch.cat([x, x_input], dim=-1)\n",
    "\n",
    "\t\t# Apply bottleneck\n",
    "\t\tif self.d_viewdirs is not None:\n",
    "\t\t\t# Split alpha from network output\n",
    "\t\t\talpha = self.alpha_out(x)\n",
    "\n",
    "\t\t\t# Pass through bottleneck to get RGB\n",
    "\t\t\tx = self.rgb_filters(x)\n",
    "\t\t\tx = torch.concat([x, viewdirs], dim=-1)\n",
    "\t\t\tx = self.act(self.branch(x))\n",
    "\t\t\tx = self.output(x)\n",
    "\n",
    "\t\t\t# Concatenate alphas to output\n",
    "\t\t\tx = torch.concat([x, alpha], dim=-1)\n",
    "\t\telse:\n",
    "\t\t\t# Simple output\n",
    "\t\t\tx = self.output(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "\tr\"\"\"\n",
    "\tSine-cosine positional encoder for input points.\n",
    "\t\"\"\"\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\td_input: int,\n",
    "\t\tn_freqs: int,\n",
    "\t\tlog_space: bool = False\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.d_input = d_input\n",
    "\t\tself.n_freqs = n_freqs\n",
    "\t\tself.log_space = log_space\n",
    "\t\tself.d_output = d_input * (1 + 2 * self.n_freqs)\n",
    "\t\tself.embed_fns = [lambda x: x]\n",
    "\n",
    "\t\t# Define frequencies in either linear or log scale\n",
    "\t\tif self.log_space:\n",
    "\t\t\tfreq_bands = 2.**torch.linspace(0., self.n_freqs - 1, self.n_freqs)\n",
    "\t\telse:\n",
    "\t\t\tfreq_bands = torch.linspace(2.**0., 2.**(self.n_freqs - 1), self.n_freqs)\n",
    "\n",
    "\t\t# Alternate sin and cos\n",
    "\t\tfor freq in freq_bands:\n",
    "\t\t\tself.embed_fns.append(lambda x, freq=freq: torch.sin(x * freq))\n",
    "\t\t\tself.embed_fns.append(lambda x, freq=freq: torch.cos(x * freq))\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tx\n",
    "\t) -> torch.Tensor:\n",
    "\t\tr\"\"\"\n",
    "\t\tApply positional encoding to input.\n",
    "\t\t\"\"\"\n",
    "\t\treturn torch.concat([fn(x) for fn in self.embed_fns], dim=-1)\n",
    "\n",
    "\n",
    "class Grid(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network grid class. The input x needs to be within [0, 1].\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 feature_dim: int,\n",
    "                 grid_dim: int,\n",
    "                 num_lvl: int,\n",
    "                 max_res: int,\n",
    "                 min_res: int,\n",
    "                 hashtable_power: int,\n",
    "                 device='cpu'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Determine the device to use (CPU or CUDA)\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize the attributes of the grid\n",
    "        self.feature_dim = feature_dim  # Dimensionality of the feature vectors\n",
    "        self.grid_dim = grid_dim  # Dimensionality of the grid (e.g., 2D, 3D)\n",
    "        self.num_lvl = num_lvl  # Number of levels in the grid hierarchy\n",
    "        self.max_res = max_res  # Maximum resolution of the grid\n",
    "        self.min_res = min_res  # Minimum resolution of the grid\n",
    "        self.hashtable_power = hashtable_power  # Power of the hashtable size (number of entries is 2^hashtable_power)\n",
    "\n",
    "        self.d_output = num_lvl * grid_dim\n",
    "\n",
    "        # Constants for hash calculations\n",
    "        self.prime = [3367900313, 2654435761, 805459861]  # Prime numbers for hashing\n",
    "        self.max_entry = 2 ** self.hashtable_power  # Maximum number of entries in the hashtable\n",
    "        # Factor to scale resolutions logarithmically\n",
    "        self.factor_b = np.exp((np.log(self.max_res) - np.log(self.min_res)) / (self.num_lvl - 1))\n",
    "\n",
    "        # Compute the resolutions for each level\n",
    "        self.resolutions = []\n",
    "        for i in range(self.num_lvl):\n",
    "            # Calculate resolution for level i\n",
    "            self.resolutions.append(np.floor(self.min_res * (self.factor_b ** i)))\n",
    "\n",
    "        # Initialize the hashtable for each resolution\n",
    "        self.hashtable = nn.ParameterList([])  # List of hashtables for each resolution\n",
    "        for res in self.resolutions:\n",
    "            total_res = res ** self.grid_dim  # Total number of cells at this resolution\n",
    "            table_size = min(total_res, self.max_entry)  # Size of the hashtable (limited by max_entry)\n",
    "            # Initialize table with random values, scaled as per InstantNGP paper\n",
    "            table = torch.randn(int(table_size), self.feature_dim, device=self.device) *0.001 #* 0.0001 #+ torch.rand(1).to(self.device)\n",
    "            table = nn.Parameter(table)  # Convert to a learnable parameter\n",
    "            self.hashtable.append(table)  # Add to the hashtable list\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalization\n",
    "        _min = torch.tensor([-3.5552, -2.1935, -2.8307]).to(self.device)\n",
    "        _max = torch.tensor([1.8859, 3.1777, 2.0705]).to(self.device)\n",
    "        x = (x - _min ) / (_max - _min)\n",
    "\n",
    "        # print(x.min(0)[0], x.max(0)[0])\n",
    "        \n",
    "        out_feature = []\n",
    "        for lvl in range(self.num_lvl):\n",
    "            # Transform coordinates to hash space\n",
    "            coord = self.to_hash_space(x, self.resolutions[lvl])\n",
    "            floor_corner = torch.floor(coord)  # Find the floor corner for interpolation\n",
    "            # Get the corners for interpolation\n",
    "            corners = self.get_corner(floor_corner).to(torch.long)\n",
    "            # Hash the corners to get feature indices\n",
    "            feature_index = self.hash(corners, self.hashtable[lvl].shape[0], self.resolutions[lvl])\n",
    "            flat_feature_index = feature_index.to(torch.long).flatten()  # Flatten the indices\n",
    "            # Retrieve corner features from the hashtable\n",
    "            corner_feature = torch.reshape(self.hashtable[lvl][flat_feature_index],\n",
    "                                           (corners.shape[0], corners.shape[1], self.feature_dim))\n",
    "            # Calculate interpolation weights\n",
    "            weights = self.interpolation_weights(coord - floor_corner)\n",
    "            weights = torch.stack([weights, weights, weights], -1)  # Stack weights for each feature\n",
    "            # Perform weighted interpolation of corner features\n",
    "            weighted_feature = corner_feature * weights\n",
    "            summed_feature = weighted_feature.sum(-2)  # Sum the weighted features\n",
    "            out_feature.append(summed_feature)  # Append the result to the output feature list\n",
    "        return torch.cat(out_feature, -1)  # Concatenate features from all levels\n",
    "\n",
    "    def to_hash_space(self, x, resolution):\n",
    "        \"\"\"\n",
    "        Transform input coordinates to hash space, ensuring they are within the grid's resolution.\n",
    "        \"\"\"\n",
    "        return torch.clip(x * (resolution - 1), 0, resolution - 1.0001)  # Scale and clip coordinates\n",
    "\n",
    "    def interpolation_weights(self, diff):\n",
    "        \"\"\"\n",
    "        Calculate the interpolation weights based on the differences from the floor corner.\n",
    "        \"\"\"\n",
    "        ones = torch.ones_like(diff, device=self.device)  # Create a tensor of ones with the same shape as diff\n",
    "        minus_x = (ones - diff)[..., 0]  # Calculate 1 - x for each dimension\n",
    "        x = diff[..., 0]  # Get the x difference\n",
    "        minus_y = (ones - diff)[..., 1]  # Calculate 1 - y for each dimension\n",
    "        y = diff[..., 1]  # Get the y difference\n",
    "\n",
    "        if self.grid_dim == 2:\n",
    "            # For 2D, calculate weights for the four corners\n",
    "            stacks = torch.stack([minus_x * minus_y, x * minus_y, minus_x * y, x * y], -1)\n",
    "            return stacks\n",
    "        else:\n",
    "            # For 3D, calculate weights for the eight corners\n",
    "            minus_z = (ones - diff)[..., 2]  # Calculate 1 - z for each dimension\n",
    "            z = diff[..., 2]  # Get the z difference\n",
    "            stacks = torch.stack([minus_x * minus_y * minus_z,\n",
    "                                  x * minus_y * minus_z,\n",
    "                                  minus_x * y * minus_z,\n",
    "                                  x * y * minus_z,\n",
    "                                  minus_x * minus_y * z,\n",
    "                                  x * minus_y * z,\n",
    "                                  minus_x * y * z,\n",
    "                                  x * y * z], -1)\n",
    "            return stacks\n",
    "\n",
    "    def alt_weights(self, corner, coord):\n",
    "        \"\"\"\n",
    "        Alternative method for calculating weights based on the distance to the corners.\n",
    "        \"\"\"\n",
    "        diag_length = torch.full_like(coord[:, 0], 2. ** (1 / 2), device=self.device)  # Diagonal length for normalization\n",
    "        w = torch.empty(corner.shape[0], corner.shape[1], device=self.device)  # Initialize weight tensor\n",
    "        for c in range(corner.shape[1]):\n",
    "            dist = torch.norm(corner[:, c, :] - coord, dim=1)  # Calculate distance to each corner\n",
    "            w[:, c] = diag_length - dist  # Calculate weight based on distance\n",
    "        normed_w = torch.nn.functional.normalize(w, p=1)  # Normalize the weights\n",
    "        return normed_w\n",
    "\n",
    "    def hash(self, x, num_entry, res):\n",
    "        \"\"\"\n",
    "        Hash function to map coordinates to hashtable indices.\n",
    "        \"\"\"\n",
    "        if num_entry != self.max_entry:\n",
    "            # For smaller hashtables, use a simple linear hash\n",
    "            index = 0\n",
    "            for i in range(self.grid_dim):\n",
    "                index += x[..., i] * res ** i\n",
    "            return index\n",
    "        else:\n",
    "            # For larger hashtables, use a more complex hash with primes\n",
    "            _sum = 0\n",
    "            for i in range(self.grid_dim):\n",
    "                _sum = _sum ^ (x[..., i] * self.prime[i])\n",
    "            index = _sum % num_entry  # Modulo operation to keep within table size\n",
    "            return index\n",
    "\n",
    "    def get_corner(self, floor_corner):\n",
    "        \"\"\"\n",
    "        Get the corner points for interpolation based on the floor corner.\n",
    "        \"\"\"\n",
    "        num_entry = floor_corner.shape[0]  # Number of entries\n",
    "\n",
    "        if self.grid_dim == 2:\n",
    "            # Calculate corners for 2D grids\n",
    "            c000 = floor_corner\n",
    "            c001 = floor_corner + torch.tensor([0, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c010 = floor_corner + torch.tensor([1, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c011 = floor_corner + torch.ones_like(floor_corner, device=self.device)\n",
    "            stacks = torch.stack([c000, c010, c001, c011], -2)\n",
    "            return stacks\n",
    "        else:\n",
    "            # Calculate corners for 3D grids\n",
    "            c000 = floor_corner\n",
    "            c001 = floor_corner + torch.tensor([0, 0, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c010 = floor_corner + torch.tensor([0, 1, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c011 = floor_corner + torch.tensor([0, 1, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c100 = floor_corner + torch.tensor([1, 0, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c101 = floor_corner + torch.tensor([1, 0, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c110 = floor_corner + torch.tensor([1, 1, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c111 = floor_corner + torch.ones_like(floor_corner, device=self.device)\n",
    "            stacks = torch.stack([c000, c010, c001, c011, c100, c101, c110, c111], -2)\n",
    "            return stacks\n",
    "\n",
    "\n",
    "\n",
    "def cumprod_exclusive(\n",
    "\ttensor: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "\tr\"\"\"\n",
    "\t(Courtesy of https://github.com/krrish94/nerf-pytorch)\n",
    "\n",
    "\tMimick functionality of tf.math.cumprod(..., exclusive=True), as it isn't available in PyTorch.\n",
    "\n",
    "\tArgs:\n",
    "\ttensor (torch.Tensor): Tensor whose cumprod (cumulative product, see `torch.cumprod`) along dim=-1\n",
    "\t\tis to be computed.\n",
    "\tReturns:\n",
    "\tcumprod (torch.Tensor): cumprod of Tensor along dim=-1, mimiciking the functionality of\n",
    "\t\ttf.math.cumprod(..., exclusive=True) (see `tf.math.cumprod` for details).\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Compute regular cumprod first (this is equivalent to `tf.math.cumprod(..., exclusive=False)`).\n",
    "\tcumprod = torch.cumprod(tensor, -1)\n",
    "\t# \"Roll\" the elements along dimension 'dim' by 1 element.\n",
    "\tcumprod = torch.roll(cumprod, 1, -1)\n",
    "\t# Replace the first element by \"1\" as this is what tf.cumprod(..., exclusive=True) does.\n",
    "\tcumprod[..., 0] = 1.\n",
    "\n",
    "\treturn cumprod\n",
    "\n",
    "def raw2outputs(\n",
    "\traw: torch.Tensor,\n",
    "\tz_vals: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\traw_noise_std: float = 0.0,\n",
    "\twhite_bkgd: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tConvert the raw NeRF output into RGB and other maps.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Difference between consecutive elements of `z_vals`. [n_rays, n_samples]\n",
    "\tdists = z_vals[..., 1:] - z_vals[..., :-1]\n",
    "\tdists = torch.cat([dists, 1e10 * torch.ones_like(dists[..., :1])], dim=-1)\n",
    "\n",
    "\t# Multiply each distance by the norm of its corresponding direction ray\n",
    "\t# to convert to real world distance (accounts for non-unit directions).\n",
    "\tdists = dists * torch.norm(rays_d[..., None, :], dim=-1)\n",
    "\n",
    "\t# Add noise to model's predictions for density. Can be used to\n",
    "\t# regularize network during training (prevents floater artifacts).\n",
    "\tnoise = 0.\n",
    "\tif raw_noise_std > 0.:\n",
    "\t\tnoise = torch.randn(raw[..., 3].shape) * raw_noise_std\n",
    "\n",
    "\t# Predict density of each sample along each ray. Higher values imply\n",
    "\t# higher likelihood of being absorbed at this point. [n_rays, n_samples]\n",
    "\talpha = 1.0 - torch.exp(-nn.functional.relu(raw[..., 3] + noise) * dists)\n",
    "\n",
    "\t# Compute weight for RGB of each sample along each ray. [n_rays, n_samples]\n",
    "\t# The higher the alpha, the lower subsequent weights are driven.\n",
    "\tweights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n",
    "\n",
    "\t# Compute weighted RGB map.\n",
    "\trgb = torch.sigmoid(raw[..., :3])  # [n_rays, n_samples, 3]\n",
    "\trgb_map = torch.sum(weights[..., None] * rgb, dim=-2)  # [n_rays, 3]\n",
    "\n",
    "\t# Estimated depth map is predicted distance.\n",
    "\tdepth_map = torch.sum(weights * z_vals, dim=-1)\n",
    "\n",
    "\t# Disparity map is inverse depth.\n",
    "\tdisp_map = 1. / torch.max(1e-10 * torch.ones_like(depth_map),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tdepth_map / torch.sum(weights, -1))\n",
    "\n",
    "\t# Sum of weights along each ray. In [0, 1] up to numerical error.\n",
    "\tacc_map = torch.sum(weights, dim=-1)\n",
    "\n",
    "\t# To composite onto a white background, use the accumulated alpha map.\n",
    "\tif white_bkgd:\n",
    "\t\trgb_map = rgb_map + (1. - acc_map[..., None])\n",
    "\n",
    "\treturn rgb_map, depth_map, acc_map, weights\n",
    "\n",
    "\n",
    "\n",
    "def sample_pdf(\n",
    "\tbins: torch.Tensor,\n",
    "\tweights: torch.Tensor,\n",
    "\tn_samples: int,\n",
    "\tperturb: bool = False\n",
    ") -> torch.Tensor:\n",
    "\tr\"\"\"\n",
    "\tApply inverse transform sampling to a weighted set of points.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Normalize weights to get PDF.\n",
    "\tpdf = (weights + 1e-5) / torch.sum(weights + 1e-5, -1, keepdims=True) # [n_rays, weights.shape[-1]]\n",
    "\n",
    "\t# Convert PDF to CDF.\n",
    "\tcdf = torch.cumsum(pdf, dim=-1) # [n_rays, weights.shape[-1]]\n",
    "\tcdf = torch.concat([torch.zeros_like(cdf[..., :1]), cdf], dim=-1) # [n_rays, weights.shape[-1] + 1]\n",
    "\n",
    "\t# Take sample positions to grab from CDF. Linear when perturb == 0.\n",
    "\tif not perturb:\n",
    "\t\tu = torch.linspace(0., 1., n_samples, device=cdf.device)\n",
    "\t\tu = u.expand(list(cdf.shape[:-1]) + [n_samples]) # [n_rays, n_samples]\n",
    "\telse:\n",
    "\t\tu = torch.rand(list(cdf.shape[:-1]) + [n_samples], device=cdf.device) # [n_rays, n_samples]\n",
    "\n",
    "\t# Find indices along CDF where values in u would be placed.\n",
    "\tu = u.contiguous() # Returns contiguous tensor with same values.\n",
    "\tinds = torch.searchsorted(cdf, u, right=True) # [n_rays, n_samples]\n",
    "\n",
    "\t# Clamp indices that are out of bounds.\n",
    "\tbelow = torch.clamp(inds - 1, min=0)\n",
    "\tabove = torch.clamp(inds, max=cdf.shape[-1] - 1)\n",
    "\tinds_g = torch.stack([below, above], dim=-1) # [n_rays, n_samples, 2]\n",
    "\n",
    "\t# Sample from cdf and the corresponding bin centers.\n",
    "\tmatched_shape = list(inds_g.shape[:-1]) + [cdf.shape[-1]]\n",
    "\tcdf_g = torch.gather(cdf.unsqueeze(-2).expand(matched_shape), dim=-1,\n",
    "\t\t\t\t\t\t\t\t\t\t\t index=inds_g)\n",
    "\tbins_g = torch.gather(bins.unsqueeze(-2).expand(matched_shape), dim=-1,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tindex=inds_g)\n",
    "\n",
    "\t# Convert samples to ray length.\n",
    "\tdenom = (cdf_g[..., 1] - cdf_g[..., 0])\n",
    "\tdenom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n",
    "\tt = (u - cdf_g[..., 0]) / denom\n",
    "\tsamples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n",
    "\n",
    "\treturn samples # [n_rays, n_samples]\n",
    "\n",
    "def sample_hierarchical(\n",
    "\trays_o: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\tz_vals: torch.Tensor,\n",
    "\tweights: torch.Tensor,\n",
    "\tn_samples: int,\n",
    "\tperturb: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tApply hierarchical sampling to the rays.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Draw samples from PDF using z_vals as bins and weights as probabilities.\n",
    "\tz_vals_mid = .5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
    "\tnew_z_samples = sample_pdf(z_vals_mid, weights[..., 1:-1], n_samples,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tperturb=perturb)\n",
    "\tnew_z_samples = new_z_samples.detach()\n",
    "\n",
    "\t# Resample points from ray based on PDF.\n",
    "\tz_vals_combined, _ = torch.sort(torch.cat([z_vals, new_z_samples], dim=-1), dim=-1)\n",
    "\tpts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals_combined[..., :, None]  # [N_rays, N_samples + n_samples, 3]\n",
    "\treturn pts, z_vals_combined, new_z_samples\n",
    "\n",
    "\n",
    "def get_chunks(\n",
    "\tinputs: torch.Tensor,\n",
    "\tchunksize: int = 2**15\n",
    ") -> List[torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tDivide an input into chunks.\n",
    "\t\"\"\"\n",
    "\treturn [inputs[i:i + chunksize] for i in range(0, inputs.shape[0], chunksize)]\n",
    "\n",
    "def prepare_chunks(\n",
    "\tpoints: torch.Tensor,\n",
    "\tencoding_function: Callable[[torch.Tensor], torch.Tensor],\n",
    "\tchunksize: int = 2**15\n",
    ") -> List[torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tEncode and chunkify points to prepare for NeRF model.\n",
    "\t\"\"\"\n",
    "\tpoints = points.reshape((-1, 3))\n",
    "\t# print('Points:\\n',points[:].min(0)[0], points[:].max(0)[0])\n",
    "\tpoints = encoding_function(points)\n",
    "\t# print('Encoded Points:\\n',points[:3].grad)\n",
    "\tpoints = get_chunks(points, chunksize=chunksize)\n",
    "\treturn points\n",
    "\n",
    "def prepare_viewdirs_chunks(\n",
    "\tpoints: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\tencoding_function: Callable[[torch.Tensor], torch.Tensor],\n",
    "\tchunksize: int = 2**15\n",
    ") -> List[torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tEncode and chunkify viewdirs to prepare for NeRF model.\n",
    "\t\"\"\"\n",
    "\t# Prepare the viewdirs\n",
    "\tviewdirs = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
    "\tviewdirs = viewdirs[:, None, ...].expand(points.shape).reshape((-1, 3))\n",
    "\tviewdirs = encoding_function(viewdirs)\n",
    "\tviewdirs = get_chunks(viewdirs, chunksize=chunksize)\n",
    "\treturn viewdirs\n",
    "\n",
    "\n",
    "def nerf_forward(\n",
    "\trays_o: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\tnear: float,\n",
    "\tfar: float,\n",
    "\tencoding_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "\tcoarse_model: nn.Module,\n",
    "\tkwargs_sample_stratified: dict = None,\n",
    "\tn_samples_hierarchical: int = 0,\n",
    "\tkwargs_sample_hierarchical: dict = None,\n",
    "\tfine_model = None,\n",
    "\tviewdirs_encoding_fn: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n",
    "\tchunksize: int = 2**15\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]:\n",
    "\tr\"\"\"\n",
    "\tCompute forward pass through model(s).\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Set no kwargs if none are given.\n",
    "\tif kwargs_sample_stratified is None:\n",
    "\t\tkwargs_sample_stratified = {}\n",
    "\tif kwargs_sample_hierarchical is None:\n",
    "\t\tkwargs_sample_hierarchical = {}\n",
    "\n",
    "\t# Sample query points along each ray.\n",
    "\tquery_points, z_vals = sample_stratified(\n",
    "\t\t\trays_o, rays_d, near, far, **kwargs_sample_stratified)\n",
    "\n",
    "\t# Prepare batches.\n",
    "\tbatches = prepare_chunks(query_points, encoding_fn, chunksize=chunksize)\n",
    "\tif viewdirs_encoding_fn is not None:\n",
    "\t\tbatches_viewdirs = prepare_viewdirs_chunks(query_points, rays_d,\n",
    "                                                viewdirs_encoding_fn,\n",
    "                                                chunksize=chunksize)\n",
    "\telse:\n",
    "\t\tbatches_viewdirs = [None] * len(batches)\n",
    "\n",
    "\t# Coarse model pass.\n",
    "\t# Split the encoded points into \"chunks\", run the model on all chunks, and\n",
    "\t# concatenate the results (to avoid out-of-memory issues).\n",
    "\tpredictions = []\n",
    "\tfor batch, batch_viewdirs in zip(batches, batches_viewdirs):\n",
    "\t\tpredictions.append(coarse_model(batch, viewdirs=batch_viewdirs))\n",
    "\traw = torch.cat(predictions, dim=0)\n",
    "\traw = raw.reshape(list(query_points.shape[:2]) + [raw.shape[-1]])\n",
    "\n",
    "\t# Perform differentiable volume rendering to re-synthesize the RGB image.\n",
    "\trgb_map, depth_map, acc_map, weights = raw2outputs(raw, z_vals, rays_d)\n",
    "\t# rgb_map, depth_map, acc_map, weights = render_volume_density(raw, rays_o, z_vals)\n",
    "\toutputs = {\n",
    "\t\t\t'z_vals_stratified': z_vals\n",
    "\t}\n",
    "\n",
    "\t# Fine model pass.\n",
    "\tif n_samples_hierarchical > 0:\n",
    "\t\t# Save previous outputs to return.\n",
    "\t\trgb_map_0, depth_map_0, acc_map_0 = rgb_map, depth_map, acc_map\n",
    "\n",
    "\t\t# Apply hierarchical sampling for fine query points.\n",
    "\t\tquery_points, z_vals_combined, z_hierarch = sample_hierarchical(\n",
    "\t\t\trays_o, rays_d, z_vals, weights, n_samples_hierarchical,\n",
    "\t\t\t**kwargs_sample_hierarchical)\n",
    "\n",
    "\t\t# Prepare inputs as before.\n",
    "\t\tbatches = prepare_chunks(query_points, encoding_fn, chunksize=chunksize)\n",
    "\t\tif viewdirs_encoding_fn is not None:\n",
    "\t\t\tbatches_viewdirs = prepare_viewdirs_chunks(query_points, rays_d,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t viewdirs_encoding_fn,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t chunksize=chunksize)\n",
    "\t\telse:\n",
    "\t\t\tbatches_viewdirs = [None] * len(batches)\n",
    "\n",
    "\t\t# Forward pass new samples through fine model.\n",
    "\t\tfine_model = fine_model if fine_model is not None else coarse_model\n",
    "\t\tpredictions = []\n",
    "\t\tfor batch, batch_viewdirs in zip(batches, batches_viewdirs):\n",
    "\t\t\tpredictions.append(fine_model(batch, viewdirs=batch_viewdirs))\n",
    "\t\traw = torch.cat(predictions, dim=0)\n",
    "\t\traw = raw.reshape(list(query_points.shape[:2]) + [raw.shape[-1]])\n",
    "\n",
    "\t\t# Perform differentiable volume rendering to re-synthesize the RGB image.\n",
    "\t\trgb_map, depth_map, acc_map, weights = raw2outputs(raw, z_vals_combined, rays_d)\n",
    "\n",
    "\t\t# Store outputs.\n",
    "\t\toutputs['z_vals_hierarchical'] = z_hierarch\n",
    "\t\toutputs['rgb_map_0'] = rgb_map_0\n",
    "\t\toutputs['depth_map_0'] = depth_map_0\n",
    "\t\toutputs['acc_map_0'] = acc_map_0\n",
    "\n",
    "\t# Store outputs.\n",
    "\toutputs['rgb_map'] = rgb_map\n",
    "\toutputs['depth_map'] = depth_map\n",
    "\toutputs['acc_map'] = acc_map\n",
    "\toutputs['weights'] = weights\n",
    "\treturn outputs\n",
    "\n",
    "\n",
    "\n",
    "# Encoders\n",
    "d_input = 3           # Number of input dimensions\n",
    "n_freqs = 10          # Number of encoding functions for samples\n",
    "log_space = True      # If set, frequencies scale in log space\n",
    "use_viewdirs = True   # If set, use view direction as input\n",
    "n_freqs_views = 4     # Number of encoding functions for views\n",
    "\n",
    "# Stratified sampling\n",
    "n_samples = 64         # Number of spatial samples per ray\n",
    "perturb = True         # If set, applies noise to sample positions\n",
    "inverse_depth = False  # If set, samples points linearly in inverse depth\n",
    "\n",
    "# Model\n",
    "d_filter = 128          # Dimensions of linear layer filters\n",
    "n_layers = 2            # Number of layers in network bottleneck\n",
    "skip = [4]               # Layers at which to apply input residual\n",
    "use_fine_model = True   # If set, creates a fine model\n",
    "d_filter_fine = 128     # Dimensions of linear layer filters of fine network\n",
    "n_layers_fine = 6       # Number of layers in fine network bottleneck\n",
    "\n",
    "# Hierarchical sampling\n",
    "n_samples_hierarchical = 64   # Number of samples per ray\n",
    "perturb_hierarchical = False  # If set, applies noise to sample positions\n",
    "\n",
    "# Optimizer\n",
    "lr = 1e-5#5e-4  # Learning rate\n",
    "\n",
    "# Training\n",
    "n_iters = 2 #1000\n",
    "batch_size = 2**14          # Number of rays per gradient step (power of 2)\n",
    "one_image_per_step = True   # One image per gradient step (disables batching)\n",
    "chunksize = 2**14           # Modify as needed to fit in GPU memory\n",
    "center_crop = True          # Crop the center of image (one_image_per_)\n",
    "center_crop_iters = 0      # Stop cropping center after this many epochs\n",
    "display_rate = 1 #25          # Display test output every X epochs\n",
    "\n",
    "# Early Stopping\n",
    "warmup_iters = 100          # Number of iterations during warmup phase\n",
    "warmup_min_fitness = 10.0   # Min val PSNR to continue training at warmup_iters\n",
    "n_restarts = 10             # Number of times to restart if training stalls\n",
    "\n",
    "# We bundle the kwargs for various functions to pass all at once.\n",
    "kwargs_sample_stratified = {\n",
    "\t'n_samples': n_samples,\n",
    "\t'perturb': perturb,\n",
    "\t'inverse_depth': inverse_depth\n",
    "}\n",
    "kwargs_sample_hierarchical = {\n",
    "\t'perturb': perturb\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# encoder = Grid(\n",
    "#     feature_dim=3,\n",
    "#     grid_dim=3,\n",
    "#     num_lvl=8,\n",
    "#     max_res=1024, #1024*2+1024*2,\n",
    "#     min_res=16,\n",
    "#     hashtable_power=19,\n",
    "#     device=device,\n",
    "# ).to(device)\n",
    "# encode = lambda x: encoder(x)\n",
    "\n",
    "\n",
    "encoder = PositionalEncoder(3, 10)\n",
    "encode = lambda x: encoder(x)\n",
    "\n",
    "# View direction encoders\n",
    "\n",
    "encoder_viewdirs = PositionalEncoder(3, 4)\n",
    "encode_viewdirs = lambda x: encoder_viewdirs(x)\n",
    "d_viewdirs = encoder_viewdirs.d_output\n",
    "\n",
    "# Models\n",
    "model = NeRF(\n",
    "    encoder.d_output, n_layers=n_layers, d_filter=d_filter, skip=skip,d_viewdirs=d_viewdirs)\n",
    "model.to(device)\n",
    "\n",
    "fine_model = NeRF(\n",
    "    encoder.d_output, n_layers=n_layers, d_filter=d_filter, skip=skip,d_viewdirs=d_viewdirs)\n",
    "fine_model.to(device)\n",
    "\n",
    "model_params = list(model.parameters())\n",
    "model_params = model_params + list(fine_model.parameters())\n",
    "model_params = model_params + list(encoder.parameters())\n",
    "\n",
    "# Optimizer\n",
    "# optimizer = torch.optim.Adam(model_params, lr=lr)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters(), 'lr': 1e-4},\n",
    "    {'params': fine_model.parameters(), 'lr': 1e-4},\n",
    "    {'params': encoder.parameters(), 'lr': 5e-4},\n",
    "])\n",
    "\n",
    "# Early Stopping\n",
    "# warmup_stopper = EarlyStopping(patience=50)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_time(i, iternums, train_psnrs, val_psnrs):\n",
    "\tmodel.eval()\n",
    "\theight, width = testimg.shape[:2]\n",
    "\trays_o, rays_d = get_rays(height, width, focal, testpose)\n",
    "\trays_o = rays_o.reshape([-1, 3]).to(device)\n",
    "\trays_d = rays_d.reshape([-1, 3]).to(device)\n",
    "\toutputs = nerf_forward(rays_o, rays_d,\n",
    "\t\t\t\t\t\t\tnear, far, encode, model,\n",
    "\t\t\t\t\t\t\tkwargs_sample_stratified=kwargs_sample_stratified,\n",
    "\t\t\t\t\t\t\tn_samples_hierarchical=n_samples_hierarchical,\n",
    "\t\t\t\t\t\t\tkwargs_sample_hierarchical=kwargs_sample_hierarchical,\n",
    "\t\t\t\t\t\t\tfine_model=fine_model,\n",
    "\t\t\t\t\t\t\tviewdirs_encoding_fn=encode_viewdirs,\n",
    "\t\t\t\t\t\t\tchunksize=chunksize)\n",
    "\n",
    "\trgb_predicted = outputs['rgb_map']\n",
    "\ttestimg_flat = testimg.reshape(-1, 3).to(device)\n",
    "\tloss = torch.nn.functional.mse_loss(rgb_predicted, testimg_flat)\n",
    "\tprint(\"Loss:\", loss.item())\n",
    "\tval_psnr = -10. * torch.log10(loss)\n",
    "\tval_psnrs.append(val_psnr.item())\n",
    "\titernums.append(i)\n",
    "\n",
    "\t# Plot example outputs outside\n",
    "\trender(i, rgb_predicted, iternums, outputs,train_psnrs,val_psnrs)\n",
    "    \n",
    "def render(i, rgb_predicted, iternums, outputs,train_psnrs,val_psnrs):\n",
    "\tfig, ax = plt.subplots(1, 4, figsize=(24,4), gridspec_kw={'width_ratios': [1, 1, 1, 3]})\n",
    "    # Plot example outputs\n",
    "\t\n",
    "\tax[0].imshow(rgb_predicted.reshape([height, width, 3]).detach().cpu().numpy())\n",
    "\tax[0].set_title(f'Iteration: {i}')\n",
    "\tax[1].imshow(testimg.detach().cpu().numpy())\n",
    "\tax[1].set_title(f'Target')\n",
    "\tax[2].plot(range(0, i + 1), train_psnrs, 'r')\n",
    "\tax[2].plot(iternums, val_psnrs, 'b')\n",
    "\tax[2].set_title('PSNR (train=red, val=blue')\n",
    "\t# z_vals_strat = outputs['z_vals_stratified'].view((-1, n_samples))\n",
    "\t# z_sample_strat = z_vals_strat[z_vals_strat.shape[0] // 2].detach().cpu().numpy()\n",
    "\t# if 'z_vals_hierarchical' in outputs:\n",
    "\t\t# z_vals_hierarch = outputs['z_vals_hierarchical'].view((-1, n_samples_hierarchical))\n",
    "\t\t# z_sample_hierarch = z_vals_hierarch[z_vals_hierarch.shape[0] // 2].detach().cpu().numpy()\n",
    "\t# else:\n",
    "\t\t# z_sample_hierarch = None\n",
    "\t# _ = plot_samples(z_sample_strat, z_sample_hierarch, ax=ax[3])\n",
    "\tax[3].margins(0)\n",
    "\tplt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_psnrs = []\n",
    "val_psnrs = []\n",
    "iternums = []\n",
    "\n",
    "for i in trange(1000):\n",
    "\n",
    "    ###\n",
    "    ### Train one time\n",
    "    ###\n",
    "    if one_image_per_step:\n",
    "        # Randomly pick an image as the target.\n",
    "        # target_img_idx = np.random.randint(images.shape[0])\n",
    "        target_img_idx = testimg_idx\n",
    "\n",
    "        target_img = images[target_img_idx].to(device)\n",
    "        \n",
    "        height, width = target_img.shape[:2]\n",
    "        target_pose = poses[target_img_idx].to(device)\n",
    "        rays_o, rays_d = get_rays(height, width, focal, target_pose)\n",
    "        rays_o = rays_o.reshape([-1, 3]).to(device)\n",
    "        rays_d = rays_d.reshape([-1, 3]).to(device)\n",
    "\n",
    "    target_img = target_img.reshape([-1, 3]).to(device)\n",
    "\n",
    "    # Run one iteration of TinyNeRF and get the rendered RGB image.\n",
    "    outputs = nerf_forward(\n",
    "        rays_o, rays_d,\n",
    "        near, far, encode, model,\n",
    "        kwargs_sample_stratified=kwargs_sample_stratified,\n",
    "        n_samples_hierarchical=n_samples_hierarchical,\n",
    "        kwargs_sample_hierarchical=kwargs_sample_hierarchical,\n",
    "        fine_model=fine_model,\n",
    "        viewdirs_encoding_fn=encode_viewdirs,\n",
    "        chunksize=chunksize)\n",
    "\n",
    "    # Check for any numerical issues.\n",
    "    for k, v in outputs.items():\n",
    "        if torch.isnan(v).any():\n",
    "            print(f\"! [Numerical Alert] {k} contains NaN.\")\n",
    "        if torch.isinf(v).any():\n",
    "            print(f\"! [Numerical Alert] {k} contains Inf.\")\n",
    "\n",
    "    # Backprop!\n",
    "    rgb_predicted = outputs['rgb_map']\n",
    "    loss = torch.nn.functional.mse_loss(rgb_predicted, target_img)\n",
    "    loss.backward()\n",
    "\n",
    "    # Debug gradient values\n",
    "    # print('GRID GRAD:')\n",
    "    # for p in encoder.parameters():\n",
    "    #     if p.grad is not None:\n",
    "    #         print(p.grad.min(), p.grad.max())\n",
    "    #     else:\n",
    "    #         print(\"No gradient for parameter:\", p)    \n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    psnr = -10. * torch.log10(loss)\n",
    "    psnr = psnr.item()\n",
    "    if i % 100 == 0:\n",
    "        print(psnr)\n",
    "    train_psnrs += [psnr]\n",
    "\n",
    "\n",
    "    ###\n",
    "    ### Evaluate\n",
    "    ###\n",
    "    if i % 25 == 0:\n",
    "        eval_one_time(i, iternums, train_psnrs, val_psnrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_samples(\n",
    "\tz_vals: torch.Tensor,\n",
    "\tz_hierarch: Optional[torch.Tensor] = None,\n",
    "\tax: Optional[np.ndarray] = None):\n",
    "\tr\"\"\"\n",
    "\tPlot stratified and (optional) hierarchical samples.\n",
    "\t\"\"\"\n",
    "\ty_vals = 1 + np.zeros_like(z_vals)\n",
    "\n",
    "\tif ax is None:\n",
    "\t\tax = plt.subplot()\n",
    "\tax.plot(z_vals, y_vals, 'b-o')\n",
    "\tif z_hierarch is not None:\n",
    "\t\ty_hierarch = np.zeros_like(z_hierarch)\n",
    "\t\tax.plot(z_hierarch, y_hierarch, 'r-o')\n",
    "\tax.set_ylim([-1, 2])\n",
    "\tax.set_title('Stratified  Samples (blue) and Hierarchical Samples (red)')\n",
    "\tax.axes.yaxis.set_visible(False)\n",
    "\tax.grid(True)\n",
    "\treturn ax\n",
    "\n",
    "\n",
    "eval_one_time(i, iternums, train_psnrs, val_psnrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "Updated notebook:  \n",
    "- Added batch training\n",
    "- Fixed device handaling for training\n",
    "---\n",
    "This notebook walks the reader through a full implementation of the original Neural Radiance Field architecture, first introduced by Mildenhall et al. in \"[NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis.](https://www.matthewtancik.com/nerf)\" For a broader overview, read the accompanying Medium article \"[It's NeRF From Nothing: Build A Complete NeRF With Pytorch.](https://medium.com/@masonmcgough/its-nerf-from-nothing-build-a-vanilla-nerf-with-pytorch-7846e4c45666)\" This notebook assumes that you have read that article and understand the basics of NeRF.\n",
    "\n",
    "Much of the code comes from or is inspired by the original implementation by GitHub user [bmild](https://github.com/bmild/nerf) as well as PyTorch implementations from GitHub users [yenchenlin](https://github.com/bmild/nerf) and [krrish94](https://github.com/krrish94/nerf-pytorch/). The code has been modified for clarity and consistency.\n",
    "\n",
    "Based on:  \n",
    "[This colab notebook](https://colab.research.google.com/drive/1TppdSsLz8uKoNwqJqDGg8se8BHQcvg_K?usp=sharing#scrollTo=jVAGjdUthecA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from typing import Optional, Tuple, List, Union, Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from tqdm import trange\n",
    "import random\n",
    "\n",
    "# For repeatability\n",
    "seed = 2024\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "First we load the data which we will train our NeRF model on. This is the Lego bulldozer commonly seen in the NeRF demonstrations and serves as a sort of \"Hello World\" for training NeRFs. Covering other datasets is outside the scope of this notebook, but feel free to try others included in the original [NeRF source code](https://github.com/bmild/nerf) or your own datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('tiny_nerf_data.npz'):\n",
    "\t!wget http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset consists of 106 images taken of the synthetic Lego bulldozer along with poses and a common focal length value. Like the original, we reserve the first 100 images for training and a single test image for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('tiny_nerf_data.npz')\n",
    "images = data['images']\n",
    "poses = data['poses']\n",
    "focal = data['focal']\n",
    "\n",
    "print(f'Images shape: {images.shape}')\n",
    "print(f'Poses shape: {poses.shape}')\n",
    "print(f'Focal length: {focal}')\n",
    "\n",
    "height, width = images.shape[1:3]\n",
    "near, far = 2., 6.\n",
    "\n",
    "n_training = 100\n",
    "testimg_idx = 99\n",
    "testimg, testpose = images[testimg_idx], poses[testimg_idx]\n",
    "\n",
    "plt.imshow(testimg)\n",
    "print('Pose')\n",
    "print(testpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Origins and directions\n",
    "\n",
    "Recall that NeRF processes inputs from a field of positions (x,y,z) and view directions (θ,φ). To gather these input points, we need to apply inverse rendering to the input images. More concretely, we draw projection lines through each pixel and across the 3D space, from which we can draw samples.\n",
    "\n",
    "To sample points from the 3D space beyond our image, we first start from the initial pose of every camera taken in the photo set. With some vector math, we can convert these 4x4 pose matrices into a 3D coordinate denoting the origin and a 3D vector indicating the direction. The two together describe a vector that indicates where a camera was pointing when the photo was taken.\n",
    "\n",
    "The code in the cell below illustrates this by drawing arrows that depict the origin and the direction of every frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = np.stack([np.sum([0, 0, -1] * pose[:3, :3], axis=-1) for pose in poses])\n",
    "origins = poses[:, :3, -1]\n",
    "\n",
    "ax = plt.figure(figsize=(12, 8)).add_subplot(projection='3d')\n",
    "_ = ax.quiver(\n",
    "\torigins[..., 0].flatten(),\n",
    "\torigins[..., 1].flatten(),\n",
    "\torigins[..., 2].flatten(),\n",
    "\tdirs[..., 0].flatten(),\n",
    "\tdirs[..., 1].flatten(),\n",
    "\tdirs[..., 2].flatten(), length=0.5, normalize=True)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this camera pose, we can now find the projection lines along each pixel of our image. Each line is defined by its origin point (x,y,z) and its direction (in this case a 3D vector). While the origin is the same for every pixel, the direction is slightly different. These lines are slightly deflected off center such that none of these lines are parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pinhole camera](https://www.researchgate.net/profile/Willy-Azarcoya-Cabiedes/publication/317498100/figure/fig10/AS:610418494013440@1522546518034/Pin-hole-camera-model-terminology-The-optical-center-pinhole-is-placed-at-the-origin.png)\n",
    "\n",
    "From [Willy Azarcoya-Cabiedes via ResearchGate](https://www.researchgate.net/figure/Pin-hole-camera-model-terminology-The-optical-center-pinhole-is-placed-at-the-origin_fig10_317498100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rays(\n",
    "\theight: int,\n",
    "\twidth: int,\n",
    "\tfocal_length: float,\n",
    "\tc2w: torch.Tensor\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tFind origin and direction of rays through every pixel and camera origin.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Apply pinhole camera model to gather directions at each pixel\n",
    "\ti, j = torch.meshgrid(\n",
    "\t\t\ttorch.arange(width, dtype=torch.float32).to(c2w),\n",
    "\t\t\ttorch.arange(height, dtype=torch.float32).to(c2w),\n",
    "\t\t\tindexing='ij')\n",
    "\ti, j = i.transpose(-1, -2), j.transpose(-1, -2)\n",
    "\tdirections = torch.stack([(i - width * .5) / focal_length,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t-(j - height * .5) / focal_length,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t-torch.ones_like(i)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t ], dim=-1)\n",
    "\n",
    "\t# Apply camera pose to directions\n",
    "\trays_d = torch.sum(directions[..., None, :] * c2w[:3, :3], dim=-1)\n",
    "\n",
    "\t# Origin is same for all directions (the optical center)\n",
    "\trays_o = c2w[:3, -1].expand(rays_d.shape)\n",
    "\treturn rays_o, rays_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather as torch tensors\n",
    "images = torch.from_numpy(data['images'][:n_training])#.to(device)\n",
    "poses = torch.from_numpy(data['poses'])#.to(device)\n",
    "focal = torch.from_numpy(data['focal'])#.to(device)\n",
    "testimg = torch.from_numpy(data['images'][testimg_idx])#.to(device)\n",
    "testpose = torch.from_numpy(data['poses'][testimg_idx])#.to(device)\n",
    "\n",
    "# Grab rays from sample image\n",
    "height, width = images.shape[1:3]\n",
    "with torch.no_grad():\n",
    "\tray_origin, ray_direction = get_rays(height, width, focal, testpose)\n",
    "\n",
    "print('Ray Origin')\n",
    "print(ray_origin.shape)\n",
    "print(ray_origin[height // 2, width // 2, :])\n",
    "print('')\n",
    "\n",
    "print('Ray Direction')\n",
    "print(ray_direction.shape)\n",
    "print(ray_direction[height // 2, width // 2, :])\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Sampling\n",
    "\n",
    "Now that we have these lines, defined as origin and direction vectors, we can begin the process of sampling them. Recall that NeRF takes a coarse-to-fine sampling strategy, starting with the stratified sampling approach.\n",
    "\n",
    "The stratified sampling approach splits the ray into evenly-spaced bins and randomly samples within each bin. The `perturb` setting determines whether to sample points uniformly from each bin or to simply use the bin center as the point. In most cases, we want to keep `perturb = True` as it will encourage the network to learn over a continuously sampled space. It may be useful to disable for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_stratified(\n",
    "\trays_o: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\tnear: float,\n",
    "\tfar: float,\n",
    "\tn_samples: int,\n",
    "\tperturb: Optional[bool] = True,\n",
    "\tinverse_depth: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tSample along ray from regularly-spaced bins.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Grab samples for space integration along ray\n",
    "\tt_vals = torch.linspace(0., 1., n_samples, device=rays_o.device)\n",
    "\tif not inverse_depth:\n",
    "\t\t# Sample linearly between `near` and `far`\n",
    "\t\tz_vals = near * (1.-t_vals) + far * (t_vals)\n",
    "\telse:\n",
    "\t\t# Sample linearly in inverse depth (disparity)\n",
    "\t\tz_vals = 1./(1./near * (1.-t_vals) + 1./far * (t_vals))\n",
    "\n",
    "\t# Draw uniform samples from bins along ray\n",
    "\tif perturb:\n",
    "\t\tmids = .5 * (z_vals[1:] + z_vals[:-1])\n",
    "\t\tupper = torch.concat([mids, z_vals[-1:]], dim=-1)\n",
    "\t\tlower = torch.concat([z_vals[:1], mids], dim=-1)\n",
    "\t\tt_rand = torch.rand([n_samples], device=z_vals.device)\n",
    "\t\tz_vals = lower + (upper - lower) * t_rand\n",
    "\tz_vals = z_vals.expand(list(rays_o.shape[:-1]) + [n_samples])\n",
    "\n",
    "\t# Apply scale from `rays_d` and offset from `rays_o` to samples\n",
    "\t# pts: (width, height, n_samples, 3)\n",
    "\tpts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals[..., :, None]\n",
    "\treturn pts, z_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Draw stratified samples from example\n",
    "# rays_o = ray_origin.view([-1, 3])\n",
    "# rays_d = ray_direction.view([-1, 3])\n",
    "# n_samples = 8\n",
    "# perturb = True\n",
    "# inverse_depth = False\n",
    "# with torch.no_grad():\n",
    "# \tpts, z_vals = sample_stratified(rays_o, rays_d, near, far, n_samples,\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tperturb=perturb, inverse_depth=inverse_depth)\n",
    "\n",
    "# print('Input Points')\n",
    "# print(pts.shape)\n",
    "# print('')\n",
    "# print('Distances Along Ray')\n",
    "# print(z_vals.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize these sampled points. The unperturbed blue points are the bin \"centers.\" The red points are a sampling of perturbed points. Notice how the red points are slightly offset from the blue points above them, but all are constrained between `near` and `far`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_vals = torch.zeros_like(z_vals)\n",
    "\n",
    "# _, z_vals_unperturbed = sample_stratified(rays_o, rays_d, near, far, n_samples,\n",
    "# \t\t\t\t\t\t\t\t  perturb=False, inverse_depth=inverse_depth)\n",
    "# plt.plot(z_vals_unperturbed[0].cpu().numpy(), 1 + y_vals[0].cpu().numpy(), 'b-o')\n",
    "# plt.plot(z_vals[0].cpu().numpy(), y_vals[0].cpu().numpy(), 'r-o')\n",
    "# plt.ylim([-1, 2])\n",
    "# plt.title('Stratified Sampling (blue) with Perturbation (red)')\n",
    "# ax = plt.gca()\n",
    "# ax.axes.yaxis.set_visible(False)\n",
    "# plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoder\n",
    "\n",
    "Much like Transformers, NeRFs make use of positional encoders. In this case, it's to map the inputs to a higher frequency space to compensate for the bias that neural networks have for learning lower-frequency functions.\n",
    "\n",
    "Here we build a simple `torch.nn.Module` of our positional encoder. The same encoder implementation can be applied to both input samples and view directions. However, we choose different parameters for these inputs. We use the default settings from the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "\tr\"\"\"\n",
    "\tSine-cosine positional encoder for input points.\n",
    "\t\"\"\"\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\td_input: int,\n",
    "\t\tn_freqs: int,\n",
    "\t\tlog_space: bool = False\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.d_input = d_input\n",
    "\t\tself.n_freqs = n_freqs\n",
    "\t\tself.log_space = log_space\n",
    "\t\tself.d_output = d_input * (1 + 2 * self.n_freqs)\n",
    "\t\tself.embed_fns = [lambda x: x]\n",
    "\n",
    "\t\t# Define frequencies in either linear or log scale\n",
    "\t\tif self.log_space:\n",
    "\t\t\tfreq_bands = 2.**torch.linspace(0., self.n_freqs - 1, self.n_freqs)\n",
    "\t\telse:\n",
    "\t\t\tfreq_bands = torch.linspace(2.**0., 2.**(self.n_freqs - 1), self.n_freqs)\n",
    "\n",
    "\t\t# Alternate sin and cos\n",
    "\t\tfor freq in freq_bands:\n",
    "\t\t\tself.embed_fns.append(lambda x, freq=freq: torch.sin(x * freq))\n",
    "\t\t\tself.embed_fns.append(lambda x, freq=freq: torch.cos(x * freq))\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tx\n",
    "\t) -> torch.Tensor:\n",
    "\t\tr\"\"\"\n",
    "\t\tApply positional encoding to input.\n",
    "\t\t\"\"\"\n",
    "\t\treturn torch.concat([fn(x) for fn in self.embed_fns], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create encoders for points and view directions\n",
    "# encoder = PositionalEncoder(3, 10)\n",
    "# viewdirs_encoder = PositionalEncoder(3, 4)\n",
    "\n",
    "# # Grab flattened points and view directions\n",
    "# pts_flattened = pts.reshape(-1, 3)\n",
    "# viewdirs = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
    "# flattened_viewdirs = viewdirs[:, None, ...].expand(pts.shape).reshape((-1, 3))\n",
    "\n",
    "# # Encode inputs\n",
    "# encoded_points = encoder(pts_flattened)\n",
    "# encoded_viewdirs = viewdirs_encoder(flattened_viewdirs)\n",
    "\n",
    "# print('Encoded Points')\n",
    "# print(encoded_points.shape)\n",
    "# print(torch.min(encoded_points), torch.max(encoded_points), torch.mean(encoded_points))\n",
    "# print('')\n",
    "\n",
    "# print(encoded_viewdirs.shape)\n",
    "# print('Encoded Viewdirs')\n",
    "# print(torch.min(encoded_viewdirs), torch.max(encoded_viewdirs), torch.mean(encoded_viewdirs))\n",
    "# print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network grid class. The input x needs to be within [0, 1].\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 feature_dim: int,\n",
    "                 grid_dim: int,\n",
    "                 num_lvl: int,\n",
    "                 max_res: int,\n",
    "                 min_res: int,\n",
    "                 hashtable_power: int,\n",
    "                 device='cpu'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Determine the device to use (CPU or CUDA)\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize the attributes of the grid\n",
    "        self.feature_dim = feature_dim  # Dimensionality of the feature vectors\n",
    "        self.grid_dim = grid_dim  # Dimensionality of the grid (e.g., 2D, 3D)\n",
    "        self.num_lvl = num_lvl  # Number of levels in the grid hierarchy\n",
    "        self.max_res = max_res  # Maximum resolution of the grid\n",
    "        self.min_res = min_res  # Minimum resolution of the grid\n",
    "        self.hashtable_power = hashtable_power  # Power of the hashtable size (number of entries is 2^hashtable_power)\n",
    "\n",
    "        self.d_output = num_lvl * grid_dim\n",
    "\n",
    "        # Constants for hash calculations\n",
    "        self.prime = [3367900313, 2654435761, 805459861]  # Prime numbers for hashing\n",
    "        self.max_entry = 2 ** self.hashtable_power  # Maximum number of entries in the hashtable\n",
    "        # Factor to scale resolutions logarithmically\n",
    "        self.factor_b = np.exp((np.log(self.max_res) - np.log(self.min_res)) / (self.num_lvl - 1))\n",
    "\n",
    "        # Compute the resolutions for each level\n",
    "        self.resolutions = []\n",
    "        for i in range(self.num_lvl):\n",
    "            # Calculate resolution for level i\n",
    "            self.resolutions.append(np.floor(self.min_res * (self.factor_b ** i)))\n",
    "\n",
    "        # Initialize the hashtable for each resolution\n",
    "        self.hashtable = nn.ParameterList([])  # List of hashtables for each resolution\n",
    "        for res in self.resolutions:\n",
    "            total_res = res ** self.grid_dim  # Total number of cells at this resolution\n",
    "            table_size = min(total_res, self.max_entry)  # Size of the hashtable (limited by max_entry)\n",
    "            # Initialize table with random values, scaled as per InstantNGP paper\n",
    "            table = torch.randn(int(table_size), self.feature_dim, device=self.device) * 0.0001 + torch.rand(1).to(self.device)\n",
    "            table = nn.Parameter(table)  # Convert to a learnable parameter\n",
    "            self.hashtable.append(table)  # Add to the hashtable list\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalization\n",
    "        _min = torch.tensor([-3.5552, -2.1935, -2.8307]).to(self.device)\n",
    "        _max = torch.tensor([1.8859, 3.1777, 2.0705]).to(self.device)\n",
    "        x = (x - _min ) / (_max - _min)\n",
    "\n",
    "        # print(x.min(0)[0], x.max(0)[0])\n",
    "        \n",
    "        out_feature = []\n",
    "        for lvl in range(self.num_lvl):\n",
    "            # Transform coordinates to hash space\n",
    "            coord = self.to_hash_space(x, self.resolutions[lvl])\n",
    "            floor_corner = torch.floor(coord)  # Find the floor corner for interpolation\n",
    "            # Get the corners for interpolation\n",
    "            corners = self.get_corner(floor_corner).to(torch.long)\n",
    "            # Hash the corners to get feature indices\n",
    "            feature_index = self.hash(corners, self.hashtable[lvl].shape[0], self.resolutions[lvl])\n",
    "            flat_feature_index = feature_index.to(torch.long).flatten()  # Flatten the indices\n",
    "            # Retrieve corner features from the hashtable\n",
    "            corner_feature = torch.reshape(self.hashtable[lvl][flat_feature_index],\n",
    "                                           (corners.shape[0], corners.shape[1], self.feature_dim))\n",
    "            # Calculate interpolation weights\n",
    "            weights = self.interpolation_weights(coord - floor_corner)\n",
    "            weights = torch.stack([weights, weights, weights], -1)  # Stack weights for each feature\n",
    "            # Perform weighted interpolation of corner features\n",
    "            weighted_feature = corner_feature * weights\n",
    "            summed_feature = weighted_feature.sum(-2)  # Sum the weighted features\n",
    "            out_feature.append(summed_feature)  # Append the result to the output feature list\n",
    "        return torch.cat(out_feature, -1)  # Concatenate features from all levels\n",
    "\n",
    "    def to_hash_space(self, x, resolution):\n",
    "        \"\"\"\n",
    "        Transform input coordinates to hash space, ensuring they are within the grid's resolution.\n",
    "        \"\"\"\n",
    "        return torch.clip(x * (resolution - 1), 0, resolution - 1.0001)  # Scale and clip coordinates\n",
    "\n",
    "    def interpolation_weights(self, diff):\n",
    "        \"\"\"\n",
    "        Calculate the interpolation weights based on the differences from the floor corner.\n",
    "        \"\"\"\n",
    "        ones = torch.ones_like(diff, device=self.device)  # Create a tensor of ones with the same shape as diff\n",
    "        minus_x = (ones - diff)[..., 0]  # Calculate 1 - x for each dimension\n",
    "        x = diff[..., 0]  # Get the x difference\n",
    "        minus_y = (ones - diff)[..., 1]  # Calculate 1 - y for each dimension\n",
    "        y = diff[..., 1]  # Get the y difference\n",
    "\n",
    "        if self.grid_dim == 2:\n",
    "            # For 2D, calculate weights for the four corners\n",
    "            stacks = torch.stack([minus_x * minus_y, x * minus_y, minus_x * y, x * y], -1)\n",
    "            return stacks\n",
    "        else:\n",
    "            # For 3D, calculate weights for the eight corners\n",
    "            minus_z = (ones - diff)[..., 2]  # Calculate 1 - z for each dimension\n",
    "            z = diff[..., 2]  # Get the z difference\n",
    "            stacks = torch.stack([minus_x * minus_y * minus_z,\n",
    "                                  x * minus_y * minus_z,\n",
    "                                  minus_x * y * minus_z,\n",
    "                                  x * y * minus_z,\n",
    "                                  minus_x * minus_y * z,\n",
    "                                  x * minus_y * z,\n",
    "                                  minus_x * y * z,\n",
    "                                  x * y * z], -1)\n",
    "            return stacks\n",
    "\n",
    "    def alt_weights(self, corner, coord):\n",
    "        \"\"\"\n",
    "        Alternative method for calculating weights based on the distance to the corners.\n",
    "        \"\"\"\n",
    "        diag_length = torch.full_like(coord[:, 0], 2. ** (1 / 2), device=self.device)  # Diagonal length for normalization\n",
    "        w = torch.empty(corner.shape[0], corner.shape[1], device=self.device)  # Initialize weight tensor\n",
    "        for c in range(corner.shape[1]):\n",
    "            dist = torch.norm(corner[:, c, :] - coord, dim=1)  # Calculate distance to each corner\n",
    "            w[:, c] = diag_length - dist  # Calculate weight based on distance\n",
    "        normed_w = torch.nn.functional.normalize(w, p=1)  # Normalize the weights\n",
    "        return normed_w\n",
    "\n",
    "    def hash(self, x, num_entry, res):\n",
    "        \"\"\"\n",
    "        Hash function to map coordinates to hashtable indices.\n",
    "        \"\"\"\n",
    "        if num_entry != self.max_entry:\n",
    "            # For smaller hashtables, use a simple linear hash\n",
    "            index = 0\n",
    "            for i in range(self.grid_dim):\n",
    "                index += x[..., i] * res ** i\n",
    "            return index\n",
    "        else:\n",
    "            # For larger hashtables, use a more complex hash with primes\n",
    "            _sum = 0\n",
    "            for i in range(self.grid_dim):\n",
    "                _sum = _sum ^ (x[..., i] * self.prime[i])\n",
    "            index = _sum % num_entry  # Modulo operation to keep within table size\n",
    "            return index\n",
    "\n",
    "    def get_corner(self, floor_corner):\n",
    "        \"\"\"\n",
    "        Get the corner points for interpolation based on the floor corner.\n",
    "        \"\"\"\n",
    "        num_entry = floor_corner.shape[0]  # Number of entries\n",
    "\n",
    "        if self.grid_dim == 2:\n",
    "            # Calculate corners for 2D grids\n",
    "            c000 = floor_corner\n",
    "            c001 = floor_corner + torch.tensor([0, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c010 = floor_corner + torch.tensor([1, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c011 = floor_corner + torch.ones_like(floor_corner, device=self.device)\n",
    "            stacks = torch.stack([c000, c010, c001, c011], -2)\n",
    "            return stacks\n",
    "        else:\n",
    "            # Calculate corners for 3D grids\n",
    "            c000 = floor_corner\n",
    "            c001 = floor_corner + torch.tensor([0, 0, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c010 = floor_corner + torch.tensor([0, 1, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c011 = floor_corner + torch.tensor([0, 1, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c100 = floor_corner + torch.tensor([1, 0, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c101 = floor_corner + torch.tensor([1, 0, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c110 = floor_corner + torch.tensor([1, 1, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c111 = floor_corner + torch.ones_like(floor_corner, device=self.device)\n",
    "            stacks = torch.stack([c000, c010, c001, c011, c100, c101, c110, c111], -2)\n",
    "            return stacks\n",
    "\n",
    "\n",
    "# grid = Grid(\n",
    "#     feature_dim=3,\n",
    "#     grid_dim=3,\n",
    "#     num_lvl=6,\n",
    "#     max_res=1024*2+1024*2,\n",
    "#     min_res=16,\n",
    "#     hashtable_power=19,\n",
    "# )\n",
    "\n",
    "# grid.d_output\n",
    "#     device=device,\n",
    "# ).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example NeRF sampling function call\n",
    "# rays_o = ray_origin.view([-1, 3])\n",
    "# rays_d = ray_direction.view([-1, 3])\n",
    "# n_samples = 32\n",
    "# perturb = True\n",
    "# inverse_depth = False\n",
    "# with torch.no_grad():\n",
    "#     pts, z_vals = sample_stratified(rays_o, rays_d, near, far, n_samples,\n",
    "#                                     perturb=perturb, inverse_depth=inverse_depth)\n",
    "\n",
    "# pts_flattened = pts.reshape(-1, 3)\n",
    "\n",
    "# # Normalize NeRF points to [0, 1]\n",
    "# bounding_box_min = torch.min(pts_flattened, dim=0).values\n",
    "# bounding_box_max = torch.max(pts_flattened, dim=0).values\n",
    "# print(bounding_box_min, bounding_box_max)\n",
    "# pts_normalized = (pts_flattened - bounding_box_min) / (bounding_box_max - bounding_box_min)\n",
    "# pts_normalized.max(0)[0], pts_normalized.min(0)[0]\n",
    "\n",
    "# print('Input Points')\n",
    "# print(pts_normalized.shape)\n",
    "# print('')\n",
    "# print('Distances Along Ray')\n",
    "# print(z_vals.shape)\n",
    "\n",
    "# # Pass the normalized points to the Grid\n",
    "# # output_features = grid(pts_normalized)\n",
    "# # output_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hook = list(grid.parameters())[0].register_hook(lambda grad: print('HOOK:\\n',grad.max(), grad.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pts_flattened.min(0)[0], pts_flattened.max(0)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # a = pts_flattened\n",
    "\n",
    "# _min = torch.tensor([-3.4545, -2.1728, -2.7219])\n",
    "# _max = torch.tensor([1.8695, 3.1028, 1.9795])\n",
    "# a = (pts_flattened - _min ) / (_max - _min)\n",
    "\n",
    " \n",
    "# a.min(0)[0], a.max(0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = grid(a)#\n",
    "# plt.plot(out[-100:, :3].detach()); plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = (out  - 1).mean()\n",
    "\n",
    "# # torch.nn.utils.clip_grad_norm_(grid.parameters(), max_norm=1.0)\n",
    "\n",
    "# print(list(grid.parameters())[0].grad)\n",
    "\n",
    "# l.backward()\n",
    "# # l.backwards()\n",
    "# # for p in grid.parameters():\n",
    "# #     print(p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeRF Model\n",
    "\n",
    "Here we define the NeRF model, which consists primarily of a `ModuleList` of `Linear` layers, separated by non-linear activation functions and the occasional residual connection. This model features an optional input for view directions, which will alter the model architecture if provided at instantiation. This implementation is based on Section 3 of the original \"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\" paper and uses the same defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRF(nn.Module):\n",
    "\tr\"\"\"\n",
    "\tNeural radiance fields module.\n",
    "\t\"\"\"\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\td_input: int = 3,\n",
    "\t\tn_layers: int = 8,\n",
    "\t\td_filter: int = 256,\n",
    "\t\tskip: Tuple[int] = (4,),\n",
    "\t\td_viewdirs: Optional[int] = None\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.d_input = d_input\n",
    "\t\tself.skip = skip\n",
    "\t\tself.act = nn.functional.relu\n",
    "\t\tself.d_viewdirs = d_viewdirs\n",
    "\n",
    "\t\t# Create model layers\n",
    "\t\tself.layers = nn.ModuleList(\n",
    "\t\t\t[nn.Linear(self.d_input, d_filter)] +\n",
    "\t\t\t[nn.Linear(d_filter + self.d_input, d_filter) if i in skip \\\n",
    "\t\t\t else nn.Linear(d_filter, d_filter) for i in range(n_layers - 1)]\n",
    "\t\t)\n",
    "\n",
    "\t\t# Bottleneck layers\n",
    "\t\tif self.d_viewdirs is not None:\n",
    "\t\t\t# If using viewdirs, split alpha and RGB\n",
    "\t\t\tself.alpha_out = nn.Linear(d_filter, 1)\n",
    "\t\t\tself.rgb_filters = nn.Linear(d_filter, d_filter)\n",
    "\t\t\tself.branch = nn.Linear(d_filter + self.d_viewdirs, d_filter // 2)\n",
    "\t\t\tself.output = nn.Linear(d_filter // 2, 3)\n",
    "\t\telse:\n",
    "\t\t\t# If no viewdirs, use simpler output\n",
    "\t\t\tself.output = nn.Linear(d_filter, 4)\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tx: torch.Tensor,\n",
    "\t\tviewdirs: Optional[torch.Tensor] = None\n",
    "\t) -> torch.Tensor:\n",
    "\t\tr\"\"\"\n",
    "\t\tForward pass with optional view direction.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# Cannot use viewdirs if instantiated with d_viewdirs = None\n",
    "\t\tif self.d_viewdirs is None and viewdirs is not None:\n",
    "\t\t\traise ValueError('Cannot input x_direction if d_viewdirs was not given.')\n",
    "\n",
    "\t\t# Apply forward pass up to bottleneck\n",
    "\t\tx_input = x\n",
    "\t\tfor i, layer in enumerate(self.layers):\n",
    "\t\t\tx = self.act(layer(x))\n",
    "\t\t\tif i in self.skip:\n",
    "\t\t\t\tx = torch.cat([x, x_input], dim=-1)\n",
    "\n",
    "\t\t# Apply bottleneck\n",
    "\t\tif self.d_viewdirs is not None:\n",
    "\t\t\t# Split alpha from network output\n",
    "\t\t\talpha = self.alpha_out(x)\n",
    "\n",
    "\t\t\t# Pass through bottleneck to get RGB\n",
    "\t\t\tx = self.rgb_filters(x)\n",
    "\t\t\tx = torch.concat([x, viewdirs], dim=-1)\n",
    "\t\t\tx = self.act(self.branch(x))\n",
    "\t\t\tx = self.output(x)\n",
    "\n",
    "\t\t\t# Concatenate alphas to output\n",
    "\t\t\tx = torch.concat([x, alpha], dim=-1)\n",
    "\t\telse:\n",
    "\t\t\t# Simple output\n",
    "\t\t\tx = self.output(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volume Rendering\n",
    "\n",
    "From the raw NeRF outputs, we still need to convert these into an image. This is where we apply the volume integration described in Equations 1-3 in Section 4 of the paper. Essentially, we take the weighted sum of all samples along the ray of each pixel to get the estimated color value at that pixel. Each RGB sample is weighted by its alpha value. Higher alpha values indicate higher likelihood that the sampled area is opaque, therefore points further along the ray are likelier to be occluded. The cumulative product ensures that those further points are dampened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumprod_exclusive(\n",
    "\ttensor: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "\tr\"\"\"\n",
    "\t(Courtesy of https://github.com/krrish94/nerf-pytorch)\n",
    "\n",
    "\tMimick functionality of tf.math.cumprod(..., exclusive=True), as it isn't available in PyTorch.\n",
    "\n",
    "\tArgs:\n",
    "\ttensor (torch.Tensor): Tensor whose cumprod (cumulative product, see `torch.cumprod`) along dim=-1\n",
    "\t\tis to be computed.\n",
    "\tReturns:\n",
    "\tcumprod (torch.Tensor): cumprod of Tensor along dim=-1, mimiciking the functionality of\n",
    "\t\ttf.math.cumprod(..., exclusive=True) (see `tf.math.cumprod` for details).\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Compute regular cumprod first (this is equivalent to `tf.math.cumprod(..., exclusive=False)`).\n",
    "\tcumprod = torch.cumprod(tensor, -1)\n",
    "\t# \"Roll\" the elements along dimension 'dim' by 1 element.\n",
    "\tcumprod = torch.roll(cumprod, 1, -1)\n",
    "\t# Replace the first element by \"1\" as this is what tf.cumprod(..., exclusive=True) does.\n",
    "\tcumprod[..., 0] = 1.\n",
    "\n",
    "\treturn cumprod\n",
    "\n",
    "def raw2outputs(\n",
    "\traw: torch.Tensor,\n",
    "\tz_vals: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\traw_noise_std: float = 0.0,\n",
    "\twhite_bkgd: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tConvert the raw NeRF output into RGB and other maps.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Difference between consecutive elements of `z_vals`. [n_rays, n_samples]\n",
    "\tdists = z_vals[..., 1:] - z_vals[..., :-1]\n",
    "\tdists = torch.cat([dists, 1e10 * torch.ones_like(dists[..., :1])], dim=-1)\n",
    "\n",
    "\t# Multiply each distance by the norm of its corresponding direction ray\n",
    "\t# to convert to real world distance (accounts for non-unit directions).\n",
    "\tdists = dists * torch.norm(rays_d[..., None, :], dim=-1)\n",
    "\n",
    "\t# Add noise to model's predictions for density. Can be used to\n",
    "\t# regularize network during training (prevents floater artifacts).\n",
    "\tnoise = 0.\n",
    "\tif raw_noise_std > 0.:\n",
    "\t\tnoise = torch.randn(raw[..., 3].shape) * raw_noise_std\n",
    "\n",
    "\t# Predict density of each sample along each ray. Higher values imply\n",
    "\t# higher likelihood of being absorbed at this point. [n_rays, n_samples]\n",
    "\talpha = 1.0 - torch.exp(-nn.functional.relu(raw[..., 3] + noise) * dists)\n",
    "\n",
    "\t# Compute weight for RGB of each sample along each ray. [n_rays, n_samples]\n",
    "\t# The higher the alpha, the lower subsequent weights are driven.\n",
    "\tweights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n",
    "\n",
    "\t# Compute weighted RGB map.\n",
    "\trgb = torch.sigmoid(raw[..., :3])  # [n_rays, n_samples, 3]\n",
    "\trgb_map = torch.sum(weights[..., None] * rgb, dim=-2)  # [n_rays, 3]\n",
    "\n",
    "\t# Estimated depth map is predicted distance.\n",
    "\tdepth_map = torch.sum(weights * z_vals, dim=-1)\n",
    "\n",
    "\t# Disparity map is inverse depth.\n",
    "\tdisp_map = 1. / torch.max(1e-10 * torch.ones_like(depth_map),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tdepth_map / torch.sum(weights, -1))\n",
    "\n",
    "\t# Sum of weights along each ray. In [0, 1] up to numerical error.\n",
    "\tacc_map = torch.sum(weights, dim=-1)\n",
    "\n",
    "\t# To composite onto a white background, use the accumulated alpha map.\n",
    "\tif white_bkgd:\n",
    "\t\trgb_map = rgb_map + (1. - acc_map[..., None])\n",
    "\n",
    "\treturn rgb_map, depth_map, acc_map, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Volume Sampling\n",
    "\n",
    "The 3D space is in fact very sparse with occlusions and so most points don't contribute much to the rendered image. It is therefore more beneficial to oversample regions with a high likelihood of contributing to the integral. Here we apply learned, normalized weights to the first set of samples to create a PDF across the ray, then apply inverse transform sampling to this PDF to gather a second set of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pdf(\n",
    "\tbins: torch.Tensor,\n",
    "\tweights: torch.Tensor,\n",
    "\tn_samples: int,\n",
    "\tperturb: bool = False\n",
    ") -> torch.Tensor:\n",
    "\tr\"\"\"\n",
    "\tApply inverse transform sampling to a weighted set of points.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Normalize weights to get PDF.\n",
    "\tpdf = (weights + 1e-5) / torch.sum(weights + 1e-5, -1, keepdims=True) # [n_rays, weights.shape[-1]]\n",
    "\n",
    "\t# Convert PDF to CDF.\n",
    "\tcdf = torch.cumsum(pdf, dim=-1) # [n_rays, weights.shape[-1]]\n",
    "\tcdf = torch.concat([torch.zeros_like(cdf[..., :1]), cdf], dim=-1) # [n_rays, weights.shape[-1] + 1]\n",
    "\n",
    "\t# Take sample positions to grab from CDF. Linear when perturb == 0.\n",
    "\tif not perturb:\n",
    "\t\tu = torch.linspace(0., 1., n_samples, device=cdf.device)\n",
    "\t\tu = u.expand(list(cdf.shape[:-1]) + [n_samples]) # [n_rays, n_samples]\n",
    "\telse:\n",
    "\t\tu = torch.rand(list(cdf.shape[:-1]) + [n_samples], device=cdf.device) # [n_rays, n_samples]\n",
    "\n",
    "\t# Find indices along CDF where values in u would be placed.\n",
    "\tu = u.contiguous() # Returns contiguous tensor with same values.\n",
    "\tinds = torch.searchsorted(cdf, u, right=True) # [n_rays, n_samples]\n",
    "\n",
    "\t# Clamp indices that are out of bounds.\n",
    "\tbelow = torch.clamp(inds - 1, min=0)\n",
    "\tabove = torch.clamp(inds, max=cdf.shape[-1] - 1)\n",
    "\tinds_g = torch.stack([below, above], dim=-1) # [n_rays, n_samples, 2]\n",
    "\n",
    "\t# Sample from cdf and the corresponding bin centers.\n",
    "\tmatched_shape = list(inds_g.shape[:-1]) + [cdf.shape[-1]]\n",
    "\tcdf_g = torch.gather(cdf.unsqueeze(-2).expand(matched_shape), dim=-1,\n",
    "\t\t\t\t\t\t\t\t\t\t\t index=inds_g)\n",
    "\tbins_g = torch.gather(bins.unsqueeze(-2).expand(matched_shape), dim=-1,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tindex=inds_g)\n",
    "\n",
    "\t# Convert samples to ray length.\n",
    "\tdenom = (cdf_g[..., 1] - cdf_g[..., 0])\n",
    "\tdenom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n",
    "\tt = (u - cdf_g[..., 0]) / denom\n",
    "\tsamples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n",
    "\n",
    "\treturn samples # [n_rays, n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_hierarchical(\n",
    "\trays_o: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\tz_vals: torch.Tensor,\n",
    "\tweights: torch.Tensor,\n",
    "\tn_samples: int,\n",
    "\tperturb: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tApply hierarchical sampling to the rays.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Draw samples from PDF using z_vals as bins and weights as probabilities.\n",
    "\tz_vals_mid = .5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
    "\tnew_z_samples = sample_pdf(z_vals_mid, weights[..., 1:-1], n_samples,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tperturb=perturb)\n",
    "\tnew_z_samples = new_z_samples.detach()\n",
    "\n",
    "\t# Resample points from ray based on PDF.\n",
    "\tz_vals_combined, _ = torch.sort(torch.cat([z_vals, new_z_samples], dim=-1), dim=-1)\n",
    "\tpts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals_combined[..., :, None]  # [N_rays, N_samples + n_samples, 3]\n",
    "\treturn pts, z_vals_combined, new_z_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Forward Pass\n",
    "\n",
    "Here is where we put everything together to compute a single forward pass through our model.\n",
    "\n",
    "Due to potential memory issues, the forward pass is computed in \"chunks,\" which are then aggregated across a single batch. The gradient propagation is done after the whole batch is processed, hence the distinction between \"chunks\" and \"batches.\" Chunking is especially important for the Google Colab environment, which provides more modest resources than those cited in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(\n",
    "\tinputs: torch.Tensor,\n",
    "\tchunksize: int = 2**15\n",
    ") -> List[torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tDivide an input into chunks.\n",
    "\t\"\"\"\n",
    "\treturn [inputs[i:i + chunksize] for i in range(0, inputs.shape[0], chunksize)]\n",
    "\n",
    "def prepare_chunks(\n",
    "\tpoints: torch.Tensor,\n",
    "\tencoding_function: Callable[[torch.Tensor], torch.Tensor],\n",
    "\tchunksize: int = 2**15\n",
    ") -> List[torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tEncode and chunkify points to prepare for NeRF model.\n",
    "\t\"\"\"\n",
    "\tpoints = points.reshape((-1, 3))\n",
    "\t# print('Points:\\n',points[:].min(0)[0], points[:].max(0)[0])\n",
    "\tpoints = encoding_function(points)\n",
    "\tprint('Encoded Points:\\n',points[:3].grad)\n",
    "\tpoints = get_chunks(points, chunksize=chunksize)\n",
    "\treturn points\n",
    "\n",
    "def prepare_viewdirs_chunks(\n",
    "\tpoints: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\tencoding_function: Callable[[torch.Tensor], torch.Tensor],\n",
    "\tchunksize: int = 2**15\n",
    ") -> List[torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tEncode and chunkify viewdirs to prepare for NeRF model.\n",
    "\t\"\"\"\n",
    "\t# Prepare the viewdirs\n",
    "\tviewdirs = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
    "\tviewdirs = viewdirs[:, None, ...].expand(points.shape).reshape((-1, 3))\n",
    "\tviewdirs = encoding_function(viewdirs)\n",
    "\tviewdirs = get_chunks(viewdirs, chunksize=chunksize)\n",
    "\treturn viewdirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nerf_forward(\n",
    "\trays_o: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\tnear: float,\n",
    "\tfar: float,\n",
    "\tencoding_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "\tcoarse_model: nn.Module,\n",
    "\tkwargs_sample_stratified: dict = None,\n",
    "\tn_samples_hierarchical: int = 0,\n",
    "\tkwargs_sample_hierarchical: dict = None,\n",
    "\tfine_model = None,\n",
    "\tviewdirs_encoding_fn: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n",
    "\tchunksize: int = 2**15\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]:\n",
    "\tr\"\"\"\n",
    "\tCompute forward pass through model(s).\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Set no kwargs if none are given.\n",
    "\tif kwargs_sample_stratified is None:\n",
    "\t\tkwargs_sample_stratified = {}\n",
    "\tif kwargs_sample_hierarchical is None:\n",
    "\t\tkwargs_sample_hierarchical = {}\n",
    "\n",
    "\t# Sample query points along each ray.\n",
    "\tquery_points, z_vals = sample_stratified(\n",
    "\t\t\trays_o, rays_d, near, far, **kwargs_sample_stratified)\n",
    "\n",
    "\t# Prepare batches.\n",
    "\tbatches = prepare_chunks(query_points, encoding_fn, chunksize=chunksize)\n",
    "\tif viewdirs_encoding_fn is not None:\n",
    "\t\tbatches_viewdirs = prepare_viewdirs_chunks(query_points, rays_d,\n",
    "                                                viewdirs_encoding_fn,\n",
    "                                                chunksize=chunksize)\n",
    "\telse:\n",
    "\t\tbatches_viewdirs = [None] * len(batches)\n",
    "\n",
    "\t# Coarse model pass.\n",
    "\t# Split the encoded points into \"chunks\", run the model on all chunks, and\n",
    "\t# concatenate the results (to avoid out-of-memory issues).\n",
    "\tpredictions = []\n",
    "\tfor batch, batch_viewdirs in zip(batches, batches_viewdirs):\n",
    "\t\tpredictions.append(coarse_model(batch, viewdirs=batch_viewdirs))\n",
    "\traw = torch.cat(predictions, dim=0)\n",
    "\traw = raw.reshape(list(query_points.shape[:2]) + [raw.shape[-1]])\n",
    "\n",
    "\t# Perform differentiable volume rendering to re-synthesize the RGB image.\n",
    "\trgb_map, depth_map, acc_map, weights = raw2outputs(raw, z_vals, rays_d)\n",
    "\t# rgb_map, depth_map, acc_map, weights = render_volume_density(raw, rays_o, z_vals)\n",
    "\toutputs = {\n",
    "\t\t\t'z_vals_stratified': z_vals\n",
    "\t}\n",
    "\n",
    "\t# Fine model pass.\n",
    "\tif n_samples_hierarchical > 0:\n",
    "\t\t# Save previous outputs to return.\n",
    "\t\trgb_map_0, depth_map_0, acc_map_0 = rgb_map, depth_map, acc_map\n",
    "\n",
    "\t\t# Apply hierarchical sampling for fine query points.\n",
    "\t\tquery_points, z_vals_combined, z_hierarch = sample_hierarchical(\n",
    "\t\t\trays_o, rays_d, z_vals, weights, n_samples_hierarchical,\n",
    "\t\t\t**kwargs_sample_hierarchical)\n",
    "\n",
    "\t\t# Prepare inputs as before.\n",
    "\t\tbatches = prepare_chunks(query_points, encoding_fn, chunksize=chunksize)\n",
    "\t\tif viewdirs_encoding_fn is not None:\n",
    "\t\t\tbatches_viewdirs = prepare_viewdirs_chunks(query_points, rays_d,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t viewdirs_encoding_fn,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t chunksize=chunksize)\n",
    "\t\telse:\n",
    "\t\t\tbatches_viewdirs = [None] * len(batches)\n",
    "\n",
    "\t\t# Forward pass new samples through fine model.\n",
    "\t\tfine_model = fine_model if fine_model is not None else coarse_model\n",
    "\t\tpredictions = []\n",
    "\t\tfor batch, batch_viewdirs in zip(batches, batches_viewdirs):\n",
    "\t\t\tpredictions.append(fine_model(batch, viewdirs=batch_viewdirs))\n",
    "\t\traw = torch.cat(predictions, dim=0)\n",
    "\t\traw = raw.reshape(list(query_points.shape[:2]) + [raw.shape[-1]])\n",
    "\n",
    "\t\t# Perform differentiable volume rendering to re-synthesize the RGB image.\n",
    "\t\trgb_map, depth_map, acc_map, weights = raw2outputs(raw, z_vals_combined, rays_d)\n",
    "\n",
    "\t\t# Store outputs.\n",
    "\t\toutputs['z_vals_hierarchical'] = z_hierarch\n",
    "\t\toutputs['rgb_map_0'] = rgb_map_0\n",
    "\t\toutputs['depth_map_0'] = depth_map_0\n",
    "\t\toutputs['acc_map_0'] = acc_map_0\n",
    "\n",
    "\t# Store outputs.\n",
    "\toutputs['rgb_map'] = rgb_map\n",
    "\toutputs['depth_map'] = depth_map\n",
    "\toutputs['acc_map'] = acc_map\n",
    "\toutputs['weights'] = weights\n",
    "\treturn outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "At long last, we have (almost) everything we need to train the model. Now we will do some setup for a simple training procedure, creating hyperparameters and helper functions, then train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "All hyperparameters for training are set here. Defaults were taken from the original, unless computational constraints prohibit them. In this case, we apply sensible defaults that are well within the resources provided by Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoders\n",
    "d_input = 3           # Number of input dimensions\n",
    "n_freqs = 10          # Number of encoding functions for samples\n",
    "log_space = True      # If set, frequencies scale in log space\n",
    "use_viewdirs = True   # If set, use view direction as input\n",
    "n_freqs_views = 4     # Number of encoding functions for views\n",
    "\n",
    "# Stratified sampling\n",
    "n_samples = 64         # Number of spatial samples per ray\n",
    "perturb = True         # If set, applies noise to sample positions\n",
    "inverse_depth = False  # If set, samples points linearly in inverse depth\n",
    "\n",
    "# Model\n",
    "d_filter = 128          # Dimensions of linear layer filters\n",
    "n_layers = 2            # Number of layers in network bottleneck\n",
    "skip = [4]               # Layers at which to apply input residual\n",
    "use_fine_model = True   # If set, creates a fine model\n",
    "d_filter_fine = 128     # Dimensions of linear layer filters of fine network\n",
    "n_layers_fine = 6       # Number of layers in fine network bottleneck\n",
    "\n",
    "# Hierarchical sampling\n",
    "n_samples_hierarchical = 64   # Number of samples per ray\n",
    "perturb_hierarchical = False  # If set, applies noise to sample positions\n",
    "\n",
    "# Optimizer\n",
    "lr = 1e-5#5e-4  # Learning rate\n",
    "\n",
    "# Training\n",
    "n_iters = 2 #1000\n",
    "batch_size = 2**14          # Number of rays per gradient step (power of 2)\n",
    "one_image_per_step = True   # One image per gradient step (disables batching)\n",
    "chunksize = 2**14           # Modify as needed to fit in GPU memory\n",
    "center_crop = True          # Crop the center of image (one_image_per_)\n",
    "center_crop_iters = 0      # Stop cropping center after this many epochs\n",
    "display_rate = 1 #25          # Display test output every X epochs\n",
    "\n",
    "# Early Stopping\n",
    "warmup_iters = 100          # Number of iterations during warmup phase\n",
    "warmup_min_fitness = 10.0   # Min val PSNR to continue training at warmup_iters\n",
    "n_restarts = 10             # Number of times to restart if training stalls\n",
    "\n",
    "# We bundle the kwargs for various functions to pass all at once.\n",
    "kwargs_sample_stratified = {\n",
    "\t'n_samples': n_samples,\n",
    "\t'perturb': perturb,\n",
    "\t'inverse_depth': inverse_depth\n",
    "}\n",
    "kwargs_sample_hierarchical = {\n",
    "\t'perturb': perturb\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Classes and Functions\n",
    "\n",
    "Here we create some helper functions for training. NeRF can be prone to local minima, in which training will quickly stall and produce blank outputs. `EarlyStopping` is used to restart the training when learning stalls, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(\n",
    "\tz_vals: torch.Tensor,\n",
    "\tz_hierarch: Optional[torch.Tensor] = None,\n",
    "\tax: Optional[np.ndarray] = None):\n",
    "\tr\"\"\"\n",
    "\tPlot stratified and (optional) hierarchical samples.\n",
    "\t\"\"\"\n",
    "\ty_vals = 1 + np.zeros_like(z_vals)\n",
    "\n",
    "\tif ax is None:\n",
    "\t\tax = plt.subplot()\n",
    "\tax.plot(z_vals, y_vals, 'b-o')\n",
    "\tif z_hierarch is not None:\n",
    "\t\ty_hierarch = np.zeros_like(z_hierarch)\n",
    "\t\tax.plot(z_hierarch, y_hierarch, 'r-o')\n",
    "\tax.set_ylim([-1, 2])\n",
    "\tax.set_title('Stratified  Samples (blue) and Hierarchical Samples (red)')\n",
    "\tax.axes.yaxis.set_visible(False)\n",
    "\tax.grid(True)\n",
    "\treturn ax\n",
    "\n",
    "def crop_center(\n",
    "\timg: torch.Tensor,\n",
    "\tfrac: float = 0.5\n",
    ") -> torch.Tensor:\n",
    "\tr\"\"\"\n",
    "\tCrop center square from image.\n",
    "\t\"\"\"\n",
    "\th_offset = round(img.shape[0] * (frac / 2))\n",
    "\tw_offset = round(img.shape[1] * (frac / 2))\n",
    "\treturn img[h_offset:-h_offset, w_offset:-w_offset]\n",
    "\n",
    "class EarlyStopping:\n",
    "\tr\"\"\"\n",
    "\tEarly stopping helper based on fitness criterion.\n",
    "\t\"\"\"\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tpatience: int = 30,\n",
    "\t\tmargin: float = 1e-4\n",
    "\t):\n",
    "\t\tself.best_fitness = 0.0  # In our case PSNR\n",
    "\t\tself.best_iter = 0\n",
    "\t\tself.margin = margin\n",
    "\t\tself.patience = patience or float('inf')  # epochs to wait after fitness stops improving to stop\n",
    "\n",
    "\tdef __call__(\n",
    "\t\tself,\n",
    "\t\titer: int,\n",
    "\t\tfitness: float\n",
    "\t):\n",
    "\t\tr\"\"\"\n",
    "\t\tCheck if criterion for stopping is met.\n",
    "\t\t\"\"\"\n",
    "\t\tif (fitness - self.best_fitness) > self.margin:\n",
    "\t\t\tself.best_iter = iter\n",
    "\t\t\tself.best_fitness = fitness\n",
    "\t\tdelta = iter - self.best_iter\n",
    "\t\tstop = delta >= self.patience  # stop training if patience exceeded\n",
    "\t\treturn stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_models():\n",
    "    r\"\"\"\n",
    "    Initialize models, encoders, and optimizer for NeRF training.\n",
    "    \"\"\"\n",
    "    # Encoders\n",
    "    \n",
    "    encoder = Grid(\n",
    "        feature_dim=3,\n",
    "        grid_dim=3,\n",
    "        num_lvl=12,\n",
    "        max_res=1024*2+1024*2,\n",
    "        min_res=16,\n",
    "        hashtable_power=19,\n",
    "        device=device,\n",
    "    ).to(device)\n",
    "    encode = lambda x: encoder(x)\n",
    "    # hook = list(encoder.parameters())[0].register_hook(lambda grad: print('HOOK:\\n',grad.max(), grad.min()))\n",
    "\n",
    "    # Before backward pass\n",
    "    print(\"First check:\")\n",
    "    for name, param in encoder.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            print(f\"{name} gradient before backward: {param.grad.max()}, {param.grad.min()}\")\n",
    "\n",
    "\n",
    "    # encoder = PositionalEncoder(d_input, n_freqs, log_space=log_space)\n",
    "    # encode = lambda x: encoder(x)\n",
    "\n",
    "    # View direction encoders\n",
    "    if use_viewdirs:\n",
    "        encoder_viewdirs = PositionalEncoder(d_input, n_freqs_views,log_space=log_space)\n",
    "        encode_viewdirs = lambda x: encoder_viewdirs(x)\n",
    "        d_viewdirs = encoder_viewdirs.d_output\n",
    "    else:\n",
    "        encode_viewdirs = None\n",
    "        d_viewdirs = None\n",
    "\n",
    "    # Models\n",
    "    model = NeRF(encoder.d_output, n_layers=n_layers, d_filter=d_filter, skip=skip,\n",
    "                            d_viewdirs=d_viewdirs)\n",
    "    model.to(device)\n",
    "    model_params = list(model.parameters())\n",
    "    if use_fine_model:\n",
    "        fine_model = NeRF(encoder.d_output, n_layers=n_layers, d_filter=d_filter, skip=skip,\n",
    "                                            d_viewdirs=d_viewdirs)\n",
    "        fine_model.to(device)\n",
    "        model_params = model_params + list(fine_model.parameters())\n",
    "    else:\n",
    "        fine_model = None\n",
    "\n",
    "    model_params = model_params + list(encoder.parameters())\n",
    "\n",
    "    # Optimizer\n",
    "    # optimizer = torch.optim.Adam(model_params, lr=lr)\n",
    "\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters(), 'lr': 1e-4},\n",
    "        {'params': fine_model.parameters(), 'lr': 1e-4},\n",
    "        {'params': encoder.parameters(), 'lr': 5e-4},\n",
    "    ])\n",
    "\n",
    "    # Early Stopping\n",
    "    warmup_stopper = EarlyStopping(patience=50)\n",
    "\n",
    "    return model, fine_model, encode, encode_viewdirs, optimizer, warmup_stopper, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Here we start training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_time(i, i_batch, encoder, rays_rgb=None):\n",
    "    model.train()\n",
    "\n",
    "    if one_image_per_step:\n",
    "        # Randomly pick an image as the target.\n",
    "        # target_img_idx = np.random.randint(images.shape[0])\n",
    "        target_img_idx = testimg_idx\n",
    "\n",
    "        target_img = images[target_img_idx].to(device)\n",
    "        if center_crop and i < center_crop_iters:\n",
    "            target_img = crop_center(target_img)\n",
    "        height, width = target_img.shape[:2]\n",
    "        target_pose = poses[target_img_idx].to(device)\n",
    "        rays_o, rays_d = get_rays(height, width, focal, target_pose)\n",
    "        rays_o = rays_o.reshape([-1, 3]).to(device)\n",
    "        rays_d = rays_d.reshape([-1, 3]).to(device)\n",
    "    else:\n",
    "        # Use batching\n",
    "        # Random over all images.\n",
    "        batch = rays_rgb[i_batch:i_batch + batch_size]\n",
    "        batch = torch.tensor(batch)\n",
    "        batch = torch.transpose(batch, 0, 1).to(device)\n",
    "        rays_o, rays_d, target_img = batch\n",
    "        height, width = target_img.shape[:2]\n",
    "        i_batch += batch_size\n",
    "        # Shuffle after one epoch\n",
    "        if i_batch >= rays_rgb.shape[0]:\n",
    "                rays_rgb = rays_rgb[torch.randperm(rays_rgb.shape[0])]\n",
    "                i_batch = 0\n",
    "    target_img = target_img.reshape([-1, 3]).to(device)\n",
    "\n",
    "    # Run one iteration of TinyNeRF and get the rendered RGB image.\n",
    "    outputs = nerf_forward(\n",
    "        rays_o, rays_d,\n",
    "                        near, far, encode, model,\n",
    "                        kwargs_sample_stratified=kwargs_sample_stratified,\n",
    "                        n_samples_hierarchical=n_samples_hierarchical,\n",
    "                        kwargs_sample_hierarchical=kwargs_sample_hierarchical,\n",
    "                        fine_model=fine_model,\n",
    "                        viewdirs_encoding_fn=encode_viewdirs,\n",
    "                        chunksize=chunksize)\n",
    "\n",
    "\n",
    "    # print('Output Features:', outputs['rgb_map'][:10])\n",
    "\n",
    "    # Check for any numerical issues.\n",
    "    for k, v in outputs.items():\n",
    "        if torch.isnan(v).any():\n",
    "            print(f\"! [Numerical Alert] {k} contains NaN.\")\n",
    "        if torch.isinf(v).any():\n",
    "            print(f\"! [Numerical Alert] {k} contains Inf.\")\n",
    "\n",
    "    # Backprop!\n",
    "    rgb_predicted = outputs['rgb_map']\n",
    "    loss = torch.nn.functional.mse_loss(rgb_predicted, target_img)\n",
    "    loss.backward()\n",
    "\n",
    "    # Debug gradient values\n",
    "    print('GRID GRAD:')\n",
    "    for p in encoder.parameters():\n",
    "        if p.grad is not None:\n",
    "            print(p.grad.min(), p.grad.max())\n",
    "        else:\n",
    "            print(\"No gradient for parameter:\", p)    \n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    psnr = -10. * torch.log10(loss)\n",
    "    \n",
    "    return psnr.item(), i_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_time(i, iternums, train_psnrs, val_psnrs):\n",
    "\tmodel.eval()\n",
    "\theight, width = testimg.shape[:2]\n",
    "\trays_o, rays_d = get_rays(height, width, focal, testpose)\n",
    "\trays_o = rays_o.reshape([-1, 3]).to(device)\n",
    "\trays_d = rays_d.reshape([-1, 3]).to(device)\n",
    "\toutputs = nerf_forward(rays_o, rays_d,\n",
    "\t\t\t\t\t\t\tnear, far, encode, model,\n",
    "\t\t\t\t\t\t\tkwargs_sample_stratified=kwargs_sample_stratified,\n",
    "\t\t\t\t\t\t\tn_samples_hierarchical=n_samples_hierarchical,\n",
    "\t\t\t\t\t\t\tkwargs_sample_hierarchical=kwargs_sample_hierarchical,\n",
    "\t\t\t\t\t\t\tfine_model=fine_model,\n",
    "\t\t\t\t\t\t\tviewdirs_encoding_fn=encode_viewdirs,\n",
    "\t\t\t\t\t\t\tchunksize=chunksize)\n",
    "\n",
    "\trgb_predicted = outputs['rgb_map']\n",
    "\ttestimg_flat = testimg.reshape(-1, 3).to(device)\n",
    "\tloss = torch.nn.functional.mse_loss(rgb_predicted, testimg_flat)\n",
    "\tprint(\"Loss:\", loss.item())\n",
    "\tval_psnr = -10. * torch.log10(loss)\n",
    "\tval_psnrs.append(val_psnr.item())\n",
    "\titernums.append(i)\n",
    "\n",
    "\t# Plot example outputs outside\n",
    "\trender(i, rgb_predicted, iternums, outputs,train_psnrs,val_psnrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def render(i, rgb_predicted, iternums, outputs,train_psnrs,val_psnrs):\n",
    "\tfig, ax = plt.subplots(1, 4, figsize=(24,4), gridspec_kw={'width_ratios': [1, 1, 1, 3]})\n",
    "    # Plot example outputs\n",
    "\t\n",
    "\tax[0].imshow(rgb_predicted.reshape([height, width, 3]).detach().cpu().numpy())\n",
    "\tax[0].set_title(f'Iteration: {i}')\n",
    "\tax[1].imshow(testimg.detach().cpu().numpy())\n",
    "\tax[1].set_title(f'Target')\n",
    "\tax[2].plot(range(0, i + 1), train_psnrs, 'r')\n",
    "\tax[2].plot(iternums, val_psnrs, 'b')\n",
    "\tax[2].set_title('PSNR (train=red, val=blue')\n",
    "\tz_vals_strat = outputs['z_vals_stratified'].view((-1, n_samples))\n",
    "\tz_sample_strat = z_vals_strat[z_vals_strat.shape[0] // 2].detach().cpu().numpy()\n",
    "\tif 'z_vals_hierarchical' in outputs:\n",
    "\t\tz_vals_hierarch = outputs['z_vals_hierarchical'].view((-1, n_samples_hierarchical))\n",
    "\t\tz_sample_hierarch = z_vals_hierarch[z_vals_hierarch.shape[0] // 2].detach().cpu().numpy()\n",
    "\telse:\n",
    "\t\tz_sample_hierarch = None\n",
    "\t_ = plot_samples(z_sample_strat, z_sample_hierarch, ax=ax[3])\n",
    "\tax[3].margins(0)\n",
    "\tplt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, fine_model, encode, encode_viewdirs, optimizer, warmup_stopper, encoder):\n",
    "\tr\"\"\"\n",
    "\tLaunch training session for NeRF.\n",
    "\t\"\"\"\n",
    "\t# Shuffle rays across all images.\n",
    "\ti_batch = 0\n",
    "\trays_rgb = None\n",
    "\tif not one_image_per_step:\n",
    "\t\theight, width = images.shape[1:3]\n",
    "\t\tall_rays = torch.stack([torch.stack(get_rays(height, width, focal, p), 0)\n",
    "\t\t\t\tfor p in poses[:n_training]], 0)\n",
    "\t\trays_rgb = torch.cat([all_rays, images[:, None]], 1)\n",
    "\t\trays_rgb = torch.permute(rays_rgb, [0, 2, 3, 1, 4])\n",
    "\t\trays_rgb = rays_rgb.reshape([-1, 3, 3])\n",
    "\t\trays_rgb = rays_rgb.type(torch.float32)\n",
    "\t\trays_rgb = rays_rgb[torch.randperm(rays_rgb.shape[0])]\n",
    "\t\t\n",
    "\n",
    "\ttrain_psnrs = []\n",
    "\tval_psnrs = []\n",
    "\titernums = []\n",
    "\t\n",
    "\tfor i in trange(n_iters):\n",
    "\t\tpsnr, i_batch = train_one_time(i, i_batch, encoder, rays_rgb)\n",
    "\t\ttrain_psnrs.append(psnr)\n",
    "\n",
    "\t\t# Evaluate testimg at given display rate.\n",
    "\t\tif i % display_rate == 0:\n",
    "\t\t\teval_one_time(i, iternums, train_psnrs, val_psnrs)\n",
    "\n",
    "\t\t# Check PSNR for issues and stop if any are found.\n",
    "\t\tval_psnr = val_psnrs[-1]\n",
    "\t\tif i == warmup_iters - 1:\n",
    "\t\t\tif val_psnr < warmup_min_fitness:\n",
    "\t\t\t\tprint(f'Val PSNR {val_psnr} below warmup_min_fitness {warmup_min_fitness}. Stopping...')\n",
    "\t\t\t\treturn False, train_psnrs, val_psnrs\n",
    "\t\telif i < warmup_iters:\n",
    "\t\t\tif warmup_stopper is not None and warmup_stopper(i, psnr):\n",
    "\t\t\t\tprint(f'Train PSNR flatlined at {psnr} for {warmup_stopper.patience} iters. Stopping...')\n",
    "\t\t\t\treturn False, train_psnrs, val_psnrs\n",
    "\tprint('val_psnrs',val_psnrs)\n",
    "\treturn True, train_psnrs, val_psnrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training session(s)\n",
    "for _ in range(n_restarts):\n",
    "\tmodel, fine_model, encode, encode_viewdirs, optimizer, warmup_stopper, encoder = init_models()\n",
    "\tsuccess, train_psnrs, val_psnrs = train(model, fine_model, encode, encode_viewdirs, optimizer, warmup_stopper, encoder)\n",
    "\tif success and val_psnrs[-1] >= warmup_min_fitness:\n",
    "\t\tprint('Training successful!')\n",
    "\t\tbreak\n",
    "\n",
    "print('')\n",
    "print(f'Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'nerf.tensor')\n",
    "torch.save(fine_model.state_dict(), 'nerf-fine.tensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Plotting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnerf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
