{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465fcfa2-71c8-4eb5-8c55-fbf3770cba3d",
   "metadata": {},
   "source": [
    "Notes:  \n",
    "- Number of levels for grid encoder seems to be main reason for instability of training. More levels is solving the issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221cebb6-005e-4826-9b7b-ccd1af781613",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25a08c2-1966-4ed0-b23a-05e812d0c0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from typing import Optional, Tuple, List, Union, Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import axes3d\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# For repeatability\n",
    "seed = 2024\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "380a9fc8-d1a8-491a-b1ce-9931a10501a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Select training parameters\n",
    "###\n",
    "# Encoders\n",
    "from enum import Enum\n",
    "Encoders = Enum('Encoder', [\"NONE\",\"FREQ\", \"HASH\"]) \n",
    "encoder_used = Encoders.HASH\n",
    "\n",
    "## Freq encoder\n",
    "d_input = 3           # Number of input dimensions\n",
    "n_freqs = 10          # Number of encoding functions for samples\n",
    "log_space = True      # If set, frequencies scale in log space\n",
    "use_viewdirs = True   # If set, use view direction as input\n",
    "n_freqs_views = 4     # Number of encoding functions for views\n",
    "n_freqs_time = 4     # Number of encoding functions for time\n",
    "\n",
    "## Hash encoder\n",
    "\n",
    "\n",
    "# Stratified sampling\n",
    "n_samples = 32         # Number of spatial samples per ray\n",
    "perturb = True         # If set, applies noise to sample positions\n",
    "inverse_depth = False  # If set, samples points linearly in inverse depth\n",
    "\n",
    "# Model\n",
    "d_filter = 128          # Dimensions of linear layer filters\n",
    "n_layers = 3#3            # Number of layers in network bottleneck\n",
    "skip = []#[4]              # Layers at which to apply input residual\n",
    "use_fine_model = True   # If set, creates a fine model\n",
    "d_filter_fine = 128     # Dimensions of linear layer filters of fine network\n",
    "n_layers_fine = 4#6       # Number of layers in fine network bottleneck\n",
    "\n",
    "# Hierarchical sampling\n",
    "n_samples_hierarchical = 64   # Number of samples per ray\n",
    "perturb_hierarchical = False  # If set, applies noise to sample positions\n",
    "\n",
    "near, far = 2., 6.\n",
    "\n",
    "# Optimizer\n",
    "lr = 1e-4  # Learning rate\n",
    "lr_hash_enc = 5e-4\n",
    "scheduler_start_end_factors = [1.0, 0.9] # Linear decay of learning rate\n",
    "\n",
    "# Training\n",
    "n_epochs = 3000\n",
    "batch_size = 2**14          # Number of rays per gradient step (power of 2)\n",
    "one_image_per_step = True   # One image per gradient step (disables batching)\n",
    "chunksize = 2**14           # Modify as needed to fit in GPU memory\n",
    "center_crop = True          # Crop the center of image (one_image_per_)\n",
    "center_crop_iters = 0      # Stop cropping center after this many epochs\n",
    "display_rate = 50          # Display test output every X epochs\n",
    "shuffle_data = True\t\t    # Shuffle rays on every iteration. Mostly for debug. \n",
    "\n",
    "# Early Stopping\n",
    "warmup_iters = 100          # Number of iterations during warmup phase\n",
    "warmup_min_fitness = 10.0   # Min val PSNR to continue training at warmup_iters\n",
    "n_restarts = 10             # Number of times to restart if training stalls\n",
    "\n",
    "# We bundle the kwargs for various functions to pass all at once.\n",
    "kwargs_sample_stratified = {\n",
    "\t'n_samples': n_samples,\n",
    "\t'perturb': perturb,\n",
    "\t'inverse_depth': inverse_depth\n",
    "}\n",
    "kwargs_sample_hierarchical = {\n",
    "\t'perturb': perturb\n",
    "}\n",
    "\n",
    "test_save_dir = f'./results/{encoder_used.name}/'\n",
    "os.makedirs(test_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b48f672-45f1-4955-bb27-00d2617b386d",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a294374-d977-45ce-ad7b-6d2cb10b6297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists('./data/tiny_dnerf_data.npz'):\n",
    "\t# !wget https://github.com/ivanvoid/Tiny_Kilo-D-NeRF-NGP/blob/main/tiny_dnerf_data.npz\n",
    "    # !wget https://github.com/ivanvoid/Tiny_Kilo-D-NeRF-NGP/blob/8091c93e53f9c77ccc84cd275aad28171d21a928/tiny_dnerf_data.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33f3bbe5-761c-4334-9584-8cbb84bc7d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the file a valid zipfile? True\n",
      "File Name                                             Modified             Size\n",
      "images.npy                                     1980-01-01 00:00:00      9720128\n",
      "poses.npy                                      1980-01-01 00:00:00         5312\n",
      "times.npy                                      1980-01-01 00:00:00          452\n",
      "focal.npy                                      1980-01-01 00:00:00          136\n"
     ]
    }
   ],
   "source": [
    "# data = np.load('tiny_dnerf_data.npz')\n",
    "# data = np.load('tiny_dnerf_data.npz', allow_pickle=True)\n",
    "import zipfile\n",
    "\n",
    "# Check if the file is a valid zip file\n",
    "is_zipfile = zipfile.is_zipfile('../data/tiny_dnerf_data.npz')\n",
    "print(f\"Is the file a valid zipfile? {is_zipfile}\")\n",
    "\n",
    "# If it is a valid zip file, inspect the contents\n",
    "if is_zipfile:\n",
    "    with zipfile.ZipFile('../data/tiny_dnerf_data.npz', 'r') as zip_ref:\n",
    "        zip_ref.printdir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccb6a26e-a03c-49ad-837b-b98a4a23679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('../data/tiny_dnerf_data.npz')\n",
    "\n",
    "n_training = 79\n",
    "n_training = 79\n",
    "testimg_idx = 80\n",
    "\n",
    "images = torch.from_numpy(data['images'][:n_training,:,:,:3])\n",
    "poses = torch.from_numpy(data['poses'][:n_training])\n",
    "focal = torch.from_numpy(data['focal'])\n",
    "times = torch.from_numpy(data['times'][:n_training])\n",
    "\n",
    "# Test\n",
    "testimg = torch.from_numpy(data['images'][testimg_idx,:,:,:3])\n",
    "testpose = torch.from_numpy(data['poses'][testimg_idx])\n",
    "testtime = torch.from_numpy(data['times'][testimg_idx:])\n",
    "\n",
    "height, width = images.shape[1:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26743d62-c04e-4973-98a8-ee49a1ce4a21",
   "metadata": {},
   "source": [
    "# Rays and Ray Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe651120-e0b1-4920-8157-be6b748303b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rays(\n",
    "\theight: int,\n",
    "\twidth: int,\n",
    "\tfocal_length: float,\n",
    "\tc2w: torch.Tensor\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tFind origin and direction of rays through every pixel and camera origin.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Apply pinhole camera model to gather directions at each pixel\n",
    "\ti, j = torch.meshgrid(\n",
    "\t\t\ttorch.arange(width, dtype=torch.float32).to(c2w),\n",
    "\t\t\ttorch.arange(height, dtype=torch.float32).to(c2w),\n",
    "\t\t\tindexing='ij')\n",
    "\ti, j = i.transpose(-1, -2), j.transpose(-1, -2)\n",
    "\tdirections = torch.stack([(i - width * .5) / focal_length,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t-(j - height * .5) / focal_length,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t-torch.ones_like(i)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t ], dim=-1)\n",
    "\n",
    "\t# Apply camera pose to directions\n",
    "\trays_d = torch.sum(directions[..., None, :] * c2w[:3, :3], dim=-1)\n",
    "\n",
    "\t# Origin is same for all directions (the optical center)\n",
    "\trays_o = c2w[:3, -1].expand(rays_d.shape)\n",
    "\treturn rays_o, rays_d\n",
    "\t\n",
    "def sample_stratified(\n",
    "\trays_o: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\tnear: float,\n",
    "\tfar: float,\n",
    "\tn_samples: int,\n",
    "\tperturb: Optional[bool] = True,\n",
    "\tinverse_depth: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tSample along ray from regularly-spaced bins.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Grab samples for space integration along ray\n",
    "\tt_vals = torch.linspace(0., 1., n_samples, device=rays_o.device)\n",
    "\tif not inverse_depth:\n",
    "\t\t# Sample linearly between `near` and `far`\n",
    "\t\tz_vals = near * (1.-t_vals) + far * (t_vals)\n",
    "\telse:\n",
    "\t\t# Sample linearly in inverse depth (disparity)\n",
    "\t\tz_vals = 1./(1./near * (1.-t_vals) + 1./far * (t_vals))\n",
    "\n",
    "\t# Draw uniform samples from bins along ray\n",
    "\tif perturb:\n",
    "\t\tmids = .5 * (z_vals[1:] + z_vals[:-1])\n",
    "\t\tupper = torch.concat([mids, z_vals[-1:]], dim=-1)\n",
    "\t\tlower = torch.concat([z_vals[:1], mids], dim=-1)\n",
    "\t\tt_rand = torch.rand([n_samples], device=z_vals.device)\n",
    "\t\tz_vals = lower + (upper - lower) * t_rand\n",
    "\tz_vals = z_vals.expand(list(rays_o.shape[:-1]) + [n_samples])\n",
    "\n",
    "\t# Apply scale from `rays_d` and offset from `rays_o` to samples\n",
    "\t# pts: (width, height, n_samples, 3)\n",
    "\tpts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals[..., :, None]\n",
    "\treturn pts, z_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c7ba44-e1a5-41e7-8876-11257d164e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pdf(\n",
    "\tbins: torch.Tensor,\n",
    "\tweights: torch.Tensor,\n",
    "\tn_samples: int,\n",
    "\tperturb: bool = False\n",
    ") -> torch.Tensor:\n",
    "\tr\"\"\"\n",
    "\tApply inverse transform sampling to a weighted set of points.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Normalize weights to get PDF.\n",
    "\tpdf = (weights + 1e-5) / torch.sum(weights + 1e-5, -1, keepdims=True) # [n_rays, weights.shape[-1]]\n",
    "\n",
    "\t# Convert PDF to CDF.\n",
    "\tcdf = torch.cumsum(pdf, dim=-1) # [n_rays, weights.shape[-1]]\n",
    "\tcdf = torch.concat([torch.zeros_like(cdf[..., :1]), cdf], dim=-1) # [n_rays, weights.shape[-1] + 1]\n",
    "\n",
    "\t# Take sample positions to grab from CDF. Linear when perturb == 0.\n",
    "\tif not perturb:\n",
    "\t\tu = torch.linspace(0., 1., n_samples, device=cdf.device)\n",
    "\t\tu = u.expand(list(cdf.shape[:-1]) + [n_samples]) # [n_rays, n_samples]\n",
    "\telse:\n",
    "\t\tu = torch.rand(list(cdf.shape[:-1]) + [n_samples], device=cdf.device) # [n_rays, n_samples]\n",
    "\n",
    "\t# Find indices along CDF where values in u would be placed.\n",
    "\tu = u.contiguous() # Returns contiguous tensor with same values.\n",
    "\tinds = torch.searchsorted(cdf, u, right=True) # [n_rays, n_samples]\n",
    "\n",
    "\t# Clamp indices that are out of bounds.\n",
    "\tbelow = torch.clamp(inds - 1, min=0)\n",
    "\tabove = torch.clamp(inds, max=cdf.shape[-1] - 1)\n",
    "\tinds_g = torch.stack([below, above], dim=-1) # [n_rays, n_samples, 2]\n",
    "\n",
    "\t# Sample from cdf and the corresponding bin centers.\n",
    "\tmatched_shape = list(inds_g.shape[:-1]) + [cdf.shape[-1]]\n",
    "\tcdf_g = torch.gather(cdf.unsqueeze(-2).expand(matched_shape), dim=-1,\n",
    "\t\t\t\t\t\t\t\t\t\t\t index=inds_g)\n",
    "\tbins_g = torch.gather(bins.unsqueeze(-2).expand(matched_shape), dim=-1,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tindex=inds_g)\n",
    "\n",
    "\t# Convert samples to ray length.\n",
    "\tdenom = (cdf_g[..., 1] - cdf_g[..., 0])\n",
    "\tdenom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n",
    "\tt = (u - cdf_g[..., 0]) / denom\n",
    "\tsamples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n",
    "\n",
    "\treturn samples # [n_rays, n_samples]\n",
    "\n",
    "def sample_hierarchical(\n",
    "\trays_o: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\tz_vals: torch.Tensor,\n",
    "\tweights: torch.Tensor,\n",
    "\tn_samples: int,\n",
    "\tperturb: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tApply hierarchical sampling to the rays.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Draw samples from PDF using z_vals as bins and weights as probabilities.\n",
    "\tz_vals_mid = .5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
    "\tnew_z_samples = sample_pdf(z_vals_mid, weights[..., 1:-1], n_samples,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tperturb=perturb)\n",
    "\tnew_z_samples = new_z_samples.detach()\n",
    "\n",
    "\t# Resample points from ray based on PDF.\n",
    "\tz_vals_combined, _ = torch.sort(torch.cat([z_vals, new_z_samples], dim=-1), dim=-1)\n",
    "\tpts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals_combined[..., :, None]  # [N_rays, N_samples + n_samples, 3]\n",
    "\treturn pts, z_vals_combined, new_z_samples\n",
    "\n",
    "def cumprod_exclusive(\n",
    "\ttensor: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "\tr\"\"\"\n",
    "\t(Courtesy of https://github.com/krrish94/nerf-pytorch)\n",
    "\n",
    "\tMimick functionality of tf.math.cumprod(..., exclusive=True), as it isn't available in PyTorch.\n",
    "\n",
    "\tArgs:\n",
    "\ttensor (torch.Tensor): Tensor whose cumprod (cumulative product, see `torch.cumprod`) along dim=-1\n",
    "\t\tis to be computed.\n",
    "\tReturns:\n",
    "\tcumprod (torch.Tensor): cumprod of Tensor along dim=-1, mimiciking the functionality of\n",
    "\t\ttf.math.cumprod(..., exclusive=True) (see `tf.math.cumprod` for details).\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Compute regular cumprod first (this is equivalent to `tf.math.cumprod(..., exclusive=False)`).\n",
    "\tcumprod = torch.cumprod(tensor, -1)\n",
    "\t# \"Roll\" the elements along dimension 'dim' by 1 element.\n",
    "\tcumprod = torch.roll(cumprod, 1, -1)\n",
    "\t# Replace the first element by \"1\" as this is what tf.cumprod(..., exclusive=True) does.\n",
    "\tcumprod[..., 0] = 1.\n",
    "\n",
    "\treturn cumprod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088c6761-4f10-4ef2-9418-53f906bf0200",
   "metadata": {},
   "source": [
    "# Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b15a5e9-19d6-49f2-ac24-07a5a6ea7239",
   "metadata": {},
   "source": [
    "## Frequency encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561ad2db-3b9c-46f2-a76a-ef880b260cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "\tr\"\"\"\n",
    "\tSine-cosine positional encoder for input points.\n",
    "\t\"\"\"\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\td_input: int,\n",
    "\t\tn_freqs: int,\n",
    "\t\tlog_space: bool = False\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.d_input = d_input\n",
    "\t\tself.n_freqs = n_freqs\n",
    "\t\tself.log_space = log_space\n",
    "\t\tself.d_output = d_input * (1 + 2 * self.n_freqs)\n",
    "\t\tself.embed_fns = [lambda x: x]\n",
    "\n",
    "\t\t# Define frequencies in either linear or log scale\n",
    "\t\tif self.log_space:\n",
    "\t\t\tfreq_bands = 2.**torch.linspace(0., self.n_freqs - 1, self.n_freqs)\n",
    "\t\telse:\n",
    "\t\t\tfreq_bands = torch.linspace(2.**0., 2.**(self.n_freqs - 1), self.n_freqs)\n",
    "\n",
    "\t\t# Alternate sin and cos\n",
    "\t\tfor freq in freq_bands:\n",
    "\t\t\tself.embed_fns.append(lambda x, freq=freq: torch.sin(x * freq))\n",
    "\t\t\tself.embed_fns.append(lambda x, freq=freq: torch.cos(x * freq))\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tx\n",
    "\t) -> torch.Tensor:\n",
    "\t\tr\"\"\"\n",
    "\t\tApply positional encoding to input.\n",
    "\t\t\"\"\"\n",
    "\t\treturn torch.concat([fn(x) for fn in self.embed_fns], dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddb2b78-c954-446d-bd76-f7c2a29a163b",
   "metadata": {},
   "source": [
    "## Hash encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8b7949-c824-4547-a90b-a2e12471693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network grid class. The input x needs to be within [0, 1].\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 feature_dim: int,\n",
    "                 grid_dim: int,\n",
    "                 num_lvl: int,\n",
    "                 max_res: int,\n",
    "                 min_res: int,\n",
    "                 hashtable_power: int,\n",
    "                 device='cpu'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Determine the device to use (CPU or CUDA)\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize the attributes of the grid\n",
    "        self.feature_dim = feature_dim  # Dimensionality of the feature vectors\n",
    "        self.grid_dim = grid_dim  # Dimensionality of the grid (e.g., 2D, 3D)\n",
    "        self.num_lvl = num_lvl  # Number of levels in the grid hierarchy\n",
    "        self.max_res = max_res  # Maximum resolution of the grid\n",
    "        self.min_res = min_res  # Minimum resolution of the grid\n",
    "        self.hashtable_power = hashtable_power  # Power of the hashtable size (number of entries is 2^hashtable_power)\n",
    "\n",
    "        self.d_output = num_lvl * grid_dim\n",
    "\n",
    "        # Constants for hash calculations\n",
    "        self.prime = [3367900313, 2654435761, 805459861]  # Prime numbers for hashing\n",
    "        self.max_entry = 2 ** self.hashtable_power  # Maximum number of entries in the hashtable\n",
    "        # Factor to scale resolutions logarithmically\n",
    "        self.factor_b = np.exp((np.log(self.max_res) - np.log(self.min_res)) / (self.num_lvl - 1))\n",
    "\n",
    "        # Compute the resolutions for each level\n",
    "        self.resolutions = []\n",
    "        for i in range(self.num_lvl):\n",
    "            # Calculate resolution for level i\n",
    "            self.resolutions.append(np.floor(self.min_res * (self.factor_b ** i)))\n",
    "\n",
    "        # Initialize the hashtable for each resolution\n",
    "        self.hashtable = nn.ParameterList([])  # List of hashtables for each resolution\n",
    "        for res in self.resolutions:\n",
    "            total_res = res ** self.grid_dim  # Total number of cells at this resolution\n",
    "            table_size = min(total_res, self.max_entry)  # Size of the hashtable (limited by max_entry)\n",
    "            # Initialize table with random values, scaled as per InstantNGP paper\n",
    "            table = torch.randn(int(table_size), self.feature_dim, device=self.device) *0.001 #* 0.0001 #+ torch.rand(1).to(self.device)\n",
    "            table = nn.Parameter(table)  # Convert to a learnable parameter\n",
    "            self.hashtable.append(table)  # Add to the hashtable list\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalization\n",
    "        _min = torch.tensor([-3.5552, -2.1935, -2.8307]).to(self.device)\n",
    "        _max = torch.tensor([1.8859, 3.1777, 2.0705]).to(self.device)\n",
    "        x = (x - _min ) / (_max - _min)\n",
    "\n",
    "        # print(x.min(0)[0], x.max(0)[0])\n",
    "        \n",
    "        out_feature = []\n",
    "        for lvl in range(self.num_lvl):\n",
    "            # Transform coordinates to hash space\n",
    "            coord = self.to_hash_space(x, self.resolutions[lvl])\n",
    "            floor_corner = torch.floor(coord)  # Find the floor corner for interpolation\n",
    "            # Get the corners for interpolation\n",
    "            corners = self.get_corner(floor_corner).to(torch.long)\n",
    "            # Hash the corners to get feature indices\n",
    "            feature_index = self.hash(corners, self.hashtable[lvl].shape[0], self.resolutions[lvl])\n",
    "            flat_feature_index = feature_index.to(torch.long).flatten()  # Flatten the indices\n",
    "            # Retrieve corner features from the hashtable\n",
    "            corner_feature = torch.reshape(self.hashtable[lvl][flat_feature_index],\n",
    "                                           (corners.shape[0], corners.shape[1], self.feature_dim))\n",
    "            # Calculate interpolation weights\n",
    "            weights = self.interpolation_weights(coord - floor_corner)\n",
    "            weights = torch.stack([weights, weights, weights], -1)  # Stack weights for each feature\n",
    "            # Perform weighted interpolation of corner features\n",
    "            weighted_feature = corner_feature * weights\n",
    "            summed_feature = weighted_feature.sum(-2)  # Sum the weighted features\n",
    "            out_feature.append(summed_feature)  # Append the result to the output feature list\n",
    "        return torch.cat(out_feature, -1)  # Concatenate features from all levels\n",
    "\n",
    "    def to_hash_space(self, x, resolution):\n",
    "        \"\"\"\n",
    "        Transform input coordinates to hash space, ensuring they are within the grid's resolution.\n",
    "        \"\"\"\n",
    "        return torch.clip(x * (resolution - 1), 0, resolution - 1.0001)  # Scale and clip coordinates\n",
    "\n",
    "    def interpolation_weights(self, diff):\n",
    "        \"\"\"\n",
    "        Calculate the interpolation weights based on the differences from the floor corner.\n",
    "        \"\"\"\n",
    "        ones = torch.ones_like(diff, device=self.device)  # Create a tensor of ones with the same shape as diff\n",
    "        minus_x = (ones - diff)[..., 0]  # Calculate 1 - x for each dimension\n",
    "        x = diff[..., 0]  # Get the x difference\n",
    "        minus_y = (ones - diff)[..., 1]  # Calculate 1 - y for each dimension\n",
    "        y = diff[..., 1]  # Get the y difference\n",
    "\n",
    "        if self.grid_dim == 2:\n",
    "            # For 2D, calculate weights for the four corners\n",
    "            stacks = torch.stack([minus_x * minus_y, x * minus_y, minus_x * y, x * y], -1)\n",
    "            return stacks\n",
    "        else:\n",
    "            # For 3D, calculate weights for the eight corners\n",
    "            minus_z = (ones - diff)[..., 2]  # Calculate 1 - z for each dimension\n",
    "            z = diff[..., 2]  # Get the z difference\n",
    "            stacks = torch.stack([minus_x * minus_y * minus_z,\n",
    "                                  x * minus_y * minus_z,\n",
    "                                  minus_x * y * minus_z,\n",
    "                                  x * y * minus_z,\n",
    "                                  minus_x * minus_y * z,\n",
    "                                  x * minus_y * z,\n",
    "                                  minus_x * y * z,\n",
    "                                  x * y * z], -1)\n",
    "            return stacks\n",
    "\n",
    "    def alt_weights(self, corner, coord):\n",
    "        \"\"\"\n",
    "        Alternative method for calculating weights based on the distance to the corners.\n",
    "        \"\"\"\n",
    "        diag_length = torch.full_like(coord[:, 0], 2. ** (1 / 2), device=self.device)  # Diagonal length for normalization\n",
    "        w = torch.empty(corner.shape[0], corner.shape[1], device=self.device)  # Initialize weight tensor\n",
    "        for c in range(corner.shape[1]):\n",
    "            dist = torch.norm(corner[:, c, :] - coord, dim=1)  # Calculate distance to each corner\n",
    "            w[:, c] = diag_length - dist  # Calculate weight based on distance\n",
    "        normed_w = torch.nn.functional.normalize(w, p=1)  # Normalize the weights\n",
    "        return normed_w\n",
    "\n",
    "    def hash(self, x, num_entry, res):\n",
    "        \"\"\"\n",
    "        Hash function to map coordinates to hashtable indices.\n",
    "        \"\"\"\n",
    "        if num_entry != self.max_entry:\n",
    "            # For smaller hashtables, use a simple linear hash\n",
    "            index = 0\n",
    "            for i in range(self.grid_dim):\n",
    "                index += x[..., i] * res ** i\n",
    "            return index\n",
    "        else:\n",
    "            # For larger hashtables, use a more complex hash with primes\n",
    "            _sum = 0\n",
    "            for i in range(self.grid_dim):\n",
    "                _sum = _sum ^ (x[..., i] * self.prime[i])\n",
    "            index = _sum % num_entry  # Modulo operation to keep within table size\n",
    "            return index\n",
    "\n",
    "    def get_corner(self, floor_corner):\n",
    "        \"\"\"\n",
    "        Get the corner points for interpolation based on the floor corner.\n",
    "        \"\"\"\n",
    "        num_entry = floor_corner.shape[0]  # Number of entries\n",
    "\n",
    "        if self.grid_dim == 2:\n",
    "            # Calculate corners for 2D grids\n",
    "            c000 = floor_corner\n",
    "            c001 = floor_corner + torch.tensor([0, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c010 = floor_corner + torch.tensor([1, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c011 = floor_corner + torch.ones_like(floor_corner, device=self.device)\n",
    "            stacks = torch.stack([c000, c010, c001, c011], -2)\n",
    "            return stacks\n",
    "        else:\n",
    "            # Calculate corners for 3D grids\n",
    "            c000 = floor_corner\n",
    "            c001 = floor_corner + torch.tensor([0, 0, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c010 = floor_corner + torch.tensor([0, 1, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c011 = floor_corner + torch.tensor([0, 1, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c100 = floor_corner + torch.tensor([1, 0, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c101 = floor_corner + torch.tensor([1, 0, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c110 = floor_corner + torch.tensor([1, 1, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c111 = floor_corner + torch.ones_like(floor_corner, device=self.device)\n",
    "            stacks = torch.stack([c000, c010, c001, c011, c100, c101, c110, c111], -2)\n",
    "            return stacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14eb091-1f08-4a17-bba9-b159a3954319",
   "metadata": {},
   "source": [
    "# Model and Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a340f-a7c2-4a00-849e-ba8bc702579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRF(nn.Module):\n",
    "\tr\"\"\"\n",
    "\tNeural radiance fields module.\n",
    "\t\"\"\"\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\td_input: int = 3,\n",
    "\t\tn_layers: int = 8,\n",
    "\t\td_filter: int = 256,\n",
    "\t\tskip: Tuple[int] = (4,),\n",
    "\t\td_viewdirs: Optional[int] = None\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.d_input = d_input\n",
    "\t\tself.skip = skip\n",
    "\t\tself.act = nn.functional.relu\n",
    "\t\tself.d_viewdirs = d_viewdirs\n",
    "\n",
    "\t\t# Create model layers\n",
    "\t\tself.layers = nn.ModuleList(\n",
    "\t\t\t[nn.Linear(self.d_input, d_filter)] +\n",
    "\t\t\t[nn.Linear(d_filter + self.d_input, d_filter) if i in skip \\\n",
    "\t\t\t else nn.Linear(d_filter, d_filter) for i in range(n_layers - 1)]\n",
    "\t\t)\n",
    "\n",
    "\t\t# Bottleneck layers\n",
    "\t\tif self.d_viewdirs is not None:\n",
    "\t\t\t# If using viewdirs, split alpha and RGB\n",
    "\t\t\tself.alpha_out = nn.Linear(d_filter, 1)\n",
    "\t\t\tself.rgb_filters = nn.Linear(d_filter, d_filter)\n",
    "\t\t\tself.branch = nn.Linear(d_filter + self.d_viewdirs, d_filter // 2)\n",
    "\t\t\tself.output = nn.Linear(d_filter // 2, 3)\n",
    "\t\telse:\n",
    "\t\t\t# If no viewdirs, use simpler output\n",
    "\t\t\tself.output = nn.Linear(d_filter, 4)\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tx: torch.Tensor,\n",
    "\t\tviewdirs: Optional[torch.Tensor] = None\n",
    "\t) -> torch.Tensor:\n",
    "\t\tr\"\"\"\n",
    "\t\tForward pass with optional view direction.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# Cannot use viewdirs if instantiated with d_viewdirs = None\n",
    "\t\tif self.d_viewdirs is None and viewdirs is not None:\n",
    "\t\t\traise ValueError('Cannot input x_direction if d_viewdirs was not given.')\n",
    "\n",
    "\t\t# Apply forward pass up to bottleneck\n",
    "\t\tx_input = x\n",
    "\t\tfor i, layer in enumerate(self.layers):\n",
    "\t\t\tx = self.act(layer(x))\n",
    "\t\t\tif i in self.skip:\n",
    "\t\t\t\tx = torch.cat([x, x_input], dim=-1)\n",
    "\n",
    "\t\t# Apply bottleneck\n",
    "\t\tif self.d_viewdirs is not None:\n",
    "\t\t\t# Split alpha from network output\n",
    "\t\t\talpha = self.alpha_out(x)\n",
    "\n",
    "\t\t\t# Pass through bottleneck to get RGB\n",
    "\t\t\tx = self.rgb_filters(x)\n",
    "\t\t\tx = torch.concat([x, viewdirs], dim=-1)\n",
    "\t\t\tx = self.act(self.branch(x))\n",
    "\t\t\tx = self.output(x)\n",
    "\n",
    "\t\t\t# Concatenate alphas to output\n",
    "\t\t\tx = torch.concat([x, alpha], dim=-1)\n",
    "\t\telse:\n",
    "\t\t\t# Simple output\n",
    "\t\t\tx = self.output(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d12284",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNeRF(nn.Module):\n",
    "\tr\"\"\"Dynamic Neural radiance fields module.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self,\n",
    "\t\td_input: int = 3,\n",
    "\t\tn_layers: int = 8,\n",
    "\t\td_filter: int = 256,\n",
    "\t\tskip: Tuple[int] = (4,),\n",
    "\t\td_viewdirs: Optional[int] = None,\n",
    "\n",
    "\t\td_time: int = 1,\n",
    "\t\tencode: Callable = None,\n",
    "\t\tzero_canonical: bool = True\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.d_input = d_input\n",
    "\t\tself.n_layers = n_layers\n",
    "\t\tself.d_filter = d_filter\n",
    "\t\tself.skip = skip\n",
    "\t\tself.act = nn.ReLU()\n",
    "\t\t# Time network\n",
    "\t\tself.d_time = d_time\n",
    "\t\tself.encode = encode\n",
    "\t\tself.zero_canonical = zero_canonical\n",
    "\n",
    "\t\t# Defining original nerf nodel\n",
    "\t\tself.nerf_model = NeRF(d_input, n_layers, d_filter, skip, d_viewdirs)\n",
    "\t\t# Defining time deformation network\n",
    "\t\tself.deformation_model = self._create()\n",
    "\t\t\n",
    "\n",
    "\tdef _create(self):\n",
    "\t\tinput_dimention = self.d_input+self.d_time\n",
    "\t\t\n",
    "\t\tlayers = [nn.Linear(input_dimention, self.d_filter)]\n",
    "\t\tfor i in range(1, self.n_layers):\n",
    "\t\t\tif i in self.skip:\n",
    "\t\t\t\tlayers += [nn.Linear(self.d_filter+input_dimention, self.d_filter)]\n",
    "\t\t\telse:\n",
    "\t\t\t\tlayers += [nn.Linear(self.d_filter, self.d_filter)]\n",
    "\t\tlayers += [nn.Linear(self.d_filter, 3)]\n",
    "\n",
    "\t\tlayers = nn.ModuleList(layers)\n",
    "\t\treturn layers \n",
    "\t\n",
    "\tdef _query_time(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\t\toriginal_input = x\n",
    "\t\tfor i, layer in enumerate(self.deformation_model):\n",
    "\t\t\tif i in self.skip:\n",
    "\t\t\t\tx = torch.cat([original_input, x], -1)\n",
    "\t\t\t\n",
    "\t\t\tx = layer(x)\n",
    "\t\t\t\n",
    "\t\t\tif i < len(self.deformation_model)-1:\n",
    "\t\t\t\tx = self.act(x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tx: torch.Tensor,\n",
    "\t\ttimesteps: torch.Tensor,\n",
    "\t\tviewdirs: Optional[torch.Tensor] = None,\n",
    "\t) -> torch.Tensor:\n",
    "\t\tr\"\"\"\n",
    "\t\tForward pass through time deformation network and NeRF\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tdebug('DNERF INPUTS: '+str(x.shape) +' ; '+ str(timesteps.shape))\n",
    "\t\tinputs = torch.cat([x, timesteps], -1)\n",
    "\t\tdx = self._query_time(inputs)\n",
    "\t\tif self.zero_canonical:\n",
    "\t\t\tcond = timesteps[:,0] == 0\n",
    "\t\t\tdx[cond] = 0.0\n",
    "\n",
    "\t\toriginal_points = x[:,:3]\n",
    "\t\tpoints_dx = original_points + dx\n",
    "\t\tpoints_dx = self.encode(points_dx)\n",
    "\n",
    "\t\toutput = self.nerf_model(points_dx, viewdirs)\n",
    "\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f464f6d9-e0a3-41ec-8bb5-80ad44ee6da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(\n",
    "\tinputs: torch.Tensor,\n",
    "\tchunksize: int = 2**15\n",
    ") -> List[torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tDivide an input into chunks.\n",
    "\t\"\"\"\n",
    "\treturn [inputs[i:i + chunksize] for i in range(0, inputs.shape[0], chunksize)]\n",
    "\n",
    "def prepare_chunks(\n",
    "\tpoints: torch.Tensor,\n",
    "\tencoding_function: Callable[[torch.Tensor], torch.Tensor],\n",
    "\tchunksize: int = 2**15\n",
    ") -> List[torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tEncode and chunkify points to prepare for NeRF model.\n",
    "\t\"\"\"\n",
    "\tpoints = points.reshape((-1, 3))\n",
    "\tpoints = encoding_function(points)\n",
    "\tpoints = get_chunks(points, chunksize=chunksize)\n",
    "\treturn points\n",
    "\n",
    "def prepare_viewdirs_chunks(\n",
    "\tpoints: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\tencoding_function: Callable[[torch.Tensor], torch.Tensor],\n",
    "\tchunksize: int = 2**15\n",
    ") -> List[torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tEncode and chunkify viewdirs to prepare for NeRF model.\n",
    "\t\"\"\"\n",
    "\t# Prepare the viewdirs\n",
    "\tviewdirs = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
    "\tviewdirs = viewdirs[:, None, ...].expand(points.shape).reshape((-1, 3))\n",
    "\tviewdirs = encoding_function(viewdirs)\n",
    "\tviewdirs = get_chunks(viewdirs, chunksize=chunksize)\n",
    "\treturn viewdirs\n",
    "\n",
    "def prepare_time(\n",
    "\ttimesteps, \n",
    "\ttime_encoding_fn: Callable[[torch.Tensor], torch.Tensor], \n",
    "\tchunksize: int = 2**15\n",
    ") -> List[torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tEncode and chunkify timepoints to prepare for NeRF model.\n",
    "\t\"\"\"\n",
    "\ttimesteps = time_encoding_fn(timesteps)\n",
    "\ttimesteps = get_chunks(timesteps, chunksize=chunksize)\n",
    "\treturn timesteps\n",
    "    \n",
    "def nerf_forward(\n",
    "\trays_o: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\ttimesteps: torch.Tensor,\n",
    "\tnear: float,\n",
    "\tfar: float,\n",
    "\tencoding_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "\tcoarse_model: nn.Module,\n",
    "\tkwargs_sample_stratified: dict = None,\n",
    "\tn_samples_hierarchical: int = 0,\n",
    "\tkwargs_sample_hierarchical: dict = None,\n",
    "\tfine_model = None,\n",
    "\tviewdirs_encoding_fn: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n",
    "\ttime_encoding_fn: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n",
    "\tchunksize: int = 2**15\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]:\n",
    "\tr\"\"\"\n",
    "\tCompute forward pass through model(s).\n",
    "\t\"\"\"\n",
    "\t# Set no kwargs if none are given.\n",
    "\tif kwargs_sample_stratified is None:\n",
    "\t\tkwargs_sample_stratified = {}\n",
    "\tif kwargs_sample_hierarchical is None:\n",
    "\t\tkwargs_sample_hierarchical = {}\n",
    "\n",
    "\t# Sample query points along each ray.\n",
    "\tquery_points, z_vals = sample_stratified(\n",
    "\t\t\trays_o, rays_d, near, far, **kwargs_sample_stratified)\n",
    "\t# Prepare batches.\n",
    "\tbatches = prepare_chunks(query_points, encoding_fn, chunksize=chunksize)\n",
    "\tif viewdirs_encoding_fn is not None:\n",
    "\t\tbatches_viewdirs = prepare_viewdirs_chunks(\n",
    "\t\t\tquery_points, rays_d,\n",
    "\t\t\tviewdirs_encoding_fn,\n",
    "\t\t\tchunksize=chunksize)\n",
    "\telse:\n",
    "\t\tbatches_viewdirs = [None] * len(batches)\n",
    "\n",
    "\t# Time\n",
    "\tif timesteps.shape[0] == 1: # only one image and one timestep\n",
    "\t\t# Expanding to match other data\n",
    "\t\tn_points = query_points.reshape(-1,3).shape[0]\n",
    "\t\texpanded_timesteps = timesteps.expand(n_points).reshape(-1,1)\n",
    "\telse: # For each ray repeating timestep along sampling dimention\n",
    "\t\tn_rays = query_points.shape[1]\n",
    "\t\texpanded_timesteps = timesteps.repeat(1,n_rays).reshape(-1,1)\n",
    "\tbatches_times = prepare_time(expanded_timesteps, time_encoding_fn, chunksize)\n",
    "\t\n",
    "\t###\n",
    "\t# Coarse model pass.\n",
    "\t# Split the encoded points into \"chunks\", run the model on all chunks, and\n",
    "\t# concatenate the results (to avoid out-of-memory issues).\n",
    "\tpredictions = []\n",
    "\tfor batch, batch_viewdirs, batch_time in zip(batches, batches_viewdirs, batches_times):\n",
    "\t\tone_batch_prediction = coarse_model(batch, viewdirs=batch_viewdirs, timesteps=batch_time)\n",
    "\t\tpredictions.append(one_batch_prediction)\n",
    "\n",
    "\tdebug('Predictions length: '+str(len(predictions)))\n",
    "\traw = torch.cat(predictions, dim=0)\n",
    "\t\n",
    "\traw = raw.reshape(list(query_points.shape[:2]) + [raw.shape[-1]])\n",
    "\traw = raw.to('cpu')\n",
    "\n",
    "\t# Perform differentiable volume rendering to re-synthesize the RGB image.\n",
    "\trgb_map, depth_map, acc_map, weights = raw2outputs(raw, z_vals, rays_d)\n",
    "\t# rgb_map, depth_map, acc_map, weights = render_volume_density(raw, rays_o, z_vals)\n",
    "\n",
    "\toutputs = {\n",
    "\t\t\t'z_vals_stratified': z_vals\n",
    "\t}\n",
    "\n",
    "\t###\n",
    "\t# Fine model pass.\n",
    "\tdebug('Fine model pass\\n')\n",
    "\tif n_samples_hierarchical > 0:\n",
    "\t\t# Save previous outputs to return.\n",
    "\t\trgb_map_0, depth_map_0, acc_map_0 = rgb_map, depth_map, acc_map\n",
    "\n",
    "\t\t# Apply hierarchical sampling for fine query points.\n",
    "\t\tquery_points, z_vals_combined, z_hierarch = sample_hierarchical(\n",
    "\t\t\trays_o, rays_d, z_vals, weights, n_samples_hierarchical,\n",
    "\t\t\t**kwargs_sample_hierarchical)\n",
    "\n",
    "\t\t# Prepare inputs as before.\n",
    "\t\tbatches = prepare_chunks(query_points, encoding_fn, chunksize=chunksize)\n",
    "\t\tif viewdirs_encoding_fn is not None:\n",
    "\t\t\tbatches_viewdirs = prepare_viewdirs_chunks(\n",
    "\t\t\t\tquery_points, rays_d,\n",
    "\t\t\t\tviewdirs_encoding_fn,\n",
    "\t\t\t\tchunksize=chunksize)\n",
    "\t\telse:\n",
    "\t\t\tbatches_viewdirs = [None] * len(batches)\n",
    "\n",
    "\t\t# Time\n",
    "\t\tif timesteps.shape[0] == 1: # only one image and one timestep\n",
    "\t\t\t# Expanding to match other data\n",
    "\t\t\tn_points = query_points.reshape(-1,3).shape[0]\n",
    "\t\t\texpanded_timesteps = timesteps.expand(n_points).reshape(-1,1)\n",
    "\t\telse: # For each ray repeating timestep along sampling dimention\n",
    "\t\t\tn_rays = query_points.shape[1]\n",
    "\t\t\texpanded_timesteps = timesteps.repeat(1,n_rays).reshape(-1,1)\n",
    "\t\tbatches_times = prepare_time(expanded_timesteps, time_encoding_fn, chunksize)\n",
    "\n",
    "\t\t# Forward pass new samples through fine model.\n",
    "\t\tfine_model = fine_model if fine_model is not None else coarse_model\n",
    "\t\tpredictions = []\n",
    "\t\tfor batch, batch_viewdirs, batch_time in zip(batches, batches_viewdirs, batches_times):\n",
    "\t\t\tone_batch_predictions = fine_model(batch, viewdirs=batch_viewdirs, timesteps=batch_time)\n",
    "\t\t\tpredictions.append(one_batch_predictions)\n",
    "\t\traw = torch.cat(predictions, dim=0)\n",
    "\t\traw = raw.reshape(list(query_points.shape[:2]) + [raw.shape[-1]])\n",
    "\t\traw = raw.to('cpu')\n",
    "\n",
    "\t\t# Perform differentiable volume rendering to re-synthesize the RGB image.\n",
    "\t\trgb_map, depth_map, acc_map, weights = raw2outputs(raw, z_vals_combined, rays_d)\n",
    "\n",
    "\t\t# Store outputs.\n",
    "\t\toutputs['z_vals_hierarchical'] = z_hierarch\n",
    "\t\toutputs['rgb_map_0'] = rgb_map_0\n",
    "\t\toutputs['depth_map_0'] = depth_map_0\n",
    "\t\toutputs['acc_map_0'] = acc_map_0\n",
    "\n",
    "\t# Store outputs.\n",
    "\toutputs['rgb_map'] = rgb_map\n",
    "\toutputs['depth_map'] = depth_map\n",
    "\toutputs['acc_map'] = acc_map\n",
    "\toutputs['weights'] = weights\n",
    "\treturn outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7498bc9c-b704-4063-8613-21dd61e71448",
   "metadata": {},
   "source": [
    "# Raw data to outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8426fb-1f4c-40c8-97c7-5a72b5ff35a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw2outputs(\n",
    "\traw: torch.Tensor,\n",
    "\tz_vals: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\traw_noise_std: float = 0.0,\n",
    "\twhite_bkgd: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tConvert the raw NeRF output into RGB and other maps.\n",
    "\t\"\"\"\n",
    "\tdevice = raw.device\n",
    "\t# Difference between consecutive elements of `z_vals`. [n_rays, n_samples]\n",
    "\tdists = z_vals[..., 1:] - z_vals[..., :-1]\n",
    "\tdists = torch.cat([dists, 1e10 * torch.ones_like(dists[..., :1])], dim=-1)\n",
    "\n",
    "\t# Multiply each distance by the norm of its corresponding direction ray\n",
    "\t# to convert to real world distance (accounts for non-unit directions).\n",
    "\tdists = dists * torch.norm(rays_d[..., None, :], dim=-1)\n",
    "\n",
    "\t# Add noise to model's predictions for density. Can be used to\n",
    "\t# regularize network during training (prevents floater artifacts).\n",
    "\tnoise = 0.\n",
    "\tif raw_noise_std > 0.:\n",
    "\t\tnoise = torch.randn(raw[..., 3].shape) * raw_noise_std\n",
    "\n",
    "\t# Predict density of each sample along each ray. Higher values imply\n",
    "\t# higher likelihood of being absorbed at this point. [n_rays, n_samples]\n",
    "\talpha = 1.0 - torch.exp(-nn.functional.relu(raw[..., 3] + noise) * dists.to(device))\n",
    "\n",
    "\t# Compute weight for RGB of each sample along each ray. [n_rays, n_samples]\n",
    "\t# The higher the alpha, the lower subsequent weights are driven.\n",
    "\tweights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n",
    "\n",
    "\t# Compute weighted RGB map.\n",
    "\trgb = torch.sigmoid(raw[..., :3])  # [n_rays, n_samples, 3]\n",
    "\trgb_map = torch.sum(weights[..., None] * rgb, dim=-2)  # [n_rays, 3]\n",
    "\n",
    "\t# Estimated depth map is predicted distance.\n",
    "\tdepth_map = torch.sum(weights * z_vals.to(device), dim=-1)\n",
    "\n",
    "\t# Disparity map is inverse depth.\n",
    "\tdisp_map = 1. / torch.max(1e-10 * torch.ones_like(depth_map),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tdepth_map / torch.sum(weights, -1))\n",
    "\n",
    "\t# Sum of weights along each ray. In [0, 1] up to numerical error.\n",
    "\tacc_map = torch.sum(weights, dim=-1)\n",
    "\n",
    "\t# To composite onto a white background, use the accumulated alpha map.\n",
    "\tif white_bkgd:\n",
    "\t\trgb_map = rgb_map + (1. - acc_map[..., None])\n",
    "\n",
    "\treturn rgb_map, depth_map, acc_map, weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4aada-7e5f-4456-aa96-821c15e5a95a",
   "metadata": {},
   "source": [
    "# Evaluation tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5185ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(\n",
    "\tz_vals: torch.Tensor,\n",
    "\tz_hierarch: Optional[torch.Tensor] = None,\n",
    "\tax: Optional[np.ndarray] = None):\n",
    "\tr\"\"\"\n",
    "\tPlot stratified and (optional) hierarchical samples.\n",
    "\t\"\"\"\n",
    "\ty_vals = 1 + np.zeros_like(z_vals)\n",
    "\n",
    "\tif ax is None:\n",
    "\t\tax = plt.subplot()\n",
    "\tax.plot(z_vals, y_vals, 'b-o')\n",
    "\tif z_hierarch is not None:\n",
    "\t\ty_hierarch = np.zeros_like(z_hierarch)\n",
    "\t\tax.plot(z_hierarch, y_hierarch, 'r-o')\n",
    "\tax.set_ylim([-1, 2])\n",
    "\tax.set_title('Stratified  Samples (blue) and Hierarchical Samples (red)')\n",
    "\tax.axes.yaxis.set_visible(False)\n",
    "\tax.grid(True)\n",
    "\treturn ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec88a8d-757f-4409-873d-bd3801c7fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(i, rgb_predicted, iternums, outputs,train_psnrs,val_psnrs, is_save=False):\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(24,4), gridspec_kw={'width_ratios': [1, 1, 1, 3]})\n",
    "    # Plot example outputs\n",
    "    \n",
    "    ax[0].imshow(rgb_predicted.reshape([height, width, 3]).detach().cpu().numpy())\n",
    "    ax[0].set_title(f'Iteration: {i}')\n",
    "    ax[1].imshow(testimg.detach().cpu().numpy())\n",
    "    ax[1].set_title(f'Target')\n",
    "    ax[2].plot(range(0, i + 1), train_psnrs, 'r')\n",
    "    ax[2].plot(iternums, val_psnrs, 'b')\n",
    "    ax[2].set_title('PSNR (train=red, val=blue')\n",
    "    z_vals_strat = outputs['z_vals_stratified'].view((-1, n_samples))\n",
    "    z_sample_strat = z_vals_strat[z_vals_strat.shape[0] // 2].detach().cpu().numpy()\n",
    "    if 'z_vals_hierarchical' in outputs:\n",
    "        z_vals_hierarch = outputs['z_vals_hierarchical'].view((-1, n_samples_hierarchical))\n",
    "        z_sample_hierarch = z_vals_hierarch[z_vals_hierarch.shape[0] // 2].detach().cpu().numpy()\n",
    "    else:\n",
    "        z_sample_hierarch = None\n",
    "    _ = plot_samples(z_sample_strat, z_sample_hierarch, ax=ax[3])\n",
    "    ax[3].margins(0)\n",
    "    plt.show()\n",
    "\n",
    "    if is_save:\n",
    "        rgb_predicted = rgb_predicted.reshape([height, width, 3]).detach().cpu().numpy()\n",
    "        rgb_predicted = (rgb_predicted*255).astype(np.uint8)\n",
    "        Image.fromarray(rgb_predicted).save('predicted_image.png')\n",
    "\n",
    "        ti = testimg.detach().cpu().numpy()\n",
    "        ti = (ti*255).astype(np.uint8)\n",
    "        Image.fromarray(ti).save('GT_image.png')\n",
    "\n",
    "\n",
    "def eval_one_time(i, iternums, train_psnrs, val_psnrs, model, encode, fine_model, encode_viewdirs, is_save=False):\n",
    "\tmodel.eval()\n",
    "\theight, width = testimg.shape[:2]\n",
    "\trays_o, rays_d = get_rays(height, width, focal, testpose)\n",
    "\trays_o = rays_o.reshape([-1, 3]).to(device)\n",
    "\trays_d = rays_d.reshape([-1, 3]).to(device)\n",
    "\toutputs = nerf_forward(rays_o, rays_d,\n",
    "\t\t\t\t\t\t\tnear, far, encode, model,\n",
    "\t\t\t\t\t\t\tkwargs_sample_stratified=kwargs_sample_stratified,\n",
    "\t\t\t\t\t\t\tn_samples_hierarchical=n_samples_hierarchical,\n",
    "\t\t\t\t\t\t\tkwargs_sample_hierarchical=kwargs_sample_hierarchical,\n",
    "\t\t\t\t\t\t\tfine_model=fine_model,\n",
    "\t\t\t\t\t\t\tviewdirs_encoding_fn=encode_viewdirs,\n",
    "\t\t\t\t\t\t\tchunksize=chunksize)\n",
    "\n",
    "\trgb_predicted = outputs['rgb_map']\n",
    "\ttestimg_flat = testimg.reshape(-1, 3)\n",
    "\tloss = torch.nn.functional.mse_loss(rgb_predicted, testimg_flat.to(device))\n",
    "\tval_psnr = -10. * torch.log10(loss)\n",
    "\tval_psnrs.append(val_psnr.item())\n",
    "\titernums.append(i)\n",
    "\n",
    "\tprint(\"Val Loss: \", loss.item(), \" Val PSRN: \", val_psnr.item())\n",
    "\t# Plot example outputs outside\n",
    "\trender(i, rgb_predicted, iternums, outputs,train_psnrs,val_psnrs, is_save)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0123f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation rendering\n",
    "def pose_spherical(theta, phi, radius):\n",
    "    trans_t = lambda t : torch.Tensor([\n",
    "        [1,0,0,0],\n",
    "        [0,1,0,0],\n",
    "        [0,0,1,t],\n",
    "        [0,0,0,1]]).float()\n",
    "    \n",
    "    rot_phi = lambda phi : torch.Tensor([\n",
    "        [1,0,0,0],\n",
    "        [0,np.cos(phi),-np.sin(phi),0],\n",
    "        [0,np.sin(phi), np.cos(phi),0],\n",
    "        [0,0,0,1]]).float()\n",
    "    \n",
    "    rot_theta = lambda th : torch.Tensor([\n",
    "        [np.cos(th),0,-np.sin(th),0],\n",
    "        [0,1,0,0],\n",
    "        [np.sin(th),0, np.cos(th),0],\n",
    "        [0,0,0,1]]).float()\n",
    "    \n",
    "    c2w = trans_t(radius)\n",
    "    c2w = rot_phi(phi/180.*np.pi) @ c2w\n",
    "    c2w = rot_theta(theta/180.*np.pi) @ c2w\n",
    "    c2w = torch.Tensor(np.array([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]])) @ c2w\n",
    "    return c2w\n",
    "\n",
    "\n",
    "def save_training_progress(\n",
    "    model,fine_model,encode,encode_viewdirs, chunksize, \n",
    "    near, far, height, width, focal, epoch, device\n",
    "):\n",
    "    render_poses = torch.stack([pose_spherical(angle, -30.0, 4.0) for angle in np.linspace(-180,180,40+1)[:-1]], 0)\n",
    "    pose = render_poses[epoch%40]\n",
    "\n",
    "    rays_o, rays_d = get_rays(height, width, focal, pose)\n",
    "    rays_o = rays_o.reshape([-1, 3]).to(device)\n",
    "    rays_d = rays_d.reshape([-1, 3]).to(device)\n",
    "    outputs = nerf_forward(rays_o, rays_d,\n",
    "\t\t\t\t\t\t\tnear, far, encode, model,\n",
    "\t\t\t\t\t\t\tkwargs_sample_stratified=kwargs_sample_stratified,\n",
    "\t\t\t\t\t\t\tn_samples_hierarchical=n_samples_hierarchical,\n",
    "\t\t\t\t\t\t\tkwargs_sample_hierarchical=kwargs_sample_hierarchical,\n",
    "\t\t\t\t\t\t\tfine_model=fine_model,\n",
    "\t\t\t\t\t\t\tviewdirs_encoding_fn=encode_viewdirs,\n",
    "\t\t\t\t\t\t\tchunksize=chunksize)\n",
    "    rgb_predicted = outputs['rgb_map']\n",
    "    rgb_predicted = rgb_predicted.reshape([height, width, 3]).detach().cpu().numpy()\n",
    "    rgb_predicted = (rgb_predicted*255).astype(np.uint8)\n",
    "    img = Image.fromarray(rgb_predicted)\n",
    "    _zeros = '0' * (5 - len(str(epoch)))\n",
    "    img.save(test_save_dir+f'{_zeros}{epoch}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b3eba-afb6-4919-b5ca-5f4857a783d3",
   "metadata": {},
   "source": [
    "# Training preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463b5a6c-76cf-47df-bb4c-768f4d1e8a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "\tr\"\"\"\n",
    "\tEarly stopping helper based on fitness criterion.\n",
    "\t\"\"\"\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tpatience: int = 30,\n",
    "\t\tmargin: float = 1e-4\n",
    "\t):\n",
    "\t\tself.best_fitness = 0.0  # In our case PSNR\n",
    "\t\tself.best_iter = 0\n",
    "\t\tself.margin = margin\n",
    "\t\tself.patience = patience or float('inf')  # epochs to wait after fitness stops improving to stop\n",
    "\n",
    "\tdef __call__(\n",
    "\t\tself,\n",
    "\t\titer: int,\n",
    "\t\tfitness: float\n",
    "\t):\n",
    "\t\tr\"\"\"\n",
    "\t\tCheck if criterion for stopping is met.\n",
    "\t\t\"\"\"\n",
    "\t\tif (fitness - self.best_fitness) > self.margin:\n",
    "\t\t\tself.best_iter = iter\n",
    "\t\t\tself.best_fitness = fitness\n",
    "\t\tdelta = iter - self.best_iter\n",
    "\t\tstop = delta >= self.patience  # stop training if patience exceeded\n",
    "\t\treturn stop\n",
    "\n",
    "def crop_center(\n",
    "\timg: torch.Tensor,\n",
    "\tfrac: float = 0.5\n",
    ") -> torch.Tensor:\n",
    "\tr\"\"\"\n",
    "\tCrop center square from image.\n",
    "\t\"\"\"\n",
    "\th_offset = round(img.shape[0] * (frac / 2))\n",
    "\tw_offset = round(img.shape[1] * (frac / 2))\n",
    "\treturn img[h_offset:-h_offset, w_offset:-w_offset]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2928e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoders\n",
    "\n",
    "if encoder_used is Encoders.FREQ:\n",
    "    encoder = PositionalEncoder(d_input, n_freqs, log_space=log_space)\n",
    "    encode = lambda x: encoder(x)\n",
    "    \n",
    "elif encoder_used is Encoders.HASH:\n",
    "    encoder = Grid(\n",
    "        feature_dim=3,\n",
    "        grid_dim=3,\n",
    "        num_lvl=17,\n",
    "        max_res=2**14, \n",
    "        min_res=16,\n",
    "        hashtable_power=19,\n",
    "    \tdevice=device,\n",
    "    ).to(device)\n",
    "    encode = lambda x: encoder(x)\n",
    "\n",
    "# View direction encoders\n",
    "if use_viewdirs:\n",
    "    encoder_viewdirs = PositionalEncoder(\n",
    "        d_input, n_freqs_views, log_space=log_space)\n",
    "    encode_viewdirs = lambda x: encoder_viewdirs(x)\n",
    "    d_viewdirs = encoder_viewdirs.d_output\n",
    "else:\n",
    "    encode_viewdirs = None\n",
    "    d_viewdirs = None\n",
    "\n",
    "# Time encoder\n",
    "encoder_time = PositionalEncoder(1, n_freqs_time, log_space=log_space)\n",
    "encode_time = lambda x: encoder_time(x)\n",
    "d_timesteps = encoder_time.d_output\n",
    "\n",
    "print(f'Encode input points into {encoder.d_output} dimentions!', )\n",
    "\n",
    "# Models\n",
    "model = DNeRF(\n",
    "\t\t\td_input = encoder.d_output, \n",
    "\t\t\tn_layers = n_layers, \n",
    "\t\t\td_filter = d_filter, \n",
    "\t\t\tskip = skip,\n",
    "\t\t\td_viewdirs = d_viewdirs,\n",
    "\t\t\td_time = d_timesteps,\n",
    "\t\t\tencode = encode,\n",
    "\t\t\tzero_canonical=zero_canonical)\n",
    "model.to(device)\n",
    "# model_params = list(model.parameters())\n",
    "if use_fine_model:\n",
    "    fine_model = DNeRF(\n",
    "        d_input = encoder.d_output, \n",
    "        n_layers=n_layers_fine, \n",
    "        d_filter=d_filter_fine, \n",
    "        skip=skip_fine,\n",
    "        d_viewdirs=d_viewdirs,\n",
    "        d_time=d_timesteps,\n",
    "        encode=encode,\n",
    "        zero_canonical=zero_canonical)\n",
    "    fine_model.to(device)\n",
    "    # model_params = model_params + list(fine_model.parameters())\n",
    "else:\n",
    "    fine_model = None\n",
    "\n",
    "# Optimizer\n",
    "if encoder_used is Encoders.HASH:\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters(), 'lr': lr},\n",
    "        {'params': fine_model.parameters(), 'lr': lr},\n",
    "        {'params': encoder.parameters(), 'lr': lr_hash_enc},\n",
    "    ])\n",
    "else:\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters(), 'lr': lr},\n",
    "        {'params': fine_model.parameters(), 'lr': lr},\n",
    "    ])\n",
    "\n",
    "# Scheduler \n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer, \n",
    "    start_factor=scheduler_start_end_factors[0], \n",
    "    end_factor=scheduler_start_end_factors[1], \n",
    "    total_iters=n_epochs)\n",
    "    \n",
    "# Early Stopping\n",
    "warmup_stopper = EarlyStopping(patience=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb823f4-46a3-471c-8f9f-ad1f977c0e2f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9a8f9c-8538-4a6a-a8e1-0d852cdcd10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "\n",
    "train_psnrs = []\n",
    "val_psnrs = []\n",
    "iternums = []\n",
    "\n",
    "\n",
    "pbar = tqdm(range(n_epochs), desc=\"Training Epochs\")\n",
    "for i in pbar:\n",
    "    ###\n",
    "    ### Train one time\n",
    "    ###\n",
    "    # One image per step\n",
    "\n",
    "    # Randomly pick an image as the target.\n",
    "    # Target Image\n",
    "    target_img_idx = np.random.randint(images.shape[0])\n",
    "    target_img = images[target_img_idx].to(device)\n",
    "    if center_crop and i < center_crop_iters:\n",
    "        target_img = crop_center(target_img)\n",
    "    height, width = target_img.shape[:2]\n",
    "    target_img = target_img.reshape([-1, 3])\n",
    "    # Pose\n",
    "    target_pose = poses[target_img_idx].to(device)\n",
    "    # Rays\n",
    "    rays_o, rays_d = get_rays(height, width, focal, target_pose)\n",
    "    rays_o = rays_o.reshape([-1, 3]).to(device)\n",
    "    rays_d = rays_d.reshape([-1, 3]).to(device)\n",
    "    # Handle Time\n",
    "    timesteps = times[target_img_idx].reshape(1).to(device)\n",
    "\n",
    "\n",
    "    # Run one iteration of TinyNeRF and get the rendered RGB image.\n",
    "\toutputs = nerf_forward(\n",
    "\t\trays_o, rays_d, timesteps,\n",
    "        near, far, encode, model,\n",
    "        kwargs_sample_stratified=kwargs_sample_stratified,\n",
    "        n_samples_hierarchical=n_samples_hierarchical,\n",
    "        kwargs_sample_hierarchical=kwargs_sample_hierarchical,\n",
    "        fine_model=fine_model,\n",
    "        viewdirs_encoding_fn=encode_viewdirs,\n",
    "\t\ttime_encoding_fn=encode_time,\n",
    "        chunksize=chunksize)\n",
    "\n",
    "    # Check for any numerical issues.\n",
    "    for k, v in outputs.items():\n",
    "        if torch.isnan(v).any():\n",
    "            print(f\"! [Numerical Alert] {k} contains NaN.\")\n",
    "        if torch.isinf(v).any():\n",
    "            print(f\"! [Numerical Alert] {k} contains Inf.\")\n",
    "\n",
    "    # Backprop!\n",
    "    rgb_predicted = outputs['rgb_map']\n",
    "\n",
    "    loss = torch.nn.functional.mse_loss(\n",
    "        rgb_predicted.to(device), target_img.to(device))\n",
    "    loss.backward()\n",
    "\n",
    "    # After the backward pass\n",
    "    # for name, param in encoder.named_parameters():\n",
    "    # \tif param.grad is not None:\n",
    "    # \t\tprint(f\"{name} gradient after backward: {param.grad}\")\n",
    "    # \telse:\n",
    "    # \t\tprint(f\"{name} has no gradients after backward\")\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    scheduler.step()\n",
    "\n",
    "    psnr = -10. * torch.log10(loss)\n",
    "    psnr = psnr.item()\n",
    "    \n",
    "    ###\n",
    "    # Evaluation and logging\n",
    "\n",
    "    epoch = i\n",
    "    pbar.set_description(f\"Epoch {epoch + 1}/{n_epochs}, PSNR: {psnr:.2f}, Loss: {loss.item():.5f}\")\n",
    "\n",
    "    train_psnrs.append(psnr)\n",
    "\n",
    "    # Evaluate testimg at given display rate.\n",
    "    if i % display_rate == 0:\n",
    "        eval_one_time(i, iternums, train_psnrs, val_psnrs, model, encode, fine_model, encode_viewdirs)\n",
    "\n",
    "        save_training_progress(\n",
    "            model,fine_model,encode,encode_viewdirs, chunksize, \n",
    "            near, far, height, width, focal, epoch//display_rate, device\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38e330c-7573-4c2a-b322-b95b68c13d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def add_frame_number(image, frame_number,fontsize=12):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default()  # You can choose a different font if you prefer\n",
    "    text = str(frame_number)\n",
    "    text_width = draw.textlength(text, font=font)\n",
    "    text_size = (text_width, fontsize)\n",
    "    # Position the text at the bottom right corner\n",
    "    text_position = (image.width - text_size[0] - 10, image.height - text_size[1] - 10)\n",
    "    \n",
    "    draw.text(text_position, text, font=font, fill=\"white\")\n",
    "    return image\n",
    "\n",
    "def make_gif(frame_folder, save_path=\"./my_awesome.gif\"):\n",
    "    im_paths = sorted(glob.glob(f\"{frame_folder}/*.png\"))\n",
    "    frames = [Image.open(image) for image in im_paths]\n",
    "    \n",
    "    # Add frame numbers\n",
    "    frame = [add_frame_number(frame, i) for i, frame in enumerate(frames)]\n",
    "    \n",
    "    # Save gif\n",
    "    frame_one = frames[0]\n",
    "    frame_one.save(save_path, format=\"GIF\", append_images=frames,optimize=True,\n",
    "               save_all=True, duration=100, loop=0)\n",
    "\n",
    "\n",
    "make_gif(test_save_dir, f'./xyz{encoder_used.name}_2RGBA_animation.gif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26603bfd-2b7b-4c47-adbd-8e3ae6327359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving one image for paper\n",
    "eval_one_time(i, iternums, train_psnrs, val_psnrs, model, encode, fine_model, encode_viewdirs, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f391dfd7-7d66-40a9-9417-6c885ec39be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model.state_dict(), f'./results/nerf_{encoder_used.name}.tensor')\n",
    "torch.save(fine_model.state_dict(), f'./results/nerf_fine_{encoder_used.name}.tensor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0527267b-d741-4d8a-8b0e-0795af60cfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_used_name = 'HASH'\n",
    "# model.load_state_dict(torch.load(f'nerf_{encoder_used_name}.tensor'))\n",
    "# fine_model.load_state_dict(torch.load(f'nerf_fine_{encoder_used_name}.tensor'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
