{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "# One pass\n",
    "%matplotlib inline\n",
    "import os\n",
    "from typing import Optional, Tuple, List, Union, Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from tqdm import trange\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# For repeatability\n",
    "seed = 2024\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.load('tiny_nerf_data.npz')\n",
    "\n",
    "testimg_idx = 99\n",
    "n_training = 100\n",
    "\n",
    "# Gather as torch tensors\n",
    "images = torch.from_numpy(data['images'][:n_training])#.to(device)\n",
    "poses = torch.from_numpy(data['poses'])#.to(device)\n",
    "focal = torch.from_numpy(data['focal'])#.to(device)\n",
    "testimg = torch.from_numpy(data['images'][testimg_idx])#.to(device)\n",
    "testpose = torch.from_numpy(data['poses'][testimg_idx])#.to(device)\n",
    "\n",
    "testimg, testpose = images[testimg_idx], poses[testimg_idx]\n",
    "height, width = images.shape[1:3]\n",
    "near, far = 2., 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray((testimg*255).numpy().astype(np.uint8)).save('results/Original_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_rays(\n",
    "\theight: int,\n",
    "\twidth: int,\n",
    "\tfocal_length: float,\n",
    "\tc2w: torch.Tensor\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tFind origin and direction of rays through every pixel and camera origin.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Apply pinhole camera model to gather directions at each pixel\n",
    "\ti, j = torch.meshgrid(\n",
    "\t\t\ttorch.arange(width, dtype=torch.float32).to(c2w),\n",
    "\t\t\ttorch.arange(height, dtype=torch.float32).to(c2w),\n",
    "\t\t\tindexing='ij')\n",
    "\ti, j = i.transpose(-1, -2), j.transpose(-1, -2)\n",
    "\tdirections = torch.stack([(i - width * .5) / focal_length,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t-(j - height * .5) / focal_length,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t-torch.ones_like(i)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t ], dim=-1)\n",
    "\n",
    "\t# Apply camera pose to directions\n",
    "\trays_d = torch.sum(directions[..., None, :] * c2w[:3, :3], dim=-1)\n",
    "\n",
    "\t# Origin is same for all directions (the optical center)\n",
    "\trays_o = c2w[:3, -1].expand(rays_d.shape)\n",
    "\treturn rays_o, rays_d\n",
    "\n",
    "\n",
    "def sample_stratified(\n",
    "\trays_o: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\tnear: float,\n",
    "\tfar: float,\n",
    "\tn_samples: int,\n",
    "\tperturb: Optional[bool] = True,\n",
    "\tinverse_depth: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tSample along ray from regularly-spaced bins.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Grab samples for space integration along ray\n",
    "\tt_vals = torch.linspace(0., 1., n_samples, device=rays_o.device)\n",
    "\tif not inverse_depth:\n",
    "\t\t# Sample linearly between `near` and `far`\n",
    "\t\tz_vals = near * (1.-t_vals) + far * (t_vals)\n",
    "\telse:\n",
    "\t\t# Sample linearly in inverse depth (disparity)\n",
    "\t\tz_vals = 1./(1./near * (1.-t_vals) + 1./far * (t_vals))\n",
    "\n",
    "\t# Draw uniform samples from bins along ray\n",
    "\tif perturb:\n",
    "\t\tmids = .5 * (z_vals[1:] + z_vals[:-1])\n",
    "\t\tupper = torch.concat([mids, z_vals[-1:]], dim=-1)\n",
    "\t\tlower = torch.concat([z_vals[:1], mids], dim=-1)\n",
    "\t\tt_rand = torch.rand([n_samples], device=z_vals.device)\n",
    "\t\tz_vals = lower + (upper - lower) * t_rand\n",
    "\tz_vals = z_vals.expand(list(rays_o.shape[:-1]) + [n_samples])\n",
    "\n",
    "\t# Apply scale from `rays_d` and offset from `rays_o` to samples\n",
    "\t# pts: (width, height, n_samples, 3)\n",
    "\tpts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals[..., :, None]\n",
    "\treturn pts, z_vals\n",
    "\n",
    "\n",
    "def cumprod_exclusive(\n",
    "\ttensor: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "\tr\"\"\"\n",
    "\t(Courtesy of https://github.com/krrish94/nerf-pytorch)\n",
    "\n",
    "\tMimick functionality of tf.math.cumprod(..., exclusive=True), as it isn't available in PyTorch.\n",
    "\n",
    "\tArgs:\n",
    "\ttensor (torch.Tensor): Tensor whose cumprod (cumulative product, see `torch.cumprod`) along dim=-1\n",
    "\t\tis to be computed.\n",
    "\tReturns:\n",
    "\tcumprod (torch.Tensor): cumprod of Tensor along dim=-1, mimiciking the functionality of\n",
    "\t\ttf.math.cumprod(..., exclusive=True) (see `tf.math.cumprod` for details).\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Compute regular cumprod first (this is equivalent to `tf.math.cumprod(..., exclusive=False)`).\n",
    "\tcumprod = torch.cumprod(tensor, -1)\n",
    "\t# \"Roll\" the elements along dimension 'dim' by 1 element.\n",
    "\tcumprod = torch.roll(cumprod, 1, -1)\n",
    "\t# Replace the first element by \"1\" as this is what tf.cumprod(..., exclusive=True) does.\n",
    "\tcumprod[..., 0] = 1.\n",
    "\n",
    "\treturn cumprod\n",
    "\n",
    "def raw2outputs(\n",
    "\traw: torch.Tensor,\n",
    "\tz_vals: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\traw_noise_std: float = 0.0,\n",
    "\twhite_bkgd: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tConvert the raw NeRF output into RGB and other maps.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Difference between consecutive elements of `z_vals`. [n_rays, n_samples]\n",
    "\tdists = z_vals[..., 1:] - z_vals[..., :-1]\n",
    "\tdists = torch.cat([dists, 1e10 * torch.ones_like(dists[..., :1])], dim=-1)\n",
    "\n",
    "\t# Multiply each distance by the norm of its corresponding direction ray\n",
    "\t# to convert to real world distance (accounts for non-unit directions).\n",
    "\tdists = dists * torch.norm(rays_d[..., None, :], dim=-1)\n",
    "\n",
    "\t# Add noise to model's predictions for density. Can be used to\n",
    "\t# regularize network during training (prevents floater artifacts).\n",
    "\tnoise = 0.\n",
    "\tif raw_noise_std > 0.:\n",
    "\t\tnoise = torch.randn(raw[..., 3].shape) * raw_noise_std\n",
    "\n",
    "\t# Predict density of each sample along each ray. Higher values imply\n",
    "\t# higher likelihood of being absorbed at this point. [n_rays, n_samples]\n",
    "\talpha = 1.0 - torch.exp(-nn.functional.relu(raw[..., 3] + noise) * dists)\n",
    "\n",
    "\t# Compute weight for RGB of each sample along each ray. [n_rays, n_samples]\n",
    "\t# The higher the alpha, the lower subsequent weights are driven.\n",
    "\tweights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n",
    "\n",
    "\t# Compute weighted RGB map.\n",
    "\trgb = torch.sigmoid(raw[..., :3])  # [n_rays, n_samples, 3]\n",
    "\trgb_map = torch.sum(weights[..., None] * rgb, dim=-2)  # [n_rays, 3]\n",
    "\n",
    "\t# Estimated depth map is predicted distance.\n",
    "\tdepth_map = torch.sum(weights * z_vals, dim=-1)\n",
    "\n",
    "\t# Disparity map is inverse depth.\n",
    "\tdisp_map = 1. / torch.max(1e-10 * torch.ones_like(depth_map),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tdepth_map / torch.sum(weights, -1))\n",
    "\n",
    "\t# Sum of weights along each ray. In [0, 1] up to numerical error.\n",
    "\tacc_map = torch.sum(weights, dim=-1)\n",
    "\n",
    "\t# To composite onto a white background, use the accumulated alpha map.\n",
    "\tif white_bkgd:\n",
    "\t\trgb_map = rgb_map + (1. - acc_map[..., None])\n",
    "\n",
    "\treturn rgb_map, depth_map, acc_map, weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRF(nn.Module):\n",
    "\tr\"\"\"\n",
    "\tNeural radiance fields module.\n",
    "\t\"\"\"\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\td_input: int = 3,\n",
    "\t\tn_layers: int = 8,\n",
    "\t\td_filter: int = 256,\n",
    "\t\tskip: Tuple[int] = (4,),\n",
    "\t\td_viewdirs: Optional[int] = None\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.d_input = d_input\n",
    "\t\tself.skip = skip\n",
    "\t\tself.act = nn.functional.relu\n",
    "\t\tself.d_viewdirs = d_viewdirs\n",
    "\n",
    "\t\t# Create model layers\n",
    "\t\tself.layers = nn.ModuleList(\n",
    "\t\t\t[nn.Linear(self.d_input, d_filter)] +\n",
    "\t\t\t[nn.Linear(d_filter + self.d_input, d_filter) if i in skip \\\n",
    "\t\t\t else nn.Linear(d_filter, d_filter) for i in range(n_layers - 1)]\n",
    "\t\t)\n",
    "\n",
    "\t\t# Bottleneck layers\n",
    "\t\tif self.d_viewdirs is not None:\n",
    "\t\t\t# If using viewdirs, split alpha and RGB\n",
    "\t\t\tself.alpha_out = nn.Linear(d_filter, 1)\n",
    "\t\t\tself.rgb_filters = nn.Linear(d_filter, d_filter)\n",
    "\t\t\tself.branch = nn.Linear(d_filter + self.d_viewdirs, d_filter // 2)\n",
    "\t\t\tself.output = nn.Linear(d_filter // 2, 3)\n",
    "\t\telse:\n",
    "\t\t\t# If no viewdirs, use simpler output\n",
    "\t\t\tself.output = nn.Linear(d_filter, 4)\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tx: torch.Tensor,\n",
    "\t\tviewdirs: Optional[torch.Tensor] = None\n",
    "\t) -> torch.Tensor:\n",
    "\t\tr\"\"\"\n",
    "\t\tForward pass with optional view direction.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# Cannot use viewdirs if instantiated with d_viewdirs = None\n",
    "\t\tif self.d_viewdirs is None and viewdirs is not None:\n",
    "\t\t\traise ValueError('Cannot input x_direction if d_viewdirs was not given.')\n",
    "\n",
    "\t\t# Apply forward pass up to bottleneck\n",
    "\t\tx_input = x\n",
    "\t\tfor i, layer in enumerate(self.layers):\n",
    "\t\t\tx = self.act(layer(x))\n",
    "\t\t\tif i in self.skip:\n",
    "\t\t\t\tx = torch.cat([x, x_input], dim=-1)\n",
    "\n",
    "\t\t# Apply bottleneck\n",
    "\t\tif self.d_viewdirs is not None:\n",
    "\t\t\t# Split alpha from network output\n",
    "\t\t\talpha = self.alpha_out(x)\n",
    "\n",
    "\t\t\t# Pass through bottleneck to get RGB\n",
    "\t\t\tx = self.rgb_filters(x)\n",
    "\t\t\tx = torch.concat([x, viewdirs], dim=-1)\n",
    "\t\t\tx = self.act(self.branch(x))\n",
    "\t\t\tx = self.output(x)\n",
    "\n",
    "\t\t\t# Concatenate alphas to output\n",
    "\t\t\tx = torch.concat([x, alpha], dim=-1)\n",
    "\t\telse:\n",
    "\t\t\t# Simple output\n",
    "\t\t\tx = self.output(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "def sample_pdf(\n",
    "\tbins: torch.Tensor,\n",
    "\tweights: torch.Tensor,\n",
    "\tn_samples: int,\n",
    "\tperturb: bool = False\n",
    ") -> torch.Tensor:\n",
    "\tr\"\"\"\n",
    "\tApply inverse transform sampling to a weighted set of points.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Normalize weights to get PDF.\n",
    "\tpdf = (weights + 1e-5) / torch.sum(weights + 1e-5, -1, keepdims=True) # [n_rays, weights.shape[-1]]\n",
    "\n",
    "\t# Convert PDF to CDF.\n",
    "\tcdf = torch.cumsum(pdf, dim=-1) # [n_rays, weights.shape[-1]]\n",
    "\tcdf = torch.concat([torch.zeros_like(cdf[..., :1]), cdf], dim=-1) # [n_rays, weights.shape[-1] + 1]\n",
    "\n",
    "\t# Take sample positions to grab from CDF. Linear when perturb == 0.\n",
    "\tif not perturb:\n",
    "\t\tu = torch.linspace(0., 1., n_samples, device=cdf.device)\n",
    "\t\tu = u.expand(list(cdf.shape[:-1]) + [n_samples]) # [n_rays, n_samples]\n",
    "\telse:\n",
    "\t\tu = torch.rand(list(cdf.shape[:-1]) + [n_samples], device=cdf.device) # [n_rays, n_samples]\n",
    "\n",
    "\t# Find indices along CDF where values in u would be placed.\n",
    "\tu = u.contiguous() # Returns contiguous tensor with same values.\n",
    "\tinds = torch.searchsorted(cdf, u, right=True) # [n_rays, n_samples]\n",
    "\n",
    "\t# Clamp indices that are out of bounds.\n",
    "\tbelow = torch.clamp(inds - 1, min=0)\n",
    "\tabove = torch.clamp(inds, max=cdf.shape[-1] - 1)\n",
    "\tinds_g = torch.stack([below, above], dim=-1) # [n_rays, n_samples, 2]\n",
    "\n",
    "\t# Sample from cdf and the corresponding bin centers.\n",
    "\tmatched_shape = list(inds_g.shape[:-1]) + [cdf.shape[-1]]\n",
    "\tcdf_g = torch.gather(cdf.unsqueeze(-2).expand(matched_shape), dim=-1,\n",
    "\t\t\t\t\t\t\t\t\t\t\t index=inds_g)\n",
    "\tbins_g = torch.gather(bins.unsqueeze(-2).expand(matched_shape), dim=-1,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tindex=inds_g)\n",
    "\n",
    "\t# Convert samples to ray length.\n",
    "\tdenom = (cdf_g[..., 1] - cdf_g[..., 0])\n",
    "\tdenom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n",
    "\tt = (u - cdf_g[..., 0]) / denom\n",
    "\tsamples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n",
    "\n",
    "\treturn samples # [n_rays, n_samples]\n",
    "\n",
    "def sample_hierarchical(\n",
    "\trays_o: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\tz_vals: torch.Tensor,\n",
    "\tweights: torch.Tensor,\n",
    "\tn_samples: int,\n",
    "\tperturb: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tApply hierarchical sampling to the rays.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Draw samples from PDF using z_vals as bins and weights as probabilities.\n",
    "\tz_vals_mid = .5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
    "\tnew_z_samples = sample_pdf(z_vals_mid, weights[..., 1:-1], n_samples,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tperturb=perturb)\n",
    "\tnew_z_samples = new_z_samples.detach()\n",
    "\n",
    "\t# Resample points from ray based on PDF.\n",
    "\tz_vals_combined, _ = torch.sort(torch.cat([z_vals, new_z_samples], dim=-1), dim=-1)\n",
    "\tpts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals_combined[..., :, None]  # [N_rays, N_samples + n_samples, 3]\n",
    "\treturn pts, z_vals_combined, new_z_samples\n",
    "\n",
    "\n",
    "def get_chunks(\n",
    "\tinputs: torch.Tensor,\n",
    "\tchunksize: int = 2**15\n",
    ") -> List[torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tDivide an input into chunks.\n",
    "\t\"\"\"\n",
    "\treturn [inputs[i:i + chunksize] for i in range(0, inputs.shape[0], chunksize)]\n",
    "\n",
    "def prepare_chunks(\n",
    "\tpoints: torch.Tensor,\n",
    "\tencoding_function: Callable[[torch.Tensor], torch.Tensor],\n",
    "\tchunksize: int = 2**15\n",
    ") -> List[torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tEncode and chunkify points to prepare for NeRF model.\n",
    "\t\"\"\"\n",
    "\tpoints = points.reshape((-1, 3))\n",
    "\t# print('Points:\\n',points[:].min(0)[0], points[:].max(0)[0])\n",
    "\tpoints = encoding_function(points)\n",
    "\t# print('Encoded Points:\\n',points[:3].grad)\n",
    "\tpoints = get_chunks(points, chunksize=chunksize)\n",
    "\treturn points\n",
    "\n",
    "def prepare_viewdirs_chunks(\n",
    "\tpoints: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\tencoding_function: Callable[[torch.Tensor], torch.Tensor],\n",
    "\tchunksize: int = 2**15\n",
    ") -> List[torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tEncode and chunkify viewdirs to prepare for NeRF model.\n",
    "\t\"\"\"\n",
    "\t# Prepare the viewdirs\n",
    "\tviewdirs = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
    "\tviewdirs = viewdirs[:, None, ...].expand(points.shape).reshape((-1, 3))\n",
    "\tviewdirs = encoding_function(viewdirs)\n",
    "\tviewdirs = get_chunks(viewdirs, chunksize=chunksize)\n",
    "\treturn viewdirs\n",
    "\n",
    "def nerf_forward(\n",
    "\trays_o: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\tnear: float,\n",
    "\tfar: float,\n",
    "\tencoding_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "\tcoarse_model: nn.Module,\n",
    "\tkwargs_sample_stratified: dict = None,\n",
    "\tn_samples_hierarchical: int = 0,\n",
    "\tkwargs_sample_hierarchical: dict = None,\n",
    "\tfine_model = None,\n",
    "\tviewdirs_encoding_fn: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n",
    "\tchunksize: int = 2**15\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]:\n",
    "\tr\"\"\"\n",
    "\tCompute forward pass through model(s).\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Set no kwargs if none are given.\n",
    "\tif kwargs_sample_stratified is None:\n",
    "\t\tkwargs_sample_stratified = {}\n",
    "\tif kwargs_sample_hierarchical is None:\n",
    "\t\tkwargs_sample_hierarchical = {}\n",
    "\n",
    "\t# Sample query points along each ray.\n",
    "\tquery_points, z_vals = sample_stratified(\n",
    "\t\t\trays_o, rays_d, near, far, **kwargs_sample_stratified)\n",
    "\n",
    "\t# Prepare batches.\n",
    "\tbatches = prepare_chunks(query_points, encoding_fn, chunksize=chunksize)\n",
    "\tif viewdirs_encoding_fn is not None:\n",
    "\t\tbatches_viewdirs = prepare_viewdirs_chunks(query_points, rays_d,\n",
    "                                                viewdirs_encoding_fn,\n",
    "                                                chunksize=chunksize)\n",
    "\telse:\n",
    "\t\tbatches_viewdirs = [None] * len(batches)\n",
    "\n",
    "\t# Coarse model pass.\n",
    "\t# Split the encoded points into \"chunks\", run the model on all chunks, and\n",
    "\t# concatenate the results (to avoid out-of-memory issues).\n",
    "\tpredictions = []\n",
    "\tfor batch, batch_viewdirs in zip(batches, batches_viewdirs):\n",
    "\t\tpredictions.append(coarse_model(batch, viewdirs=batch_viewdirs))\n",
    "\traw = torch.cat(predictions, dim=0)\n",
    "\traw = raw.reshape(list(query_points.shape[:2]) + [raw.shape[-1]])\n",
    "\n",
    "\t# Perform differentiable volume rendering to re-synthesize the RGB image.\n",
    "\trgb_map, depth_map, acc_map, weights = raw2outputs(raw, z_vals, rays_d)\n",
    "\t# rgb_map, depth_map, acc_map, weights = render_volume_density(raw, rays_o, z_vals)\n",
    "\toutputs = {\n",
    "\t\t\t'z_vals_stratified': z_vals\n",
    "\t}\n",
    "\n",
    "\t# Fine model pass.\n",
    "\tif n_samples_hierarchical > 0:\n",
    "\t\t# Save previous outputs to return.\n",
    "\t\trgb_map_0, depth_map_0, acc_map_0 = rgb_map, depth_map, acc_map\n",
    "\n",
    "\t\t# Apply hierarchical sampling for fine query points.\n",
    "\t\tquery_points, z_vals_combined, z_hierarch = sample_hierarchical(\n",
    "\t\t\trays_o, rays_d, z_vals, weights, n_samples_hierarchical,\n",
    "\t\t\t**kwargs_sample_hierarchical)\n",
    "\n",
    "\t\t# Prepare inputs as before.\n",
    "\t\tbatches = prepare_chunks(query_points, encoding_fn, chunksize=chunksize)\n",
    "\t\tif viewdirs_encoding_fn is not None:\n",
    "\t\t\tbatches_viewdirs = prepare_viewdirs_chunks(query_points, rays_d,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t viewdirs_encoding_fn,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t chunksize=chunksize)\n",
    "\t\telse:\n",
    "\t\t\tbatches_viewdirs = [None] * len(batches)\n",
    "\n",
    "\t\t# Forward pass new samples through fine model.\n",
    "\t\tfine_model = fine_model if fine_model is not None else coarse_model\n",
    "\t\tpredictions = []\n",
    "\t\tfor batch, batch_viewdirs in zip(batches, batches_viewdirs):\n",
    "\t\t\tpredictions.append(fine_model(batch, viewdirs=batch_viewdirs))\n",
    "\t\traw = torch.cat(predictions, dim=0)\n",
    "\t\traw = raw.reshape(list(query_points.shape[:2]) + [raw.shape[-1]])\n",
    "\n",
    "\t\t# Perform differentiable volume rendering to re-synthesize the RGB image.\n",
    "\t\trgb_map, depth_map, acc_map, weights = raw2outputs(raw, z_vals_combined, rays_d)\n",
    "\n",
    "\t\t# Store outputs.\n",
    "\t\toutputs['z_vals_hierarchical'] = z_hierarch\n",
    "\t\toutputs['rgb_map_0'] = rgb_map_0\n",
    "\t\toutputs['depth_map_0'] = depth_map_0\n",
    "\t\toutputs['acc_map_0'] = acc_map_0\n",
    "\n",
    "\t# Store outputs.\n",
    "\toutputs['rgb_map'] = rgb_map\n",
    "\toutputs['depth_map'] = depth_map\n",
    "\toutputs['acc_map'] = acc_map\n",
    "\toutputs['weights'] = weights\n",
    "\treturn outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoders\n",
    "d_input = 3           # Number of input dimensions\n",
    "n_freqs = 10          # Number of encoding functions for samples\n",
    "log_space = True      # If set, frequencies scale in log space\n",
    "use_viewdirs = True   # If set, use view direction as input\n",
    "n_freqs_views = 4     # Number of encoding functions for views\n",
    "\n",
    "# Stratified sampling\n",
    "n_samples = 64         # Number of spatial samples per ray\n",
    "perturb = True         # If set, applies noise to sample positions\n",
    "inverse_depth = False  # If set, samples points linearly in inverse depth\n",
    "\n",
    "# Model\n",
    "d_filter = 128          # Dimensions of linear layer filters\n",
    "n_layers = 6#2            # Number of layers in network bottleneck\n",
    "skip = [4]               # Layers at which to apply input residual\n",
    "use_fine_model = True   # If set, creates a fine model\n",
    "d_filter_fine = 128     # Dimensions of linear layer filters of fine network\n",
    "n_layers_fine = 6       # Number of layers in fine network bottleneck\n",
    "\n",
    "# Hierarchical sampling\n",
    "n_samples_hierarchical = 64   # Number of samples per ray\n",
    "perturb_hierarchical = False  # If set, applies noise to sample positions\n",
    "\n",
    "# Optimizer\n",
    "lr = 5e-4  # Learning rate\n",
    "\n",
    "# Training\n",
    "n_iters = 1000#0\n",
    "batch_size = 2**14          # Number of rays per gradient step (power of 2)\n",
    "one_image_per_step = True   # One image per gradient step (disables batching)\n",
    "chunksize = 2**14           # Modify as needed to fit in GPU memory\n",
    "center_crop = True          # Crop the center of image (one_image_per_)\n",
    "center_crop_iters = 50      # Stop cropping center after this many epochs\n",
    "display_rate = 20          # Display test output every X epochs\n",
    "\n",
    "# Early Stopping\n",
    "warmup_iters = 100          # Number of iterations during warmup phase\n",
    "warmup_min_fitness = 10.0   # Min val PSNR to continue training at warmup_iters\n",
    "n_restarts = 10             # Number of times to restart if training stalls\n",
    "\n",
    "# We bundle the kwargs for various functions to pass all at once.\n",
    "kwargs_sample_stratified = {\n",
    "\t'n_samples': n_samples,\n",
    "\t'perturb': perturb,\n",
    "\t'inverse_depth': inverse_depth\n",
    "}\n",
    "kwargs_sample_hierarchical = {\n",
    "\t'perturb': perturb\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_time(i, iternums, train_psnrs, val_psnrs, folder=None):\n",
    "    model.eval()\n",
    "    height, width = testimg.shape[:2]\n",
    "    rays_o, rays_d = get_rays(height, width, focal, testpose)\n",
    "    rays_o = rays_o.reshape([-1, 3]).to(device)\n",
    "    rays_d = rays_d.reshape([-1, 3]).to(device)\n",
    "    outputs = nerf_forward(rays_o, rays_d,\n",
    "                            near, far, encode, model,\n",
    "                            kwargs_sample_stratified=kwargs_sample_stratified,\n",
    "                            n_samples_hierarchical=n_samples_hierarchical,\n",
    "                            kwargs_sample_hierarchical=kwargs_sample_hierarchical,\n",
    "                            fine_model=fine_model,\n",
    "                            viewdirs_encoding_fn=encode_viewdirs,\n",
    "                            chunksize=chunksize)\n",
    "\n",
    "    rgb_predicted = outputs['rgb_map']\n",
    "    testimg_flat = testimg.reshape(-1, 3).to(device)\n",
    "    loss = torch.nn.functional.mse_loss(rgb_predicted, testimg_flat)\n",
    "    print(\"Loss:\", loss.item())\n",
    "    val_psnr = -10. * torch.log10(loss)\n",
    "    val_psnrs.append(val_psnr.item())\n",
    "    iternums.append(i)\n",
    "\n",
    "    # Save predicted image\n",
    "    _img = rgb_predicted.reshape([height, width, 3]).detach().cpu().numpy()\n",
    "    _img = _img * 255\n",
    "    _img = _img.astype(np.uint8)\n",
    "    zerosname = '0'*(4 - len(str(i)))\n",
    "    Image.fromarray(_img).save(f'{folder}/{zerosname + str(i)}.png')\n",
    "\n",
    "    # Plot example outputs outside\n",
    "    render(i, rgb_predicted, iternums, outputs,train_psnrs,val_psnrs)\n",
    "    \n",
    "def render(i, rgb_predicted, iternums, outputs,train_psnrs,val_psnrs):\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(24,4), gridspec_kw={'width_ratios': [1, 1, 1, 3]})\n",
    "    # Plot example outputs\n",
    "    \n",
    "    ax[0].imshow(rgb_predicted.reshape([height, width, 3]).detach().cpu().numpy())\n",
    "    ax[0].set_title(f'Iteration: {i}')\n",
    "    ax[1].imshow(testimg.detach().cpu().numpy())\n",
    "    ax[1].set_title(f'Target')\n",
    "    ax[2].plot(range(0, i + 1), train_psnrs, 'r')\n",
    "    ax[2].plot(iternums, val_psnrs, 'b')\n",
    "    ax[2].set_title('PSNR (train=red, val=blue')\n",
    "    # z_vals_strat = outputs['z_vals_stratified'].view((-1, n_samples))\n",
    "    # z_sample_strat = z_vals_strat[z_vals_strat.shape[0] // 2].detach().cpu().numpy()\n",
    "    # if 'z_vals_hierarchical' in outputs:\n",
    "        # z_vals_hierarch = outputs['z_vals_hierarchical'].view((-1, n_samples_hierarchical))\n",
    "        # z_sample_hierarch = z_vals_hierarch[z_vals_hierarch.shape[0] // 2].detach().cpu().numpy()\n",
    "    # else:\n",
    "        # z_sample_hierarch = None\n",
    "    # _ = plot_samples(z_sample_strat, z_sample_hierarch, ax=ax[3])\n",
    "    ax[3].margins(0)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freq encoder test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = 'results/Freq_encoder'\n",
    "os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "\tr\"\"\"\n",
    "\tSine-cosine positional encoder for input points.\n",
    "\t\"\"\"\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\td_input: int,\n",
    "\t\tn_freqs: int,\n",
    "\t\tlog_space: bool = False\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.d_input = d_input\n",
    "\t\tself.n_freqs = n_freqs\n",
    "\t\tself.log_space = log_space\n",
    "\t\tself.d_output = d_input * (1 + 2 * self.n_freqs)\n",
    "\t\tself.embed_fns = [lambda x: x]\n",
    "\n",
    "\t\t# Define frequencies in either linear or log scale\n",
    "\t\tif self.log_space:\n",
    "\t\t\tfreq_bands = 2.**torch.linspace(0., self.n_freqs - 1, self.n_freqs)\n",
    "\t\telse:\n",
    "\t\t\tfreq_bands = torch.linspace(2.**0., 2.**(self.n_freqs - 1), self.n_freqs)\n",
    "\n",
    "\t\t# Alternate sin and cos\n",
    "\t\tfor freq in freq_bands:\n",
    "\t\t\tself.embed_fns.append(lambda x, freq=freq: torch.sin(x * freq))\n",
    "\t\t\tself.embed_fns.append(lambda x, freq=freq: torch.cos(x * freq))\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tx\n",
    "\t) -> torch.Tensor:\n",
    "\t\tr\"\"\"\n",
    "\t\tApply positional encoding to input.\n",
    "\t\t\"\"\"\n",
    "\t\treturn torch.concat([fn(x) for fn in self.embed_fns], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = PositionalEncoder(d_input, n_freqs, log_space=log_space)\n",
    "encode = lambda x: encoder(x)\n",
    "# View direction encoders\n",
    "\n",
    "encoder_viewdirs = PositionalEncoder(d_input, n_freqs_views,log_space=log_space)\n",
    "encode_viewdirs = lambda x: encoder_viewdirs(x)\n",
    "d_viewdirs = encoder_viewdirs.d_output\n",
    "\n",
    "# Models\n",
    "model = NeRF(\n",
    "    encoder.d_output, n_layers=n_layers, d_filter=d_filter, skip=skip,d_viewdirs=d_viewdirs)\n",
    "model.to(device)\n",
    "\n",
    "fine_model = NeRF(\n",
    "    encoder.d_output, n_layers=n_layers, d_filter=d_filter, skip=skip,d_viewdirs=d_viewdirs)\n",
    "fine_model.to(device)\n",
    "\n",
    "model_params = list(model.parameters())\n",
    "model_params = model_params + list(fine_model.parameters())\n",
    "model_params = model_params + list(encoder.parameters())\n",
    "\n",
    "# Optimizer\n",
    "# optimizer = torch.optim.Adam(model_params, lr=lr)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters(), 'lr': 1e-4},\n",
    "    {'params': fine_model.parameters(), 'lr': 1e-4},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_psnrs = []\n",
    "val_psnrs = []\n",
    "iternums = []\n",
    "\n",
    "for i in trange(1000):\n",
    "\n",
    "    ###\n",
    "    ### Train one time\n",
    "    ###\n",
    "    if one_image_per_step:\n",
    "        # Randomly pick an image as the target.\n",
    "        # target_img_idx = np.random.randint(images.shape[0])\n",
    "        target_img_idx = testimg_idx\n",
    "\n",
    "        target_img = images[target_img_idx].to(device)\n",
    "        \n",
    "        height, width = target_img.shape[:2]\n",
    "        target_pose = poses[target_img_idx].to(device)\n",
    "        rays_o, rays_d = get_rays(height, width, focal, target_pose)\n",
    "        rays_o = rays_o.reshape([-1, 3]).to(device)\n",
    "        rays_d = rays_d.reshape([-1, 3]).to(device)\n",
    "\n",
    "    target_img = target_img.reshape([-1, 3]).to(device)\n",
    "\n",
    "    # Run one iteration of TinyNeRF and get the rendered RGB image.\n",
    "    outputs = nerf_forward(\n",
    "        rays_o, rays_d,\n",
    "        near, far, encode, model,\n",
    "        kwargs_sample_stratified=kwargs_sample_stratified,\n",
    "        n_samples_hierarchical=n_samples_hierarchical,\n",
    "        kwargs_sample_hierarchical=kwargs_sample_hierarchical,\n",
    "        fine_model=fine_model,\n",
    "        viewdirs_encoding_fn=encode_viewdirs,\n",
    "        chunksize=chunksize)\n",
    "\n",
    "    # Check for any numerical issues.\n",
    "    for k, v in outputs.items():\n",
    "        if torch.isnan(v).any():\n",
    "            print(f\"! [Numerical Alert] {k} contains NaN.\")\n",
    "        if torch.isinf(v).any():\n",
    "            print(f\"! [Numerical Alert] {k} contains Inf.\")\n",
    "\n",
    "    # Backprop!\n",
    "    rgb_predicted = outputs['rgb_map']\n",
    "    loss = torch.nn.functional.mse_loss(rgb_predicted, target_img)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    psnr = -10. * torch.log10(loss)\n",
    "    psnr = psnr.item()\n",
    "    if i % 100 == 0:\n",
    "        print(psnr)\n",
    "    train_psnrs += [psnr]\n",
    "\n",
    "\n",
    "    ###\n",
    "    ### Evaluate\n",
    "    ###\n",
    "    if i % 20 == 0:\n",
    "        eval_one_time(i, iternums, train_psnrs, val_psnrs, save_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_train_psnrs = train_psnrs\n",
    "freq_val_psnrs = val_psnrs\n",
    "freq_iternums = iternums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a gif\n",
    "# You need to change scale if you will try to change output size\n",
    "\n",
    "# Change to the images directory\n",
    "os.chdir(save_folder)\n",
    "# Define the ffmpeg command\n",
    "\n",
    "# General \n",
    "# !ffmpeg -y -pattern_type glob -i \"*.png\" -vf \"scale=128:128,fps=48,drawtext=text='%{n}':start_number=0:x=10:y=10:fontsize=12:fontcolor=white\" ../xyzFreq2rgb_animation.gif\n",
    "# os.chdir('../..')\n",
    "\n",
    "# If first one doesn't work \n",
    "# font_path = '/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Bold.ttf'\n",
    "!ffmpeg -y -pattern_type glob -i \"*.png\" -vf \"scale=128:128,fps=48,drawtext=text='%{n}':fontfile=/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Bold.ttf':start_number=0:x=10:y=10:fontsize=24:fontcolor=white\" ../Cam_xyzFreq2rgb_animation.gif\n",
    "os.chdir('../..')\n",
    "\n",
    "# Compress gif with \n",
    "# gifsicle -i ani.gif --lossy=30 --colors 256 -o new_ani.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiresolution Hash Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = 'results/Hash_encoder'\n",
    "os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network grid class. The input x needs to be within [0, 1].\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 feature_dim: int,\n",
    "                 grid_dim: int,\n",
    "                 num_lvl: int,\n",
    "                 max_res: int,\n",
    "                 min_res: int,\n",
    "                 hashtable_power: int,\n",
    "                 device='cpu'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Determine the device to use (CPU or CUDA)\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize the attributes of the grid\n",
    "        self.feature_dim = feature_dim  # Dimensionality of the feature vectors\n",
    "        self.grid_dim = grid_dim  # Dimensionality of the grid (e.g., 2D, 3D)\n",
    "        self.num_lvl = num_lvl  # Number of levels in the grid hierarchy\n",
    "        self.max_res = max_res  # Maximum resolution of the grid\n",
    "        self.min_res = min_res  # Minimum resolution of the grid\n",
    "        self.hashtable_power = hashtable_power  # Power of the hashtable size (number of entries is 2^hashtable_power)\n",
    "\n",
    "        self.d_output = num_lvl * grid_dim\n",
    "\n",
    "        # Constants for hash calculations\n",
    "        self.prime = [3367900313, 2654435761, 805459861]  # Prime numbers for hashing\n",
    "        self.max_entry = 2 ** self.hashtable_power  # Maximum number of entries in the hashtable\n",
    "        # Factor to scale resolutions logarithmically\n",
    "        self.factor_b = np.exp((np.log(self.max_res) - np.log(self.min_res)) / (self.num_lvl - 1))\n",
    "\n",
    "        # Compute the resolutions for each level\n",
    "        self.resolutions = []\n",
    "        for i in range(self.num_lvl):\n",
    "            # Calculate resolution for level i\n",
    "            self.resolutions.append(np.floor(self.min_res * (self.factor_b ** i)))\n",
    "\n",
    "        # Initialize the hashtable for each resolution\n",
    "        self.hashtable = nn.ParameterList([])  # List of hashtables for each resolution\n",
    "        for res in self.resolutions:\n",
    "            total_res = res ** self.grid_dim  # Total number of cells at this resolution\n",
    "            table_size = min(total_res, self.max_entry)  # Size of the hashtable (limited by max_entry)\n",
    "            # Initialize table with random values, scaled as per InstantNGP paper\n",
    "            table = torch.randn(int(table_size), self.feature_dim, device=self.device) *0.001 #* 0.0001 #+ torch.rand(1).to(self.device)\n",
    "            table = nn.Parameter(table)  # Convert to a learnable parameter\n",
    "            self.hashtable.append(table)  # Add to the hashtable list\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalization\n",
    "        _min = torch.tensor([-3.5552, -2.1935, -2.8307]).to(self.device)\n",
    "        _max = torch.tensor([1.8859, 3.1777, 2.0705]).to(self.device)\n",
    "        x = (x - _min ) / (_max - _min)\n",
    "\n",
    "        # print(x.min(0)[0], x.max(0)[0])\n",
    "        \n",
    "        out_feature = []\n",
    "        for lvl in range(self.num_lvl):\n",
    "            # Transform coordinates to hash space\n",
    "            coord = self.to_hash_space(x, self.resolutions[lvl])\n",
    "            floor_corner = torch.floor(coord)  # Find the floor corner for interpolation\n",
    "            # Get the corners for interpolation\n",
    "            corners = self.get_corner(floor_corner).to(torch.long)\n",
    "            # Hash the corners to get feature indices\n",
    "            feature_index = self.hash(corners, self.hashtable[lvl].shape[0], self.resolutions[lvl])\n",
    "            flat_feature_index = feature_index.to(torch.long).flatten()  # Flatten the indices\n",
    "            # Retrieve corner features from the hashtable\n",
    "            corner_feature = torch.reshape(self.hashtable[lvl][flat_feature_index],\n",
    "                                           (corners.shape[0], corners.shape[1], self.feature_dim))\n",
    "            # Calculate interpolation weights\n",
    "            weights = self.interpolation_weights(coord - floor_corner)\n",
    "            weights = torch.stack([weights, weights, weights], -1)  # Stack weights for each feature\n",
    "            # Perform weighted interpolation of corner features\n",
    "            weighted_feature = corner_feature * weights\n",
    "            summed_feature = weighted_feature.sum(-2)  # Sum the weighted features\n",
    "            out_feature.append(summed_feature)  # Append the result to the output feature list\n",
    "        return torch.cat(out_feature, -1)  # Concatenate features from all levels\n",
    "\n",
    "    def to_hash_space(self, x, resolution):\n",
    "        \"\"\"\n",
    "        Transform input coordinates to hash space, ensuring they are within the grid's resolution.\n",
    "        \"\"\"\n",
    "        return torch.clip(x * (resolution - 1), 0, resolution - 1.0001)  # Scale and clip coordinates\n",
    "\n",
    "    def interpolation_weights(self, diff):\n",
    "        \"\"\"\n",
    "        Calculate the interpolation weights based on the differences from the floor corner.\n",
    "        \"\"\"\n",
    "        ones = torch.ones_like(diff, device=self.device)  # Create a tensor of ones with the same shape as diff\n",
    "        minus_x = (ones - diff)[..., 0]  # Calculate 1 - x for each dimension\n",
    "        x = diff[..., 0]  # Get the x difference\n",
    "        minus_y = (ones - diff)[..., 1]  # Calculate 1 - y for each dimension\n",
    "        y = diff[..., 1]  # Get the y difference\n",
    "\n",
    "        if self.grid_dim == 2:\n",
    "            # For 2D, calculate weights for the four corners\n",
    "            stacks = torch.stack([minus_x * minus_y, x * minus_y, minus_x * y, x * y], -1)\n",
    "            return stacks\n",
    "        else:\n",
    "            # For 3D, calculate weights for the eight corners\n",
    "            minus_z = (ones - diff)[..., 2]  # Calculate 1 - z for each dimension\n",
    "            z = diff[..., 2]  # Get the z difference\n",
    "            stacks = torch.stack([minus_x * minus_y * minus_z,\n",
    "                                  x * minus_y * minus_z,\n",
    "                                  minus_x * y * minus_z,\n",
    "                                  x * y * minus_z,\n",
    "                                  minus_x * minus_y * z,\n",
    "                                  x * minus_y * z,\n",
    "                                  minus_x * y * z,\n",
    "                                  x * y * z], -1)\n",
    "            return stacks\n",
    "\n",
    "    def alt_weights(self, corner, coord):\n",
    "        \"\"\"\n",
    "        Alternative method for calculating weights based on the distance to the corners.\n",
    "        \"\"\"\n",
    "        diag_length = torch.full_like(coord[:, 0], 2. ** (1 / 2), device=self.device)  # Diagonal length for normalization\n",
    "        w = torch.empty(corner.shape[0], corner.shape[1], device=self.device)  # Initialize weight tensor\n",
    "        for c in range(corner.shape[1]):\n",
    "            dist = torch.norm(corner[:, c, :] - coord, dim=1)  # Calculate distance to each corner\n",
    "            w[:, c] = diag_length - dist  # Calculate weight based on distance\n",
    "        normed_w = torch.nn.functional.normalize(w, p=1)  # Normalize the weights\n",
    "        return normed_w\n",
    "\n",
    "    def hash(self, x, num_entry, res):\n",
    "        \"\"\"\n",
    "        Hash function to map coordinates to hashtable indices.\n",
    "        \"\"\"\n",
    "        if num_entry != self.max_entry:\n",
    "            # For smaller hashtables, use a simple linear hash\n",
    "            index = 0\n",
    "            for i in range(self.grid_dim):\n",
    "                index += x[..., i] * res ** i\n",
    "            return index\n",
    "        else:\n",
    "            # For larger hashtables, use a more complex hash with primes\n",
    "            _sum = 0\n",
    "            for i in range(self.grid_dim):\n",
    "                _sum = _sum ^ (x[..., i] * self.prime[i])\n",
    "            index = _sum % num_entry  # Modulo operation to keep within table size\n",
    "            return index\n",
    "\n",
    "    def get_corner(self, floor_corner):\n",
    "        \"\"\"\n",
    "        Get the corner points for interpolation based on the floor corner.\n",
    "        \"\"\"\n",
    "        num_entry = floor_corner.shape[0]  # Number of entries\n",
    "\n",
    "        if self.grid_dim == 2:\n",
    "            # Calculate corners for 2D grids\n",
    "            c000 = floor_corner\n",
    "            c001 = floor_corner + torch.tensor([0, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c010 = floor_corner + torch.tensor([1, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c011 = floor_corner + torch.ones_like(floor_corner, device=self.device)\n",
    "            stacks = torch.stack([c000, c010, c001, c011], -2)\n",
    "            return stacks\n",
    "        else:\n",
    "            # Calculate corners for 3D grids\n",
    "            c000 = floor_corner\n",
    "            c001 = floor_corner + torch.tensor([0, 0, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c010 = floor_corner + torch.tensor([0, 1, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c011 = floor_corner + torch.tensor([0, 1, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c100 = floor_corner + torch.tensor([1, 0, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c101 = floor_corner + torch.tensor([1, 0, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c110 = floor_corner + torch.tensor([1, 1, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c111 = floor_corner + torch.ones_like(floor_corner, device=self.device)\n",
    "            stacks = torch.stack([c000, c010, c001, c011, c100, c101, c110, c111], -2)\n",
    "            return stacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "encoder = Grid(\n",
    "    feature_dim=3,\n",
    "    grid_dim=3,\n",
    "    num_lvl=12,\n",
    "    max_res=1024, #1024*2+1024*2,\n",
    "    min_res=16,\n",
    "    hashtable_power=19,\n",
    "    device=device,\n",
    ").to(device)\n",
    "encode = lambda x: encoder(x)\n",
    "\n",
    "# View direction encoders\n",
    "\n",
    "encoder_viewdirs = PositionalEncoder(3, 4)\n",
    "encode_viewdirs = lambda x: encoder_viewdirs(x)\n",
    "d_viewdirs = encoder_viewdirs.d_output\n",
    "\n",
    "# Models\n",
    "model = NeRF(\n",
    "    encoder.d_output, n_layers=n_layers, d_filter=d_filter, skip=skip,d_viewdirs=d_viewdirs)\n",
    "model.to(device)\n",
    "\n",
    "fine_model = NeRF(\n",
    "    encoder.d_output, n_layers=n_layers, d_filter=d_filter, skip=skip,d_viewdirs=d_viewdirs)\n",
    "fine_model.to(device)\n",
    "\n",
    "model_params = list(model.parameters())\n",
    "model_params = model_params + list(fine_model.parameters())\n",
    "model_params = model_params + list(encoder.parameters())\n",
    "\n",
    "# Optimizer\n",
    "# optimizer = torch.optim.Adam(model_params, lr=lr)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters(), 'lr': 1e-4},\n",
    "    {'params': fine_model.parameters(), 'lr': 1e-4},\n",
    "    {'params': encoder.parameters(), 'lr': 5e-4},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_psnrs = []\n",
    "val_psnrs = []\n",
    "iternums = []\n",
    "\n",
    "for i in trange(1000):\n",
    "    ###\n",
    "    ### Train one time\n",
    "    ###\n",
    "    if one_image_per_step:\n",
    "        # Randomly pick an image as the target.\n",
    "        # target_img_idx = np.random.randint(images.shape[0])\n",
    "        target_img_idx = testimg_idx\n",
    "\n",
    "        target_img = images[target_img_idx].to(device)\n",
    "        \n",
    "        height, width = target_img.shape[:2]\n",
    "        target_pose = poses[target_img_idx].to(device)\n",
    "        rays_o, rays_d = get_rays(height, width, focal, target_pose)\n",
    "        rays_o = rays_o.reshape([-1, 3]).to(device)\n",
    "        rays_d = rays_d.reshape([-1, 3]).to(device)\n",
    "\n",
    "    target_img = target_img.reshape([-1, 3]).to(device)\n",
    "\n",
    "    # Run one iteration of TinyNeRF and get the rendered RGB image.\n",
    "    outputs = nerf_forward(\n",
    "        rays_o, rays_d,\n",
    "        near, far, encode, model,\n",
    "        kwargs_sample_stratified=kwargs_sample_stratified,\n",
    "        n_samples_hierarchical=n_samples_hierarchical,\n",
    "        kwargs_sample_hierarchical=kwargs_sample_hierarchical,\n",
    "        fine_model=fine_model,\n",
    "        viewdirs_encoding_fn=encode_viewdirs,\n",
    "        chunksize=chunksize)\n",
    "\n",
    "    # Check for any numerical issues.\n",
    "    for k, v in outputs.items():\n",
    "        if torch.isnan(v).any():\n",
    "            print(f\"! [Numerical Alert] {k} contains NaN.\")\n",
    "        if torch.isinf(v).any():\n",
    "            print(f\"! [Numerical Alert] {k} contains Inf.\")\n",
    "\n",
    "    # Backprop!\n",
    "    rgb_predicted = outputs['rgb_map']\n",
    "    loss = torch.nn.functional.mse_loss(rgb_predicted, target_img)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    psnr = -10. * torch.log10(loss)\n",
    "    psnr = psnr.item()\n",
    "    if i % 100 == 0:\n",
    "        print(psnr)\n",
    "    train_psnrs += [psnr]\n",
    "\n",
    "\n",
    "    ###\n",
    "    ### Evaluate\n",
    "    ###\n",
    "    if i % 20 == 0:\n",
    "        eval_one_time(i, iternums, train_psnrs, val_psnrs, save_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a gif\n",
    "# You need to change scale if you will try to change output size\n",
    "\n",
    "# Change to the images directory\n",
    "os.chdir(save_folder)\n",
    "# Define the ffmpeg command\n",
    "\n",
    "# General \n",
    "# !ffmpeg -y -pattern_type glob -i \"*.png\" -vf \"scale=128:128,fps=48,drawtext=text='%{n}':start_number=0:x=10:y=10:fontsize=12:fontcolor=white\" ../xyzFreq2rgb_animation.gif\n",
    "# os.chdir('../..')\n",
    "\n",
    "# If first one doesn't work \n",
    "# font_path = '/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Bold.ttf'\n",
    "!ffmpeg -y -pattern_type glob -i \"*.png\" -vf \"scale=128:128,fps=48,drawtext=text='%{n}':fontfile=/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Bold.ttf':start_number=0:x=10:y=10:fontsize=24:fontcolor=white\" ../Cam_xyzHash2rgb_animation.gif\n",
    "os.chdir('../..')\n",
    "\n",
    "# Compress gif with \n",
    "# gifsicle -i ani.gif --lossy=30 --colors 256 -o new_ani.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
