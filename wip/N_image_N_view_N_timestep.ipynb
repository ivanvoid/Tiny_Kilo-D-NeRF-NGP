{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from logging import debug\n",
    "\n",
    "logging.basicConfig(filename='debug_logs.log',\n",
    "                    level=logging.DEBUG,\n",
    "                    # level=logging.ERROR,\n",
    "                    filemode='w',\n",
    "                    # format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                    # datefmt='%H:%M:%S',\n",
    "                    )\n",
    "\n",
    "datalog = logging.getLogger('data_log',)\n",
    "modellog = logging.getLogger('model_log',)\n",
    "# datalog.setLevel(logging.DEBUG)\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "logging.getLogger('matplotlib.pyplot').disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One pass\n",
    "%matplotlib inline\n",
    "import os\n",
    "from typing import Optional, Tuple, List, Union, Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# For repeatability\n",
    "seed = 2024\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "debug('#'*32)\n",
    "debug('RUN')\n",
    "debug('#'*32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network grid class. The input x needs to be within [0, 1].\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 feature_dim: int,\n",
    "                 grid_dim: int,\n",
    "                 num_lvl: int,\n",
    "                 max_res: int,\n",
    "                 min_res: int,\n",
    "                 hashtable_power: int,\n",
    "                 device='cpu'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Determine the device to use (CPU or CUDA)\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize the attributes of the grid\n",
    "        self.feature_dim = feature_dim  # Dimensionality of the feature vectors\n",
    "        self.grid_dim = grid_dim  # Dimensionality of the grid (e.g., 2D, 3D)\n",
    "        self.num_lvl = num_lvl  # Number of levels in the grid hierarchy\n",
    "        self.max_res = max_res  # Maximum resolution of the grid\n",
    "        self.min_res = min_res  # Minimum resolution of the grid\n",
    "        self.hashtable_power = hashtable_power  # Power of the hashtable size (number of entries is 2^hashtable_power)\n",
    "\n",
    "        self.d_output = num_lvl * feature_dim\n",
    "\n",
    "        # Constants for hash calculations\n",
    "        self.prime = [3367900313, 2654435761, 805459861, 9322189897]  # Prime numbers for hashing\n",
    "        self.max_entry = 2 ** self.hashtable_power  # Maximum number of entries in the hashtable\n",
    "        # Factor to scale resolutions logarithmically\n",
    "        self.factor_b = np.exp((np.log(self.max_res) - np.log(self.min_res)) / (self.num_lvl - 1))\n",
    "\n",
    "        # Compute the resolutions for each level\n",
    "        self.resolutions = []\n",
    "        for i in range(self.num_lvl):\n",
    "            # Calculate resolution for level i\n",
    "            self.resolutions.append(np.floor(self.min_res * (self.factor_b ** i)))\n",
    "\n",
    "        # Initialize the hashtable for each resolution\n",
    "        self.hashtable = nn.ParameterList([])  # List of hashtables for each resolution\n",
    "        for res in self.resolutions:\n",
    "            total_res = res ** self.grid_dim  # Total number of cells at this resolution\n",
    "            table_size = min(total_res, self.max_entry)  # Size of the hashtable (limited by max_entry)\n",
    "            # Initialize table with random values, scaled as per InstantNGP paper\n",
    "            table = torch.randn(int(table_size), self.feature_dim, device=self.device) *0.001 #* 0.0001 #+ torch.rand(1).to(self.device)\n",
    "            table = nn.Parameter(table)  # Convert to a learnable parameter\n",
    "            self.hashtable.append(table)  # Add to the hashtable list\n",
    "\n",
    "    def forward(self, x):\n",
    "        # if x.shape[-1] == 4:\n",
    "        #     # Normalization\n",
    "        #     _min = torch.tensor([-6, -6, -6, 0]).to(self.device)\n",
    "        #     _max = torch.tensor([ 6,  6,  3, 1]).to(self.device)\n",
    "        #     x = (x - _min ) / (_max - _min)\n",
    "        datalog.debug(f'X in HASH GRID distribution {x.min(), x.max()}')\n",
    "        \n",
    "        out_feature = []\n",
    "        for lvl in range(self.num_lvl):\n",
    "            # Transform coordinates to hash space\n",
    "            coord = self.to_hash_space(x, self.resolutions[lvl])\n",
    "            # Find the floor corner for interpolation\n",
    "            floor_corner = torch.floor(coord)  \n",
    "            debug('Floor Corners shape: '+str(floor_corner.shape))\n",
    "\n",
    "            # Get the corners for interpolation\n",
    "            corners = self.get_corner(floor_corner).to(torch.long)\n",
    "            debug('Corners shape: '+str(corners.shape))\n",
    "\n",
    "            # Hash the corners to get feature indices\n",
    "            feature_index = self.hash(corners, self.hashtable[lvl].shape[0], self.resolutions[lvl])\n",
    "            flat_feature_index = feature_index.to(torch.long).flatten()  # Flatten the indices\n",
    "            debug('Feature_index / flat shape: '+str(feature_index.shape)+' / '+str(flat_feature_index.shape))\n",
    "\n",
    "            # Retrieve corner features from the hashtable\n",
    "            corner_feature = torch.reshape(\n",
    "                self.hashtable[lvl][flat_feature_index],\n",
    "                (corners.shape[0], corners.shape[1], self.feature_dim))\n",
    "            debug('Corner_feature shape: '+str(corner_feature.shape))\n",
    "\n",
    "            # Calculate interpolation weights\n",
    "            weights = self.interpolation_weights(coord - floor_corner)\n",
    "            debug('WEIGHTS Before stack shape: '+str(weights.shape))\n",
    "            \n",
    "            # Stack weights for each feature\n",
    "            weights = torch.stack([weights]*self.feature_dim, -1)  \n",
    "            # if self.grid_dim == 2:\n",
    "            #     weights = torch.stack([weights, weights], -1)  \n",
    "            # elif self.grid_dim == 3:\n",
    "            #     weights = torch.stack([weights, weights, weights], -1)  \n",
    "            # elif self.grid_dim == 4:\n",
    "            #     weights = torch.stack([weights, weights, weights, weights], -1)  \n",
    "            debug('WEIGHTS After stack: '+str(weights.shape))\n",
    "\n",
    "            # Perform weighted interpolation of corner features\n",
    "            debug('Corner_feature shape: '+str(corner_feature.shape))\n",
    "            weighted_feature = corner_feature * weights\n",
    "            summed_feature = weighted_feature.sum(-2)  # Sum the weighted features\n",
    "            out_feature.append(summed_feature)  # Append the result to the output feature list\n",
    "        outs = torch.cat(out_feature, -1)  # Concatenate features from all levels\n",
    "        # debug('Hash (max:min) check: '+str(outs.max().item())+' : '+str(outs.min().item()))\n",
    "        return outs\n",
    "\n",
    "    def to_hash_space(self, x, resolution):\n",
    "        \"\"\"\n",
    "        Transform input coordinates to hash space, ensuring they are within the grid's resolution.\n",
    "        \"\"\"\n",
    "        return torch.clip(x * (resolution - 1), 0, resolution - 1.0001)  # Scale and clip coordinates\n",
    "\n",
    "    def interpolation_weights(self, diff):\n",
    "        \"\"\"\n",
    "        Calculate the interpolation weights based on the differences from the floor corner.\n",
    "        \"\"\"\n",
    "        ones = torch.ones_like(diff, device=self.device)  # Create a tensor of ones with the same shape as diff\n",
    "        minus_x = (ones - diff)[..., 0]  # Calculate 1 - x for each dimension\n",
    "        x = diff[..., 0]  # Get the x difference\n",
    "        minus_y = (ones - diff)[..., 1]  # Calculate 1 - y for each dimension\n",
    "        y = diff[..., 1]  # Get the y difference\n",
    "\n",
    "        if self.grid_dim == 2:\n",
    "            # For 2D, calculate weights for the four corners\n",
    "            stacks = torch.stack([minus_x * minus_y, x * minus_y, minus_x * y, x * y], -1)\n",
    "            return stacks\n",
    "        elif self.grid_dim == 3:\n",
    "            # For 3D, calculate weights for the eight corners\n",
    "            minus_z = (ones - diff)[..., 2]  # Calculate 1 - z for each dimension\n",
    "            z = diff[..., 2]  # Get the z difference\n",
    "            stacks = torch.stack([minus_x * minus_y * minus_z,\n",
    "                                x * minus_y * minus_z,\n",
    "                                minus_x * y * minus_z,\n",
    "                                x * y * minus_z,\n",
    "                                minus_x * minus_y * z,\n",
    "                                x * minus_y * z,\n",
    "                                minus_x * y * z,\n",
    "                                x * y * z], -1)\n",
    "            return stacks\n",
    "        elif self.grid_dim == 4:\n",
    "            # For 4D, calculate weights for the sixteen corners\n",
    "            minus_z = (ones - diff)[..., 2]  # Calculate 1 - z for each dimension\n",
    "            z = diff[..., 2]  # Get the z difference\n",
    "            minus_w = (ones - diff)[..., 3]  # Calculate 1 - w for each dimension\n",
    "            w = diff[..., 3]  # Get the w difference\n",
    "            stacks = torch.stack([minus_x * minus_y * minus_z * minus_w,\n",
    "                                x * minus_y * minus_z * minus_w,\n",
    "                                minus_x * y * minus_z * minus_w,\n",
    "                                x * y * minus_z * minus_w,\n",
    "                                minus_x * minus_y * z * minus_w,\n",
    "                                x * minus_y * z * minus_w,\n",
    "                                minus_x * y * z * minus_w,\n",
    "                                x * y * z * minus_w,\n",
    "                                minus_x * minus_y * minus_z * w,\n",
    "                                x * minus_y * minus_z * w,\n",
    "                                minus_x * y * minus_z * w,\n",
    "                                x * y * minus_z * w,\n",
    "                                minus_x * minus_y * z * w,\n",
    "                                x * minus_y * z * w,\n",
    "                                minus_x * y * z * w,\n",
    "                                x * y * z * w], -1)\n",
    "            return stacks\n",
    "\n",
    "    def alt_weights(self, corner, coord):\n",
    "        \"\"\"\n",
    "        Alternative method for calculating weights based on the distance to the corners.\n",
    "        \"\"\"\n",
    "        diag_length = torch.full_like(coord[:, 0], 2. ** (1 / 2), device=self.device)  # Diagonal length for normalization\n",
    "        w = torch.empty(corner.shape[0], corner.shape[1], device=self.device)  # Initialize weight tensor\n",
    "        for c in range(corner.shape[1]):\n",
    "            dist = torch.norm(corner[:, c, :] - coord, dim=1)  # Calculate distance to each corner\n",
    "            w[:, c] = diag_length - dist  # Calculate weight based on distance\n",
    "        normed_w = torch.nn.functional.normalize(w, p=1)  # Normalize the weights\n",
    "        return normed_w\n",
    "\n",
    "    def hash(self, x, num_entry, res):\n",
    "        \"\"\"\n",
    "        Hash function to map coordinates to hashtable indices.\n",
    "        \"\"\"\n",
    "        if num_entry != self.max_entry:\n",
    "            # For smaller hashtables, use a simple linear hash\n",
    "            index = 0\n",
    "            for i in range(self.grid_dim):\n",
    "                index += x[..., i] * res ** i\n",
    "            return index\n",
    "        else:\n",
    "            # For larger hashtables, use a more complex hash with primes\n",
    "            _sum = 0\n",
    "            for i in range(self.grid_dim):\n",
    "                _sum = _sum ^ (x[..., i] * self.prime[i])\n",
    "            index = _sum % num_entry  # Modulo operation to keep within table size\n",
    "            return index\n",
    "\n",
    "    def get_corner(self, floor_corner):\n",
    "        \"\"\"\n",
    "        Get the corner points for interpolation based on the floor corner.\n",
    "        \"\"\"\n",
    "        num_entry = floor_corner.shape[0]  # Number of entries\n",
    "\n",
    "        if self.grid_dim == 2:\n",
    "            # Calculate corners for 2D grids\n",
    "            c000 = floor_corner\n",
    "            c001 = floor_corner + torch.tensor([0, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c010 = floor_corner + torch.tensor([1, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c011 = floor_corner + torch.ones_like(floor_corner, device=self.device)\n",
    "            debug('Last corner points shape in weights: '+str(c011.shape))\n",
    "            stacks = torch.stack([c000, c010, c001, c011], -2)\n",
    "            debug('Stacks shape: '+str(stacks.shape))\n",
    "            return stacks\n",
    "        elif self.grid_dim == 3:\n",
    "            # Calculate corners for 3D grids\n",
    "            c000 = floor_corner\n",
    "            c001 = floor_corner + torch.tensor([0, 0, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c010 = floor_corner + torch.tensor([0, 1, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c011 = floor_corner + torch.tensor([0, 1, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c100 = floor_corner + torch.tensor([1, 0, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c101 = floor_corner + torch.tensor([1, 0, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c110 = floor_corner + torch.tensor([1, 1, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c111 = floor_corner + torch.ones_like(floor_corner, device=self.device)\n",
    "            debug('Last corner points shape in weights: '+str(c111.shape))\n",
    "            stacks = torch.stack([c000, c010, c001, c011, c100, c101, c110, c111], -2)\n",
    "            debug('Stacks shape: '+str(stacks.shape))\n",
    "            return stacks\n",
    "        elif self.grid_dim == 4:\n",
    "            # Calculate corners for 4D grids\n",
    "            c0000 = floor_corner\n",
    "            c0001 = floor_corner + torch.tensor([0, 0, 0, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c0010 = floor_corner + torch.tensor([0, 0, 1, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c0011 = floor_corner + torch.tensor([0, 0, 1, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c0100 = floor_corner + torch.tensor([0, 1, 0, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c0101 = floor_corner + torch.tensor([0, 1, 0, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c0110 = floor_corner + torch.tensor([0, 1, 1, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c0111 = floor_corner + torch.tensor([0, 1, 1, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c1000 = floor_corner + torch.tensor([1, 0, 0, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c1001 = floor_corner + torch.tensor([1, 0, 0, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c1010 = floor_corner + torch.tensor([1, 0, 1, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c1011 = floor_corner + torch.tensor([1, 0, 1, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c1100 = floor_corner + torch.tensor([1, 1, 0, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c1101 = floor_corner + torch.tensor([1, 1, 0, 1], device=self.device).repeat(num_entry, 1)\n",
    "            c1110 = floor_corner + torch.tensor([1, 1, 1, 0], device=self.device).repeat(num_entry, 1)\n",
    "            c1111 = floor_corner + torch.ones_like(floor_corner, device=self.device)\n",
    "            debug('Last corner points shape in weights: '+str(c1111.shape))\n",
    "            stacks = torch.stack([\n",
    "                c0000, c0001, c0010, c0011, c0100, c0101, c0110, c0111, \n",
    "                c1000, c1001, c1010, c1011, c1100, c1101, c1110, c1111], -2)\n",
    "            debug('Stacks shape: '+str(stacks.shape))\n",
    "            return stacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug('GRID DEBUG FINISHED!')\n",
    "debug('#'*32)\n",
    "# raise SystemExit(\"Stop right there!\")\n",
    "def _exit():\n",
    "    raise SystemExit(\"Stop right there!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# SHULD COVER WHOLE TIME DOMAIN???\n",
    "N_samples = 60\n",
    "\n",
    "data = np.load('../data/tiny_dnerf_data.npz')\n",
    "\n",
    "n_training = N_samples\n",
    "# testimg_idx = 1\n",
    "# testimg_idx2 = 20\n",
    "\n",
    "images = torch.from_numpy(data['images'][:n_training,:,:,:3])\n",
    "poses = torch.from_numpy(data['poses'][:n_training])\n",
    "focal = torch.from_numpy(data['focal'])\n",
    "times = torch.from_numpy(data['times'][:n_training])\n",
    "\n",
    "# Test\n",
    "testimgs = torch.from_numpy(data['images'][:,:,:,:3])\n",
    "testposes = torch.from_numpy(data['poses'])\n",
    "testtimes = torch.tensor([data['times']])[0]\n",
    "\n",
    "height, width = images.shape[1:3]\n",
    "near, far = 2., 6.\n",
    "\n",
    "fig, ax = plt.subplots(1,N_samples)\n",
    "for i in range(N_samples):\n",
    "    _img = Image.fromarray((testimgs[i]*255).numpy().astype(np.uint8))\n",
    "    ax[i].imshow(_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rays(\n",
    "    height: int,\n",
    "    width: int,\n",
    "    focal_length: float,\n",
    "    c2w: torch.Tensor\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    r\"\"\"\n",
    "    Find origin and direction of rays through every pixel and camera origin.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply pinhole camera model to gather directions at each pixel\n",
    "    i, j = torch.meshgrid(\n",
    "            torch.arange(width, dtype=torch.float32).to(c2w),\n",
    "            torch.arange(height, dtype=torch.float32).to(c2w),\n",
    "            indexing='ij')\n",
    "    i, j = i.transpose(-1, -2), j.transpose(-1, -2)\n",
    "    directions = torch.stack([(i - width * .5) / focal_length,\n",
    "                                                        -(j - height * .5) / focal_length,\n",
    "                                                        -torch.ones_like(i)\n",
    "                                                     ], dim=-1)\n",
    "\n",
    "    # Apply camera pose to directions\n",
    "    rays_d = torch.sum(directions[..., None, :] * c2w[:3, :3], dim=-1)\n",
    "\n",
    "    # Origin is same for all directions (the optical center)\n",
    "    rays_o = c2w[:3, -1].expand(rays_d.shape)\n",
    "    return rays_o, rays_d\n",
    "    \n",
    "def sample_stratified(\n",
    "    rays_o: torch.Tensor,\n",
    "    rays_d: torch.Tensor,\n",
    "    near: float,\n",
    "    far: float,\n",
    "    n_samples: int,\n",
    "    perturb: Optional[bool] = True,\n",
    "    inverse_depth: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    r\"\"\"\n",
    "    Sample along ray from regularly-spaced bins.\n",
    "    \"\"\"\n",
    "\n",
    "    # Grab samples for space integration along ray\n",
    "    t_vals = torch.linspace(0., 1., n_samples, device=rays_o.device)\n",
    "    if not inverse_depth:\n",
    "        # Sample linearly between `near` and `far`\n",
    "        z_vals = near * (1.-t_vals) + far * (t_vals)\n",
    "    else:\n",
    "        # Sample linearly in inverse depth (disparity)\n",
    "        z_vals = 1./(1./near * (1.-t_vals) + 1./far * (t_vals))\n",
    "\n",
    "    # Draw uniform samples from bins along ray\n",
    "    if perturb:\n",
    "        mids = .5 * (z_vals[1:] + z_vals[:-1])\n",
    "        upper = torch.concat([mids, z_vals[-1:]], dim=-1)\n",
    "        lower = torch.concat([z_vals[:1], mids], dim=-1)\n",
    "        t_rand = torch.rand([n_samples], device=z_vals.device)\n",
    "        z_vals = lower + (upper - lower) * t_rand\n",
    "    z_vals = z_vals.expand(list(rays_o.shape[:-1]) + [n_samples])\n",
    "\n",
    "    # Apply scale from `rays_d` and offset from `rays_o` to samples\n",
    "    # pts: (width, height, n_samples, 3)\n",
    "    pts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals[..., :, None]\n",
    "    return pts, z_vals\n",
    "\n",
    "def raw2outputs(\n",
    "    raw: torch.Tensor,\n",
    "    z_vals: torch.Tensor,\n",
    "    rays_d: torch.Tensor,\n",
    "    raw_noise_std: float = 0.0,\n",
    "    white_bkgd: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    r\"\"\"\n",
    "    Convert the raw NeRF output into RGB and other maps.\n",
    "    \"\"\"\n",
    "    device = raw.device\n",
    "    # Difference between consecutive elements of `z_vals`. [n_rays, n_samples]\n",
    "    dists = z_vals[..., 1:] - z_vals[..., :-1]\n",
    "    dists = torch.cat([dists, 1e10 * torch.ones_like(dists[..., :1])], dim=-1)\n",
    "\n",
    "    # Multiply each distance by the norm of its corresponding direction ray\n",
    "    # to convert to real world distance (accounts for non-unit directions).\n",
    "    dists = dists * torch.norm(rays_d[..., None, :], dim=-1)\n",
    "\n",
    "    # Add noise to model's predictions for density. Can be used to\n",
    "    # regularize network during training (prevents floater artifacts).\n",
    "    noise = 0.\n",
    "    if raw_noise_std > 0.:\n",
    "        noise = torch.randn(raw[..., 3].shape) * raw_noise_std\n",
    "\n",
    "    # Predict density of each sample along each ray. Higher values imply\n",
    "    # higher likelihood of being absorbed at this point. [n_rays, n_samples]\n",
    "    alpha = 1.0 - torch.exp(-nn.functional.relu(raw[..., 3] + noise) * dists.to(device))\n",
    "\n",
    "    # Compute weight for RGB of each sample along each ray. [n_rays, n_samples]\n",
    "    # The higher the alpha, the lower subsequent weights are driven.\n",
    "    weights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n",
    "\n",
    "    # Compute weighted RGB map.\n",
    "    rgb = torch.sigmoid(raw[..., :3])  # [n_rays, n_samples, 3]\n",
    "    rgb_map = torch.sum(weights[..., None] * rgb, dim=-2)  # [n_rays, 3]\n",
    "\n",
    "    # Estimated depth map is predicted distance.\n",
    "    depth_map = torch.sum(weights * z_vals.to(device), dim=-1)\n",
    "\n",
    "    # Disparity map is inverse depth.\n",
    "    disp_map = 1. / torch.max(1e-10 * torch.ones_like(depth_map),\n",
    "                                                        depth_map / torch.sum(weights, -1))\n",
    "\n",
    "    # Sum of weights along each ray. In [0, 1] up to numerical error.\n",
    "    acc_map = torch.sum(weights, dim=-1)\n",
    "\n",
    "    # To composite onto a white background, use the accumulated alpha map.\n",
    "    if white_bkgd:\n",
    "        rgb_map = rgb_map + (1. - acc_map[..., None])\n",
    "\n",
    "    return rgb_map, depth_map, acc_map, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRF(nn.Module):\n",
    "    r\"\"\"\n",
    "    Neural radiance fields module.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_input: int = 3,\n",
    "        n_layers: int = 8,\n",
    "        d_filter: int = 256,\n",
    "        skip: Tuple[int] = (4,),\n",
    "        d_viewdirs: Optional[int] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_input = d_input\n",
    "        self.skip = skip\n",
    "        # self.act = nn.functional.relu\n",
    "        self.act = nn.LeakyReLU()\n",
    "        self.d_viewdirs = d_viewdirs\n",
    "\n",
    "        # Create model layers\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Linear(self.d_input, d_filter)] +\n",
    "            [nn.Linear(d_filter + self.d_input, d_filter) if i in skip \\\n",
    "             else nn.Linear(d_filter, d_filter) for i in range(n_layers - 1)]\n",
    "        )\n",
    "\n",
    "        # Bottleneck layers\n",
    "        if self.d_viewdirs is not None:\n",
    "            # If using viewdirs, split alpha and RGB\n",
    "            self.alpha_out = nn.Linear(d_filter, 1)\n",
    "            self.rgb_filters = nn.Linear(d_filter, d_filter)\n",
    "            self.branch = nn.Linear(d_filter + self.d_viewdirs, d_filter // 2)\n",
    "            self.output = nn.Linear(d_filter // 2, 3)\n",
    "        else:\n",
    "            # If no viewdirs, use simpler output\n",
    "            self.output = nn.Linear(d_filter, 4)\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        viewdirs: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Forward pass with optional view direction.\n",
    "        \"\"\"\n",
    "        debug(f'MODEL FORWARD: X shape: {x.shape}, viewdirs shape{viewdirs.shape}')\n",
    "\n",
    "        # Cannot use viewdirs if instantiated with d_viewdirs = None\n",
    "        if self.d_viewdirs is None and viewdirs is not None:\n",
    "            raise ValueError('Cannot input x_direction if d_viewdirs was not given.')\n",
    "\n",
    "        # Apply forward pass up to bottleneck\n",
    "        x_input = x\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = self.act(layer(x))\n",
    "            if i in self.skip:\n",
    "                x = torch.cat([x, x_input], dim=-1)\n",
    "\n",
    "        # Apply bottleneck\n",
    "        if self.d_viewdirs is not None:\n",
    "            # Split alpha from network output\n",
    "            alpha = self.alpha_out(x)\n",
    "\n",
    "            # Pass through bottleneck to get RGB\n",
    "            x = self.rgb_filters(x)\n",
    "            x = torch.concat([x, viewdirs], dim=-1)\n",
    "            x = self.act(self.branch(x))\n",
    "            x = self.output(x)\n",
    "\n",
    "            # Concatenate alphas to output\n",
    "            x = torch.concat([x, alpha], dim=-1)\n",
    "        else:\n",
    "            # Simple output\n",
    "            x = self.output(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pdf(\n",
    "    bins: torch.Tensor,\n",
    "    weights: torch.Tensor,\n",
    "    n_samples: int,\n",
    "    perturb: bool = False\n",
    ") -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    Apply inverse transform sampling to a weighted set of points.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize weights to get PDF.\n",
    "    pdf = (weights + 1e-5) / torch.sum(weights + 1e-5, -1, keepdims=True) # [n_rays, weights.shape[-1]]\n",
    "\n",
    "    # Convert PDF to CDF.\n",
    "    cdf = torch.cumsum(pdf, dim=-1) # [n_rays, weights.shape[-1]]\n",
    "    cdf = torch.concat([torch.zeros_like(cdf[..., :1]), cdf], dim=-1) # [n_rays, weights.shape[-1] + 1]\n",
    "\n",
    "    # Take sample positions to grab from CDF. Linear when perturb == 0.\n",
    "    if not perturb:\n",
    "        u = torch.linspace(0., 1., n_samples, device=cdf.device)\n",
    "        u = u.expand(list(cdf.shape[:-1]) + [n_samples]) # [n_rays, n_samples]\n",
    "    else:\n",
    "        u = torch.rand(list(cdf.shape[:-1]) + [n_samples], device=cdf.device) # [n_rays, n_samples]\n",
    "\n",
    "    # Find indices along CDF where values in u would be placed.\n",
    "    u = u.contiguous() # Returns contiguous tensor with same values.\n",
    "    inds = torch.searchsorted(cdf, u, right=True) # [n_rays, n_samples]\n",
    "\n",
    "    # Clamp indices that are out of bounds.\n",
    "    below = torch.clamp(inds - 1, min=0)\n",
    "    above = torch.clamp(inds, max=cdf.shape[-1] - 1)\n",
    "    inds_g = torch.stack([below, above], dim=-1) # [n_rays, n_samples, 2]\n",
    "\n",
    "    # Sample from cdf and the corresponding bin centers.\n",
    "    matched_shape = list(inds_g.shape[:-1]) + [cdf.shape[-1]]\n",
    "    cdf_g = torch.gather(cdf.unsqueeze(-2).expand(matched_shape), dim=-1,\n",
    "                                             index=inds_g)\n",
    "    bins_g = torch.gather(bins.unsqueeze(-2).expand(matched_shape), dim=-1,\n",
    "                                                index=inds_g)\n",
    "\n",
    "    # Convert samples to ray length.\n",
    "    denom = (cdf_g[..., 1] - cdf_g[..., 0])\n",
    "    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n",
    "    t = (u - cdf_g[..., 0]) / denom\n",
    "    samples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n",
    "\n",
    "    return samples # [n_rays, n_samples]\n",
    "\n",
    "def sample_hierarchical(\n",
    "    rays_o: torch.Tensor,\n",
    "    rays_d: torch.Tensor,\n",
    "    z_vals: torch.Tensor,\n",
    "    weights: torch.Tensor,\n",
    "    n_samples: int,\n",
    "    perturb: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    r\"\"\"\n",
    "    Apply hierarchical sampling to the rays.\n",
    "    \"\"\"\n",
    "\n",
    "    # Draw samples from PDF using z_vals as bins and weights as probabilities.\n",
    "    z_vals_mid = .5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
    "    new_z_samples = sample_pdf(z_vals_mid, weights[..., 1:-1], n_samples,\n",
    "                                                    perturb=perturb)\n",
    "    new_z_samples = new_z_samples.detach()\n",
    "\n",
    "    # Resample points from ray based on PDF.\n",
    "    z_vals_combined, _ = torch.sort(torch.cat([z_vals, new_z_samples], dim=-1), dim=-1)\n",
    "    pts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals_combined[..., :, None]  # [N_rays, N_samples + n_samples, 3]\n",
    "    return pts, z_vals_combined, new_z_samples\n",
    "\n",
    "def cumprod_exclusive(\n",
    "    tensor: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    (Courtesy of https://github.com/krrish94/nerf-pytorch)\n",
    "\n",
    "    Mimick functionality of tf.math.cumprod(..., exclusive=True), as it isn't available in PyTorch.\n",
    "\n",
    "    Args:\n",
    "    tensor (torch.Tensor): Tensor whose cumprod (cumulative product, see `torch.cumprod`) along dim=-1\n",
    "        is to be computed.\n",
    "    Returns:\n",
    "    cumprod (torch.Tensor): cumprod of Tensor along dim=-1, mimiciking the functionality of\n",
    "        tf.math.cumprod(..., exclusive=True) (see `tf.math.cumprod` for details).\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute regular cumprod first (this is equivalent to `tf.math.cumprod(..., exclusive=False)`).\n",
    "    cumprod = torch.cumprod(tensor, -1)\n",
    "    # \"Roll\" the elements along dimension 'dim' by 1 element.\n",
    "    cumprod = torch.roll(cumprod, 1, -1)\n",
    "    # Replace the first element by \"1\" as this is what tf.cumprod(..., exclusive=True) does.\n",
    "    cumprod[..., 0] = 1.\n",
    "\n",
    "    return cumprod\n",
    "\n",
    "\n",
    "def get_chunks(\n",
    "    inputs: torch.Tensor,\n",
    "    chunksize: int = 2**15\n",
    ") -> List[torch.Tensor]:\n",
    "    r\"\"\"\n",
    "    Divide an input into chunks.\n",
    "    \"\"\"\n",
    "    return [inputs[i:i + chunksize] for i in range(0, inputs.shape[0], chunksize)]\n",
    "\n",
    "def prepare_chunks(\n",
    "    points: torch.Tensor,\n",
    "    encoding_function: Callable[[torch.Tensor], torch.Tensor],\n",
    "    chunksize: int = 2**15\n",
    ") -> List[torch.Tensor]:\n",
    "    r\"\"\"\n",
    "    Encode and chunkify points to prepare for NeRF model.\n",
    "    \"\"\"\n",
    "    points = points.reshape((-1, 4))\n",
    "    datalog.debug(f'Points Before encoding MIN/MAX')\n",
    "    datalog.debug(str(points.min(0)[0]))\n",
    "    datalog.debug(str(points.max(0)[0]))\n",
    "    datalog.debug(f'--------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    datalog.debug(f'Points After Norm MIN/MAX')\n",
    "    _min = torch.tensor([-4, -4, -3,  0.0000]).to(device)\n",
    "    _max = torch.tensor([4, 4, 3, 1.0000]).to(device)\n",
    "    points = (points - _min) / (_max - _min)\n",
    "    datalog.debug(str(points.min(0)[0]))\n",
    "    datalog.debug(str(points.max(0)[0]))\n",
    "    datalog.debug(f'--------------------------------------------------------------------')\n",
    "\n",
    "    points = encoding_function(points)\n",
    "    datalog.debug(f'Points after encoding MIN/MAX: {points.min()} / {points.max()}')\n",
    "    points = get_chunks(points, chunksize=chunksize)\n",
    "    return points\n",
    "\n",
    "def prepare_viewdirs_chunks(\n",
    "    points: torch.Tensor,\n",
    "    rays_d: torch.Tensor,\n",
    "    encoding_function: Callable[[torch.Tensor], torch.Tensor],\n",
    "    chunksize: int = 2**15\n",
    ") -> List[torch.Tensor]:\n",
    "    r\"\"\"\n",
    "    Encode and chunkify viewdirs to prepare for NeRF model.\n",
    "    \"\"\"\n",
    "    # Prepare the viewdirs\n",
    "    viewdirs = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
    "    viewdirs = viewdirs[:, None, ...].expand(points.shape).reshape((-1, 3))\n",
    "    viewdirs = encoding_function(viewdirs)\n",
    "    viewdirs = get_chunks(viewdirs, chunksize=chunksize)\n",
    "    return viewdirs\n",
    "\n",
    "# def prepare_time(\n",
    "#     timesteps, \n",
    "#     time_encoding_fn: Callable[[torch.Tensor], torch.Tensor], \n",
    "#     chunksize: int = 2**15\n",
    "# ) -> List[torch.Tensor]:\n",
    "#     r\"\"\"\n",
    "#     Encode and chunkify timepoints to prepare for NeRF model.\n",
    "#     \"\"\"\n",
    "#     timesteps = time_encoding_fn(timesteps)\n",
    "#     timesteps = get_chunks(timesteps, chunksize=chunksize)\n",
    "#     return timesteps\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(points_time, encoding_fn, chunksize, query_points, rays_d, viewdirs_encoding_fn):\n",
    "    # Prepare batches.\n",
    "    batches = prepare_chunks(points_time, encoding_fn, chunksize=chunksize)\n",
    "    debug(f'Batches len: {len(batches)}, Batches[0] shape: {batches[0].shape};')\n",
    "    \n",
    "    # Prepare viewd directions\n",
    "    if viewdirs_encoding_fn is not None:\n",
    "        batches_viewdirs = prepare_viewdirs_chunks(\n",
    "            query_points, \n",
    "            rays_d,\n",
    "            viewdirs_encoding_fn,\n",
    "            chunksize=chunksize)\n",
    "    else:\n",
    "        batches_viewdirs = [None] * len(batches)\n",
    "    return batches, batches_viewdirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(query_points, timesteps):\n",
    "    # Time\n",
    "    datalog.debug(f'Timesteps inputs: {timesteps}')\n",
    "    batch_size, rays_size, coords_size = query_points.shape\n",
    "    if timesteps.shape[0] == 1: # only one image and one timestep\n",
    "        # Expanding to match other data\n",
    "        # n_points = query_points.reshape(-1,3).shape[0]\n",
    "        # expanded_timesteps = timesteps.expand(n_points).reshape(batch_size, rays_size, 1)\n",
    "        expanded_timesteps = timesteps.expand((batch_size, rays_size, 1))\n",
    "        datalog.debug(f'Timesteps expanded sample:\\n{torch.unique(expanded_timesteps)}')\n",
    "    else: # For each ray repeating timestep along sampling dimention\n",
    "        expanded_timesteps = timesteps.repeat(1, rays_size).reshape(batch_size, rays_size, 1)\n",
    "\n",
    "    # Make 4D (x,y,z,t) tensor\n",
    "    datalog.debug(f'Points shape: {query_points.shape}; Time shape: {expanded_timesteps.shape}')\n",
    "    points_time = torch.cat([query_points, expanded_timesteps], -1)\n",
    "    datalog.debug('(X,Y,Z,T) shape: '+str(points_time.shape))\n",
    "    return points_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nerf_forward(\n",
    "    rays_o: torch.Tensor,\n",
    "    rays_d: torch.Tensor,\n",
    "    timesteps: torch.Tensor,\n",
    "    near: float,\n",
    "    far: float,\n",
    "    encoding_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "    coarse_model: nn.Module,\n",
    "    kwargs_sample_stratified: dict = None,\n",
    "    n_samples_hierarchical: int = 0,\n",
    "    kwargs_sample_hierarchical: dict = None,\n",
    "    fine_model = None,\n",
    "    viewdirs_encoding_fn: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n",
    "    # time_encoding_fn: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n",
    "    chunksize: int = 2**15\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]:\n",
    "    r\"\"\"\n",
    "    Compute forward pass through model(s).\n",
    "    \"\"\"\n",
    "    # Set no kwargs if none are given.\n",
    "    if kwargs_sample_stratified is None:\n",
    "        kwargs_sample_stratified = {}\n",
    "    if kwargs_sample_hierarchical is None:\n",
    "        kwargs_sample_hierarchical = {}\n",
    "\n",
    "    # Get all points data \n",
    "    # Sample query points along each ray.\n",
    "    query_points, z_vals = sample_stratified(\n",
    "            rays_o, rays_d, near, far, **kwargs_sample_stratified)\n",
    "    points_time = get_inputs(query_points, timesteps)\n",
    "    \n",
    "    datalog.debug(f'Points-Time MIN/MAX IN: {points_time.min()}, {points_time.max()}')\n",
    "\n",
    "    # Bachefy all the data\n",
    "    batches, batches_viewdirs = get_batches(\n",
    "        points_time, encoding_fn, chunksize, query_points, rays_d, viewdirs_encoding_fn)\n",
    "\n",
    "\n",
    "    ###\n",
    "    # Coarse model pass.\n",
    "    # Split the encoded points into \"chunks\", run the model on all chunks, and\n",
    "    # concatenate the results (to avoid out-of-memory issues).\n",
    "    predictions = []\n",
    "    for batch, batch_viewdirs in zip(batches, batches_viewdirs):\n",
    "        one_batch_prediction = coarse_model(batch, viewdirs=batch_viewdirs)\n",
    "        predictions.append(one_batch_prediction)\n",
    "    raw = torch.cat(predictions, dim=0)\n",
    "    raw = raw.reshape(list(query_points.shape[:2]) + [raw.shape[-1]])\n",
    "\n",
    "\n",
    "    # Perform differentiable volume rendering to re-synthesize the RGB image.\n",
    "    rgb_map, depth_map, acc_map, weights = raw2outputs(raw, z_vals, rays_d)\n",
    "    # rgb_map, depth_map, acc_map, weights = render_volume_density(raw, rays_o, z_vals)\n",
    "\n",
    "    outputs = {\n",
    "            'z_vals_stratified': z_vals\n",
    "    }\n",
    "\n",
    "    ###\n",
    "    # Fine model pass.\n",
    "    if n_samples_hierarchical > 0:\n",
    "        # Save previous outputs to return.\n",
    "        rgb_map_0, depth_map_0, acc_map_0 = rgb_map, depth_map, acc_map\n",
    "\n",
    "        # Apply hierarchical sampling for fine query points.\n",
    "        query_points, z_vals_combined, z_hierarch = sample_hierarchical(\n",
    "            rays_o, rays_d, z_vals, weights, n_samples_hierarchical,**kwargs_sample_hierarchical)\n",
    "        points_time = get_inputs(query_points, timesteps)\n",
    "\n",
    "        # Bachefy all the data\n",
    "        batches, batches_viewdirs = get_batches(\n",
    "            points_time, encoding_fn, chunksize, query_points, rays_d, viewdirs_encoding_fn)\n",
    "\n",
    "        # Forward pass new samples through fine model.\n",
    "        fine_model = fine_model if fine_model is not None else coarse_model\n",
    "        predictions = []\n",
    "        for batch, batch_viewdirs in zip(batches, batches_viewdirs):\n",
    "            one_batch_predictions = fine_model(batch, viewdirs=batch_viewdirs)\n",
    "            predictions.append(one_batch_predictions)\n",
    "        raw = torch.cat(predictions, dim=0)\n",
    "        raw = raw.reshape(list(query_points.shape[:2]) + [raw.shape[-1]])\n",
    "\n",
    "        # Perform differentiable volume rendering to re-synthesize the RGB image.\n",
    "        rgb_map, depth_map, acc_map, weights = raw2outputs(raw, z_vals_combined, rays_d)\n",
    "\n",
    "        # Store outputs.\n",
    "        outputs['z_vals_hierarchical'] = z_hierarch\n",
    "        outputs['rgb_map_0'] = rgb_map_0\n",
    "        outputs['depth_map_0'] = depth_map_0\n",
    "        outputs['acc_map_0'] = acc_map_0\n",
    "\n",
    "    # Store outputs.\n",
    "    outputs['rgb_map'] = rgb_map\n",
    "    outputs['depth_map'] = depth_map\n",
    "    outputs['acc_map'] = acc_map\n",
    "    outputs['weights'] = weights\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Select training parameters\n",
    "###\n",
    "# Encoders\n",
    "from enum import Enum\n",
    "Encoders = Enum('Encoder', [\"NONE\",\"FREQ\", \"HASH\",\"FREQHASH\"]) \n",
    "encoder_used = Encoders.FREQHASH\n",
    "\n",
    "## Freq encoder\n",
    "d_input = 3           # Number of input dimensions\n",
    "n_freqs = 10          # Number of encoding functions for samples\n",
    "log_space = True      # If set, frequencies scale in log space\n",
    "use_viewdirs = True   # If set, use view direction as input\n",
    "n_freqs_views = 4     # Number of encoding functions for views\n",
    "n_freqs_time = 4     # Number of encoding functions for time\n",
    "\n",
    "## Hash encoder\n",
    "hash_feature_dim = 3\n",
    "hash_grid_dim = 3\n",
    "hash_num_lvl = 17\n",
    "hash_max_res = 2**14\n",
    "hash_min_res = 16\n",
    "hash_hashtable_power = 19\n",
    "\n",
    "# Stratified sampling\n",
    "n_samples = 8#32         # Number of spatial samples per ray\n",
    "perturb = True         # If set, applies noise to sample positions\n",
    "inverse_depth = False  # If set, samples points linearly in inverse depth\n",
    "\n",
    "# Model NeRF parameters\n",
    "d_filter = 128#256            # Dimensions of linear layer filters\n",
    "n_layers = 2#6            # Number of layers in network bottleneck\n",
    "skip = []#[4]             # Layers at which to apply input residual\n",
    "skip_fine = []\n",
    "use_fine_model = True     # If set, creates a fine model\n",
    "d_filter_fine = 128#256       # Dimensions of linear layer filters of fine network\n",
    "n_layers_fine = 3#6       # Number of layers in fine network bottleneck\n",
    "\n",
    "# D-NeRF parameters\n",
    "zero_canonical = True     # Use zero time step as canonical space?  \n",
    "d_filter_time = 128#256        # Dimensions of linear layer filters\n",
    "n_layers_time = 3#6         # Number of layers in network bottleneck\n",
    "skip_time = []#[4]            # Layers at which to apply input residual\n",
    "\n",
    "# Hierarchical sampling\n",
    "n_samples_hierarchical = 16#64   # Number of samples per ray\n",
    "perturb_hierarchical = False  # If set, applies noise to sample positions\n",
    "\n",
    "near, far = 2., 6.\n",
    "\n",
    "# Optimizer\n",
    "lr = 1e-4  # Learning rate\n",
    "lr_hash_enc = 5e-4\n",
    "scheduler_start_end_factors = [1.0, 0.9] # Linear decay of learning rate\n",
    "\n",
    "# Training\n",
    "n_epochs = 2000\n",
    "batch_size = 2**14          # Number of rays per gradient step (power of 2)\n",
    "one_image_per_step = True   # One image per gradient step (disables batching)\n",
    "chunksize = 2**14           # Modify as needed to fit in GPU memory\n",
    "center_crop = True          # Crop the center of image (one_image_per_)\n",
    "center_crop_iters = 0      # Stop cropping center after this many epochs\n",
    "display_rate = 50          # Display test output every X epochs\n",
    "shuffle_data = True\t\t    # Shuffle rays on every iteration. Mostly for debug. \n",
    "\n",
    "# Early Stopping\n",
    "warmup_iters = 100          # Number of iterations during warmup phase\n",
    "warmup_min_fitness = 10.0   # Min val PSNR to continue training at warmup_iters\n",
    "n_restarts = 10             # Number of times to restart if training stalls\n",
    "\n",
    "# We bundle the kwargs for various functions to pass all at once.\n",
    "kwargs_sample_stratified = {\n",
    "\t'n_samples': n_samples,\n",
    "\t'perturb': perturb,\n",
    "\t'inverse_depth': inverse_depth\n",
    "}\n",
    "kwargs_sample_hierarchical = {\n",
    "\t'perturb': perturb\n",
    "}\n",
    "\n",
    "test_save_dir = f'./results/DNeRF_{encoder_used.name}/'\n",
    "os.makedirs(test_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(i, rgb_predicted, testimg, iternums, outputs,train_psnrs,val_psnrs, is_save=False):\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(24,4), gridspec_kw={'width_ratios': [1, 1, 1, 3]})\n",
    "    # Plot example outputs\n",
    "    \n",
    "    ax[0].imshow(rgb_predicted.reshape([height, width, 3]).detach().cpu().numpy())\n",
    "    ax[0].set_title(f'Iteration: {i}')\n",
    "    ax[1].imshow(testimg.detach().cpu().numpy())\n",
    "    ax[1].set_title(f'Target')\n",
    "    ax[2].plot(range(0, i + 1), train_psnrs, 'r')\n",
    "    ax[2].plot(iternums, val_psnrs, 'b')\n",
    "    ax[2].set_title('PSNR (train=red, val=blue')\n",
    "    z_vals_strat = outputs['z_vals_stratified'].view((-1, n_samples))\n",
    "    z_sample_strat = z_vals_strat[z_vals_strat.shape[0] // 2].detach().cpu().numpy()\n",
    "    if 'z_vals_hierarchical' in outputs:\n",
    "        z_vals_hierarch = outputs['z_vals_hierarchical'].view((-1, n_samples_hierarchical))\n",
    "        z_sample_hierarch = z_vals_hierarch[z_vals_hierarch.shape[0] // 2].detach().cpu().numpy()\n",
    "    else:\n",
    "        z_sample_hierarch = None\n",
    "    _ = plot_samples(z_sample_strat, z_sample_hierarch, ax=ax[3])\n",
    "    ax[3].margins(0)\n",
    "    plt.show()\n",
    "\n",
    "    if is_save:\n",
    "        rgb_predicted = rgb_predicted.reshape([height, width, 3]).detach().cpu().numpy()\n",
    "        rgb_predicted = (rgb_predicted*255).astype(np.uint8)\n",
    "        Image.fromarray(rgb_predicted).save('predicted_image.png')\n",
    "\n",
    "        ti = testimg.detach().cpu().numpy()\n",
    "        ti = (ti*255).astype(np.uint8)\n",
    "        Image.fromarray(ti).save('GT_image.png')\n",
    "\n",
    "def plot_samples(\n",
    "    z_vals: torch.Tensor,\n",
    "    z_hierarch: Optional[torch.Tensor] = None,\n",
    "    ax: Optional[np.ndarray] = None):\n",
    "    r\"\"\"\n",
    "    Plot stratified and (optional) hierarchical samples.\n",
    "    \"\"\"\n",
    "    y_vals = 1 + np.zeros_like(z_vals)\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.subplot()\n",
    "    ax.plot(z_vals, y_vals, 'b-o')\n",
    "    if z_hierarch is not None:\n",
    "        y_hierarch = np.zeros_like(z_hierarch)\n",
    "        ax.plot(z_hierarch, y_hierarch, 'r-o')\n",
    "    ax.set_ylim([-1, 2])\n",
    "    ax.set_title('Stratified  Samples (blue) and Hierarchical Samples (red)')\n",
    "    ax.axes.yaxis.set_visible(False)\n",
    "    ax.grid(True)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def eval_one_time(\n",
    "        i, \n",
    "        iternums, \n",
    "        train_psnrs, \n",
    "        val_psnrs, \n",
    "        model, \n",
    "        encode, \n",
    "        fine_model, \n",
    "        encode_viewdirs, \n",
    "        testimg, \n",
    "        testpose, \n",
    "        testtime,\n",
    "        is_save=False):\n",
    "    model.eval()\n",
    "\n",
    "    testimg = testimg.to(device) \n",
    "    testpose = testpose.to(device)\n",
    "    testtime = testtime.to(device)\n",
    "\n",
    "    height, width = testimg.shape[:2]\n",
    "    rays_o, rays_d = get_rays(height, width, focal, testpose)\n",
    "    rays_o = rays_o.reshape([-1, 3]).to(device)\n",
    "    rays_d = rays_d.reshape([-1, 3]).to(device)\n",
    "    outputs = nerf_forward(rays_o, rays_d, testtime,\n",
    "\t\t\t\t\t\t\tnear, far, encode, model,\n",
    "\t\t\t\t\t\t\tkwargs_sample_stratified=kwargs_sample_stratified,\n",
    "\t\t\t\t\t\t\tn_samples_hierarchical=n_samples_hierarchical,\n",
    "\t\t\t\t\t\t\tkwargs_sample_hierarchical=kwargs_sample_hierarchical,\n",
    "\t\t\t\t\t\t\tfine_model=fine_model,\n",
    "\t\t\t\t\t\t\tviewdirs_encoding_fn=encode_viewdirs,\n",
    "\t\t\t\t\t\t\t# time_encoding_fn=encode_time,\n",
    "\t\t\t\t\t\t\tchunksize=chunksize)\n",
    "\n",
    "    rgb_predicted = outputs['rgb_map']\n",
    "    testimg_flat = testimg.reshape(-1, 3)\n",
    "    loss = torch.nn.functional.mse_loss(rgb_predicted, testimg_flat.to(device))\n",
    "    val_psnr = -10. * torch.log10(loss)\n",
    "    val_psnrs.append(val_psnr.item())\n",
    "    iternums.append(i)\n",
    "\n",
    "    print(\"Val Loss: \", loss.item(), \" Val PSRN: \", val_psnr.item())\n",
    "    # Plot example outputs outside\n",
    "    render(i, rgb_predicted, testimg, iternums, outputs,train_psnrs,val_psnrs, is_save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_eval(\n",
    "        i, \n",
    "        model, \n",
    "        encode, \n",
    "        fine_model, \n",
    "        encode_viewdirs, \n",
    "        testimgs, \n",
    "        testposes, \n",
    "        testtimes,\n",
    "        train_psnrs, \n",
    "        val_psnrs, ):\n",
    "    clear_output(wait=True)\n",
    "    model.eval()\n",
    "\n",
    "    _number_of_extra_images = 6\n",
    "    _ROW = 12\n",
    "    _N = N_samples + _number_of_extra_images\n",
    "    _COL = (_N // _ROW) * 2\n",
    "    if _N % _ROW != 0:\n",
    "        _COL += 2\n",
    "\n",
    "    fig,ax = plt.subplots(_COL, _ROW, figsize=(_ROW*2,_COL*2))\n",
    "\n",
    "    n = 0\n",
    "    m = 0\n",
    "    for ii in range(_N):\n",
    "\n",
    "        testimg = testimgs[ii].to(device) \n",
    "        testpose = testposes[ii].to(device)\n",
    "        testtime = testtimes[ii].to(device).unsqueeze(0)\n",
    "\n",
    "        # Predict\n",
    "        height, width = testimg.shape[:2]\n",
    "        rays_o, rays_d = get_rays(height, width, focal, testpose)\n",
    "        rays_o = rays_o.reshape([-1, 3]).to(device)\n",
    "        rays_d = rays_d.reshape([-1, 3]).to(device)\n",
    "        outputs = nerf_forward(rays_o, rays_d, testtime,\n",
    "                                near, far, encode, model,\n",
    "                                kwargs_sample_stratified=kwargs_sample_stratified,\n",
    "                                n_samples_hierarchical=n_samples_hierarchical,\n",
    "                                kwargs_sample_hierarchical=kwargs_sample_hierarchical,\n",
    "                                fine_model=fine_model,\n",
    "                                viewdirs_encoding_fn=encode_viewdirs,\n",
    "                                chunksize=chunksize)\n",
    "        rgb_predicted = outputs['rgb_map']\n",
    "        testimg_flat = testimg.reshape(-1, 3)\n",
    "        loss = torch.nn.functional.mse_loss(rgb_predicted, testimg_flat.to(device))\n",
    "        val_psnr = -10. * torch.log10(loss)\n",
    "        val_psnrs.append(val_psnr.item())\n",
    "\n",
    "        fig.suptitle(f'Iteration: {i}')\n",
    "        ax[n, m].imshow(testimg.detach().cpu().numpy())\n",
    "        ax[n+1, m].imshow(rgb_predicted.reshape([height, width, 3]).detach().cpu().numpy())\n",
    "        if ii < N_samples:\n",
    "            ax[n, m].set_title(f'Train t={testtime.item():.2f}')\n",
    "        else:\n",
    "            ax[n, m].set_title(f'Eval t={testtime.item():.2f}')\n",
    "\n",
    "        ax[n, m].axis(\"off\")\n",
    "        ax[n+1, m].axis(\"off\")\n",
    "\n",
    "        m += 1\n",
    "        if m >= _ROW:\n",
    "            m  = 0\n",
    "            n += 2\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = 'results/Hash_DNeRF_N_samples'\n",
    "os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "\tr\"\"\"\n",
    "\tSine-cosine positional encoder for input points.\n",
    "\t\"\"\"\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\td_input: int,\n",
    "\t\tn_freqs: int,\n",
    "\t\tlog_space: bool = False\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.d_input = d_input\n",
    "\t\tself.n_freqs = n_freqs\n",
    "\t\tself.log_space = log_space\n",
    "\t\tself.d_output = d_input * (1 + 2 * self.n_freqs)\n",
    "\t\tself.embed_fns = [lambda x: x]\n",
    "\n",
    "\t\t# Define frequencies in either linear or log scale\n",
    "\t\tif self.log_space:\n",
    "\t\t\tfreq_bands = 2.**torch.linspace(0., self.n_freqs - 1, self.n_freqs)\n",
    "\t\telse:\n",
    "\t\t\tfreq_bands = torch.linspace(2.**0., 2.**(self.n_freqs - 1), self.n_freqs)\n",
    "\n",
    "\t\t# Alternate sin and cos\n",
    "\t\tfor freq in freq_bands:\n",
    "\t\t\tself.embed_fns.append(lambda x, freq=freq: torch.sin(x * freq))\n",
    "\t\t\tself.embed_fns.append(lambda x, freq=freq: torch.cos(x * freq))\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tx\n",
    "\t) -> torch.Tensor:\n",
    "\t\tr\"\"\"\n",
    "\t\tApply positional encoding to input.\n",
    "\t\t\"\"\"\n",
    "\t\treturn torch.concat([fn(x) for fn in self.embed_fns], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoders\n",
    "encoder = Grid(\n",
    "    feature_dim=3,\n",
    "    grid_dim=4,\n",
    "    num_lvl=3,\n",
    "    max_res=1024, \n",
    "    min_res=16,\n",
    "    hashtable_power=19,\n",
    "    device=device,\n",
    ").to(device)\n",
    "# encoder = PositionalEncoder(4,10,True)\n",
    "encode = lambda x: encoder(x)\n",
    "\n",
    "# View direction encoders\n",
    "if use_viewdirs:\n",
    "    encoder_viewdirs = PositionalEncoder(\n",
    "        d_input, n_freqs_views, log_space=log_space)\n",
    "    encode_viewdirs = lambda x: encoder_viewdirs(x)\n",
    "    d_viewdirs = encoder_viewdirs.d_output\n",
    "else:\n",
    "    encode_viewdirs = None\n",
    "    d_viewdirs = None\n",
    "\n",
    "# Time encoder\n",
    "# encoder_time = PositionalEncoder(1, n_freqs_time, log_space=log_space)\n",
    "# encode_time = lambda x: encoder_time(x)\n",
    "# d_timesteps = encoder_time.d_output\n",
    "\n",
    "print(f'Encode input points into {encoder.d_output} dimentions!')\n",
    "print(f'Encode input views into {d_viewdirs} dimentions!', )\n",
    "# print(f'Encode input time steps into {d_timesteps} dimentions!', )\n",
    "\n",
    "'''\n",
    "SEEMS LIKE SIZE OF THE NETWORK AFFECTS REPRESENTATION ABILITY\n",
    "TOO SMALL AND MODEL COLLAPSE\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# Models\n",
    "model = NeRF(\n",
    "    d_input = encoder.d_output,\n",
    "    n_layers = 3,\n",
    "    d_filter = 256,#128,\n",
    "    skip = [],\n",
    "    d_viewdirs = encoder_viewdirs.d_output\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "if use_fine_model:\n",
    "    fine_model = NeRF(\n",
    "        d_input = encoder.d_output,\n",
    "        n_layers = 6,\n",
    "        d_filter = 256,#128,\n",
    "        skip = [4],\n",
    "        d_viewdirs = encoder_viewdirs.d_output\n",
    "    )\n",
    "    fine_model.to(device)\n",
    "    \n",
    "else:\n",
    "    fine_model = None\n",
    "\n",
    "def print_params(model):\n",
    "    print(np.sum([np.cumprod([y for y in x.size()])[-1] for x in model.parameters()]))\n",
    "print('Parameters for models:')\n",
    "print_params(model)\n",
    "print_params(fine_model)\n",
    "print_params(encoder)\n",
    "\n",
    "# Optimizer\n",
    "# optimizer = torch.optim.Adam([\n",
    "#     {'params': model.parameters(), 'lr': 1e-4},\n",
    "#     {'params': fine_model.parameters(), 'lr': 1e-4},\n",
    "#     {'params': encoder.parameters(), 'lr': 5e-4},\n",
    "# ])\n",
    "\n",
    "# WORKS WELL AS A BOOTSTRAP\n",
    "optimizer = torch.optim.Rprop([\n",
    "    {'params': model.parameters(), 'lr': 1e-4},\n",
    "    {'params': fine_model.parameters(), 'lr': 1e-4},\n",
    "    {'params': encoder.parameters(), 'lr': 5e-4},\n",
    "])\n",
    "\n",
    "\n",
    "# Scheduler \n",
    "# scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "#     optimizer, \n",
    "#     start_factor=scheduler_start_end_factors[0], \n",
    "#     end_factor=scheduler_start_end_factors[1], \n",
    "#     total_iters=n_epochs)\n",
    "    \n",
    "# Early Stopping\n",
    "# warmup_stopper = EarlyStopping(patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters(), 'lr': 1e-4},\n",
    "    {'params': fine_model.parameters(), 'lr': 1e-4},\n",
    "    {'params': encoder.parameters(), 'lr': 5e-4},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_psnrs = []\n",
    "val_psnrs = []\n",
    "iternums = []\n",
    "train_psnrs = []\n",
    "val_psnrs = []\n",
    "iternums = []\n",
    "\n",
    "lambda_reg = 1e-4\n",
    "parameters = list(model.parameters()) + list(fine_model.parameters()) + list(encoder.parameters())\n",
    "def regularization_loss(parameters):\n",
    "    reg_loss = 0\n",
    "    for param in parameters:\n",
    "        reg_loss += torch.sum(param ** 2)\n",
    "    return reg_loss\n",
    "\n",
    "def check_gradients(model_name='', model=None):\n",
    "    modellog.debug('*'*16)\n",
    "    modellog.debug(model_name)\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.grad is not None:\n",
    "            modellog.debug(f'{name} - Gradient mean: {param.grad.mean().item()}')\n",
    "    modellog.debug('*'*16)\n",
    "\n",
    "_S=0\n",
    "pbar = tqdm(range(n_epochs), desc=\"Training Epochs\")\n",
    "# pbar = tqdm(range(1), desc=\"Training Epochs\")\n",
    "for i in pbar:\n",
    "    debug('###')\n",
    "    debug('### Train one time')\n",
    "    debug('###')\n",
    "    model.train()\n",
    "    # One image per step\n",
    "\n",
    "    # Randomly pick an image as the target.\n",
    "    # Target Image\n",
    "    target_img_idx = range(N_samples)[i%N_samples]\n",
    "    # target_img_idx = np.random.randint(images.shape[0])\n",
    "    target_img = images[target_img_idx].to(device)\n",
    "    # if center_crop and i < center_crop_iters:\n",
    "    #     target_img = crop_center(target_img)\n",
    "    height, width = target_img.shape[:2]\n",
    "    target_img = target_img.reshape([-1, 3])\n",
    "    # Pose\n",
    "    target_pose = poses[target_img_idx].to(device)\n",
    "    # Rays\n",
    "    rays_o, rays_d = get_rays(height, width, focal, target_pose)\n",
    "    rays_o = rays_o.reshape([-1, 3]).to(device)\n",
    "    rays_d = rays_d.reshape([-1, 3]).to(device)\n",
    "    # Handle Time\n",
    "    timesteps = times[target_img_idx].reshape(1).to(device)\n",
    "    datalog.debug(f'TIMESTEPS: {timesteps}')\n",
    "\n",
    "    # Run one iteration of TinyNeRF and get the rendered RGB image.\n",
    "    outputs = nerf_forward(\n",
    "        rays_o, rays_d, timesteps,\n",
    "        near, far, encode, model,\n",
    "        kwargs_sample_stratified=kwargs_sample_stratified,\n",
    "        n_samples_hierarchical=n_samples_hierarchical,\n",
    "        kwargs_sample_hierarchical=kwargs_sample_hierarchical,\n",
    "        fine_model=fine_model,\n",
    "        viewdirs_encoding_fn=encode_viewdirs,\n",
    "        # time_encoding_fn=encode_time,\n",
    "        chunksize=chunksize)\n",
    "\n",
    "    # Check for any numerical issues.\n",
    "    for k, v in outputs.items():\n",
    "        if torch.isnan(v).any():\n",
    "            print(f\"! [Numerical Alert] {k} contains NaN.\")\n",
    "        if torch.isinf(v).any():\n",
    "            print(f\"! [Numerical Alert] {k} contains Inf.\")\n",
    "\n",
    "    # Backprop!\n",
    "    rgb_predicted = outputs['rgb_map']\n",
    "\n",
    "    loss = torch.nn.functional.mse_loss(\n",
    "        rgb_predicted.to(device), target_img.to(device)) #+ lambda_reg * regularization_loss(parameters)\n",
    "    loss.backward()\n",
    "\n",
    "    # torch.nn.utils.clip_grad_norm_(parameters, max_norm=1.0)\n",
    "\n",
    "    check_gradients(model_name='Model', model=model)\n",
    "    check_gradients(model_name='Fine Model', model=fine_model)\n",
    "    check_gradients(model_name='Encoder', model=encoder)\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    # scheduler.step()\n",
    "\n",
    "    psnr = -10. * torch.log10(loss)\n",
    "    psnr = psnr.item()\n",
    "    \n",
    "    ###\n",
    "    # Evaluation and logging\n",
    "\n",
    "    epoch = i\n",
    "    pbar.set_description(f\"Epoch {epoch + 1}/{n_epochs}, PSNR: {psnr:.2f}, Loss: {loss.item():.5f}\")\n",
    "\n",
    "    train_psnrs.append(psnr)\n",
    "\n",
    "    # Evaluate testimg at given display rate.\n",
    "    if i % display_rate == 0:\n",
    "        my_eval(i, model, encode, fine_model, encode_viewdirs, testimgs, testposes, testtimes,train_psnrs, val_psnrs)\n",
    "\n",
    "\n",
    "        # for jj in range(N_samples):\n",
    "        #     eval_one_time(i, iternums, train_psnrs, val_psnrs, model, encode, fine_model, encode_viewdirs,\n",
    "        #                     testimg=testimgs[jj], testpose=testposes[jj], testtime=testtimes[jj].unsqueeze(0))\n",
    "    \n",
    "        # eval_one_time(i, iternums, train_psnrs, val_psnrs, model, encode, fine_model, encode_viewdirs,\n",
    "        #                 testimgs=[testimg2,testimg], testposes=[testpose2,testpose], testtimes=[testtime2,testtimegit ])\n",
    "        \n",
    "        # save_training_progress(\n",
    "        #     model,fine_model,encode,encode_viewdirs, encode_time, chunksize, \n",
    "        #     near, far, height, width, focal, epoch//display_rate, device\n",
    "        # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug('END'+'##'*64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
