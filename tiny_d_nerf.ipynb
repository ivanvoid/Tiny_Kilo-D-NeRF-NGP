{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "This notebook walks the reader through a full implementation of the original Neural Radiance Field architecture, first introduced by Mildenhall et al. in \"[NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis.](https://www.matthewtancik.com/nerf)\" \n",
    "\n",
    "Much of the code comes from or is inspired by the original implementation by GitHub user [bmild](https://github.com/bmild/nerf) as well as PyTorch implementations from GitHub users [yenchenlin](https://github.com/bmild/nerf) and [krrish94](https://github.com/krrish94/nerf-pytorch/). The code has been modified for clarity and consistency.\n",
    "\n",
    "Based on:  \n",
    "[This colab notebook](https://colab.research.google.com/drive/1TppdSsLz8uKoNwqJqDGg8se8BHQcvg_K?usp=sharing#scrollTo=jVAGjdUthecA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from typing import Optional, Tuple, List, Union, Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from tqdm import trange\n",
    "import random\n",
    "\n",
    "# For repeatability\n",
    "seed = 2024\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Imputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "First we load the data which we will train our NeRF model on. This is the Lego bulldozer commonly seen in the NeRF demonstrations and serves as a sort of \"Hello World\" for training NeRFs. Covering other datasets is outside the scope of this notebook, but feel free to try others included in the original [NeRF source code](https://github.com/bmild/nerf) or your own datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./data/tiny_dnerf_data.npz'):\n",
    "\t!wget https://github.com/ivanvoid/Tiny_Kilo-D-NeRF-NGP/blob/main/data/tiny_dnerf_data.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset consists of 106 images taken of the synthetic Lego bulldozer along with poses and a common focal length value. Like the original, we reserve the first 100 images for training and a single test image for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./data/tiny_dnerf_data.npz')\n",
    "images = data['images'][:,:,:,:3]\n",
    "poses = data['poses']\n",
    "focal = data['focal']\n",
    "times = data['times']\n",
    "\n",
    "print(f'Images shape: {images.shape}')\n",
    "print(f'Poses shape: {poses.shape}')\n",
    "print(f'Focal length: {focal}')\n",
    "print(f'Times shape: {times.shape}')\n",
    "print('-'*32)\n",
    "\n",
    "height, width = images.shape[1:3]\n",
    "near, far = 2., 6.\n",
    "\n",
    "n_training = 79\n",
    "testimg_idx = 80\n",
    "testimg, testpose, testtime = images[testimg_idx], poses[testimg_idx], times[testimg_idx]\n",
    "\n",
    "plt.imshow(testimg)\n",
    "print('Pose')\n",
    "print(testpose)\n",
    "print('Time')\n",
    "print(testtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Origins and directions\n",
    "\n",
    "Recall that NeRF processes inputs from a field of positions (x,y,z) and view directions (θ,φ). To gather these input points, we need to apply inverse rendering to the input images. More concretely, we draw projection lines through each pixel and across the 3D space, from which we can draw samples.\n",
    "\n",
    "To sample points from the 3D space beyond our image, we first start from the initial pose of every camera taken in the photo set. With some vector math, we can convert these 4x4 pose matrices into a 3D coordinate denoting the origin and a 3D vector indicating the direction. The two together describe a vector that indicates where a camera was pointing when the photo was taken.\n",
    "\n",
    "The code in the cell below illustrates this by drawing arrows that depict the origin and the direction of every frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = np.stack([np.sum([0, 0, -1] * pose[:3, :3], axis=-1) for pose in poses])\n",
    "origins = poses[:, :3, -1]\n",
    "\n",
    "ax = plt.figure(figsize=(12, 8)).add_subplot(projection='3d')\n",
    "_ = ax.quiver(\n",
    "\torigins[..., 0].flatten(),\n",
    "\torigins[..., 1].flatten(),\n",
    "\torigins[..., 2].flatten(),\n",
    "\tdirs[..., 0].flatten(),\n",
    "\tdirs[..., 1].flatten(),\n",
    "\tdirs[..., 2].flatten(), length=0.5, normalize=True)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this camera pose, we can now find the projection lines along each pixel of our image. Each line is defined by its origin point (x,y,z) and its direction (in this case a 3D vector). While the origin is the same for every pixel, the direction is slightly different. These lines are slightly deflected off center such that none of these lines are parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pinhole camera](https://www.researchgate.net/profile/Willy-Azarcoya-Cabiedes/publication/317498100/figure/fig10/AS:610418494013440@1522546518034/Pin-hole-camera-model-terminology-The-optical-center-pinhole-is-placed-at-the-origin.png)\n",
    "\n",
    "From [Willy Azarcoya-Cabiedes via ResearchGate](https://www.researchgate.net/figure/Pin-hole-camera-model-terminology-The-optical-center-pinhole-is-placed-at-the-origin_fig10_317498100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rays(\n",
    "\theight: int,\n",
    "\twidth: int,\n",
    "\tfocal_length: float,\n",
    "\tc2w: torch.Tensor\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tFind origin and direction of rays through every pixel and camera origin.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Apply pinhole camera model to gather directions at each pixel\n",
    "\ti, j = torch.meshgrid(\n",
    "\t\t\ttorch.arange(width, dtype=torch.float32).to(c2w),\n",
    "\t\t\ttorch.arange(height, dtype=torch.float32).to(c2w),\n",
    "\t\t\tindexing='ij')\n",
    "\ti, j = i.transpose(-1, -2), j.transpose(-1, -2)\n",
    "\tdirections = torch.stack([(i - width * .5) / focal_length,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t-(j - height * .5) / focal_length,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t-torch.ones_like(i)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t ], dim=-1)\n",
    "\n",
    "\t# Apply camera pose to directions\n",
    "\trays_d = torch.sum(directions[..., None, :] * c2w[:3, :3], dim=-1)\n",
    "\n",
    "\t# Origin is same for all directions (the optical center)\n",
    "\trays_o = c2w[:3, -1].expand(rays_d.shape)\n",
    "\treturn rays_o, rays_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather as torch tensors\n",
    "# Training\n",
    "images = torch.from_numpy(data['images'][:n_training,:,:,:3])\n",
    "poses = torch.from_numpy(data['poses'][:n_training])\n",
    "focal = torch.from_numpy(data['focal'])\n",
    "times = torch.from_numpy(data['times'][:n_training])\n",
    "\n",
    "# Test\n",
    "testimg = torch.from_numpy(data['images'][testimg_idx,:,:,:3])\n",
    "testpose = torch.from_numpy(data['poses'][testimg_idx])\n",
    "testtime = torch.from_numpy(data['times'][testimg_idx:])\n",
    "\n",
    "# Grab rays from sample image\n",
    "height, width = images.shape[1:3]\n",
    "with torch.no_grad():\n",
    "\tray_origin, ray_direction = get_rays(height, width, focal, testpose)\n",
    "\n",
    "print('Ray Origin')\n",
    "print(ray_origin.shape)\n",
    "print(ray_origin[height // 2, width // 2, :])\n",
    "print('')\n",
    "\n",
    "print('Ray Direction')\n",
    "print(ray_direction.shape)\n",
    "print(ray_direction[height // 2, width // 2, :])\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Sampling\n",
    "\n",
    "Now that we have these lines, defined as origin and direction vectors, we can begin the process of sampling them. Recall that NeRF takes a coarse-to-fine sampling strategy, starting with the stratified sampling approach.\n",
    "\n",
    "The stratified sampling approach splits the ray into evenly-spaced bins and randomly samples within each bin. The `perturb` setting determines whether to sample points uniformly from each bin or to simply use the bin center as the point. In most cases, we want to keep `perturb = True` as it will encourage the network to learn over a continuously sampled space. It may be useful to disable for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_stratified(\n",
    "\trays_o: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\tnear: float,\n",
    "\tfar: float,\n",
    "\tn_samples: int,\n",
    "\tperturb: Optional[bool] = True,\n",
    "\tinverse_depth: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tSample along ray from regularly-spaced bins.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Grab samples for space integration along ray\n",
    "\tt_vals = torch.linspace(0., 1., n_samples, device=rays_o.device)\n",
    "\tif not inverse_depth:\n",
    "\t\t# Sample linearly between `near` and `far`\n",
    "\t\tz_vals = near * (1.-t_vals) + far * (t_vals)\n",
    "\telse:\n",
    "\t\t# Sample linearly in inverse depth (disparity)\n",
    "\t\tz_vals = 1./(1./near * (1.-t_vals) + 1./far * (t_vals))\n",
    "\n",
    "\t# Draw uniform samples from bins along ray\n",
    "\tif perturb:\n",
    "\t\tmids = .5 * (z_vals[1:] + z_vals[:-1])\n",
    "\t\tupper = torch.concat([mids, z_vals[-1:]], dim=-1)\n",
    "\t\tlower = torch.concat([z_vals[:1], mids], dim=-1)\n",
    "\t\tt_rand = torch.rand([n_samples], device=z_vals.device)\n",
    "\t\tz_vals = lower + (upper - lower) * t_rand\n",
    "\tz_vals = z_vals.expand(list(rays_o.shape[:-1]) + [n_samples])\n",
    "\n",
    "\t# Apply scale from `rays_d` and offset from `rays_o` to samples\n",
    "\t# pts: (width, height, n_samples, 3)\n",
    "\tpts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals[..., :, None]\n",
    "\treturn pts, z_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw stratified samples from example\n",
    "rays_o = ray_origin.view([-1, 3])\n",
    "rays_d = ray_direction.view([-1, 3])\n",
    "n_samples = 8\n",
    "perturb = True\n",
    "inverse_depth = False\n",
    "with torch.no_grad():\n",
    "\tpts, z_vals = sample_stratified(\n",
    "\t\trays_o, rays_d, \n",
    "\t\tnear, far, \n",
    "\t\tn_samples,\n",
    "\t\tperturb=perturb, \n",
    "\t\tinverse_depth=inverse_depth)\n",
    "\n",
    "print('Input Points')\n",
    "print(pts.shape)\n",
    "print('')\n",
    "print('Distances Along Ray')\n",
    "print(z_vals.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize these sampled points. The unperturbed blue points are the bin \"centers.\" The red points are a sampling of perturbed points. Notice how the red points are slightly offset from the blue points above them, but all are constrained between `near` and `far`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vals = torch.zeros_like(z_vals)\n",
    "\n",
    "_, z_vals_unperturbed = sample_stratified(rays_o, rays_d, near, far, n_samples,\n",
    "\t\t\t\t\t\t\t\t  perturb=False, inverse_depth=inverse_depth)\n",
    "plt.plot(z_vals_unperturbed[0].cpu().numpy(), 1 + y_vals[0].cpu().numpy(), 'b-o')\n",
    "plt.plot(z_vals[0].cpu().numpy(), y_vals[0].cpu().numpy(), 'r-o')\n",
    "plt.ylim([-1, 2])\n",
    "plt.title('Stratified Sampling (blue) with Perturbation (red)')\n",
    "ax = plt.gca()\n",
    "ax.axes.yaxis.set_visible(False)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoder\n",
    "\n",
    "Much like Transformers, NeRFs make use of positional encoders. In this case, it's to map the inputs to a higher frequency space to compensate for the bias that neural networks have for learning lower-frequency functions.\n",
    "\n",
    "Here we build a simple `torch.nn.Module` of our positional encoder. The same encoder implementation can be applied to both input samples and view directions. However, we choose different parameters for these inputs. We use the default settings from the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "\tr\"\"\"\n",
    "\tSine-cosine positional encoder for input points.\n",
    "\t\"\"\"\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\td_input: int,\n",
    "\t\tn_freqs: int,\n",
    "\t\tlog_space: bool = False\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.d_input = d_input\n",
    "\t\tself.n_freqs = n_freqs\n",
    "\t\tself.log_space = log_space\n",
    "\t\tself.d_output = d_input * (1 + 2 * self.n_freqs)\n",
    "\t\tself.embed_fns = [lambda x: x]\n",
    "\n",
    "\t\t# Define frequencies in either linear or log scale\n",
    "\t\tif self.log_space:\n",
    "\t\t\tfreq_bands = 2.**torch.linspace(0., self.n_freqs - 1, self.n_freqs)\n",
    "\t\telse:\n",
    "\t\t\tfreq_bands = torch.linspace(2.**0., 2.**(self.n_freqs - 1), self.n_freqs)\n",
    "\n",
    "\t\t# Alternate sin and cos\n",
    "\t\tfor freq in freq_bands:\n",
    "\t\t\tself.embed_fns.append(lambda x, freq=freq: torch.sin(x * freq))\n",
    "\t\t\tself.embed_fns.append(lambda x, freq=freq: torch.cos(x * freq))\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tx\n",
    "\t) -> torch.Tensor:\n",
    "\t\tr\"\"\"\n",
    "\t\tApply positional encoding to input.\n",
    "\t\t\"\"\"\n",
    "\t\treturn torch.concat([fn(x) for fn in self.embed_fns], dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeRF Model\n",
    "\n",
    "Here we define the NeRF model, which consists primarily of a `ModuleList` of `Linear` layers, separated by non-linear activation functions and the occasional residual connection. This model features an optional input for view directions, which will alter the model architecture if provided at instantiation. This implementation is based on Section 3 of the original \"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\" paper and uses the same defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRF(nn.Module):\n",
    "\tr\"\"\"\n",
    "\tNeural radiance fields module.\n",
    "\t\"\"\"\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\td_input: int = 3,\n",
    "\t\tn_layers: int = 8,\n",
    "\t\td_filter: int = 256,\n",
    "\t\tskip: Tuple[int] = (4,),\n",
    "\t\td_viewdirs: Optional[int] = None\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.d_input = d_input\n",
    "\t\tself.skip = skip\n",
    "\t\tself.act = nn.functional.relu\n",
    "\t\tself.d_viewdirs = d_viewdirs\n",
    "\n",
    "\t\t# Create model layers\n",
    "\t\tself.layers = nn.ModuleList(\n",
    "\t\t\t[nn.Linear(self.d_input, d_filter)] +\n",
    "\t\t\t[nn.Linear(d_filter + self.d_input, d_filter) if i in skip \\\n",
    "\t\t\t else nn.Linear(d_filter, d_filter) for i in range(n_layers - 1)]\n",
    "\t\t)\n",
    "\n",
    "\t\t# Bottleneck layers\n",
    "\t\tif self.d_viewdirs is not None:\n",
    "\t\t\t# If using viewdirs, split alpha and RGB\n",
    "\t\t\tself.alpha_out = nn.Linear(d_filter, 1)\n",
    "\t\t\tself.rgb_filters = nn.Linear(d_filter, d_filter)\n",
    "\t\t\tself.branch = nn.Linear(d_filter + self.d_viewdirs, d_filter // 2)\n",
    "\t\t\tself.output = nn.Linear(d_filter // 2, 3)\n",
    "\t\telse:\n",
    "\t\t\t# If no viewdirs, use simpler output\n",
    "\t\t\tself.output = nn.Linear(d_filter, 4)\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tx: torch.Tensor,\n",
    "\t\tviewdirs: Optional[torch.Tensor] = None\n",
    "\t) -> torch.Tensor:\n",
    "\t\tr\"\"\"\n",
    "\t\tForward pass with optional view direction.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# Cannot use viewdirs if instantiated with d_viewdirs = None\n",
    "\t\tif self.d_viewdirs is None and viewdirs is not None:\n",
    "\t\t\traise ValueError('Cannot input x_direction if d_viewdirs was not given.')\n",
    "\n",
    "\t\t# Apply forward pass up to bottleneck\n",
    "\t\tx_input = x\n",
    "\t\tfor i, layer in enumerate(self.layers):\n",
    "\t\t\tx = self.act(layer(x))\n",
    "\t\t\tif i in self.skip:\n",
    "\t\t\t\tx = torch.cat([x, x_input], dim=-1)\n",
    "\n",
    "\t\t# Apply bottleneck\n",
    "\t\tif self.d_viewdirs is not None:\n",
    "\t\t\t# Split alpha from network output\n",
    "\t\t\talpha = self.alpha_out(x)\n",
    "\n",
    "\t\t\t# Pass through bottleneck to get RGB\n",
    "\t\t\tx = self.rgb_filters(x)\n",
    "\t\t\tx = torch.concat([x, viewdirs], dim=-1)\n",
    "\t\t\tx = self.act(self.branch(x))\n",
    "\t\t\tx = self.output(x)\n",
    "\n",
    "\t\t\t# Concatenate alphas to output\n",
    "\t\t\tx = torch.concat([x, alpha], dim=-1)\n",
    "\t\telse:\n",
    "\t\t\t# Simple output\n",
    "\t\t\tx = self.output(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic NeRF model\n",
    "description pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNeRF(nn.Module):\n",
    "\tr\"\"\"Dynamic Neural radiance fields module.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self,\n",
    "\t\td_input: int = 3,\n",
    "\t\tn_layers: int = 8,\n",
    "\t\td_filter: int = 256,\n",
    "\t\tskip: Tuple[int] = (4,),\n",
    "\t\td_viewdirs: Optional[int] = None,\n",
    "\n",
    "\t\td_time: int = 1,\n",
    "\t\tencode: Callable = None,\n",
    "\t\tzero_canonical: bool = False\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.d_input = d_input\n",
    "\t\tself.n_layers = n_layers\n",
    "\t\tself.d_filter = d_filter\n",
    "\t\tself.skip = skip\n",
    "\t\tself.act = nn.ReLU()\n",
    "\t\t# Time network\n",
    "\t\tself.d_time = d_time\n",
    "\t\tself.encode = encode\n",
    "\t\tself.zero_canonical = zero_canonical\n",
    "\n",
    "\t\t# Defining original nerf nodel\n",
    "\t\tself.nerf_model = NeRF(d_input, n_layers, d_filter, skip, d_viewdirs)\n",
    "\t\t# Defining time deformation network\n",
    "\t\tself.deformation_model = self._create()\n",
    "\t\t\n",
    "\n",
    "\tdef _create(self):\n",
    "\t\tinput_dimention = self.d_input+self.d_time\n",
    "\t\t\n",
    "\t\tlayers = [nn.Linear(input_dimention, self.d_filter)]\n",
    "\t\tfor i in range(1, self.n_layers):\n",
    "\t\t\tif i in self.skip:\n",
    "\t\t\t\tlayers += [nn.Linear(self.d_filter+input_dimention, self.d_filter)]\n",
    "\t\t\telse:\n",
    "\t\t\t\tlayers += [nn.Linear(self.d_filter, self.d_filter)]\n",
    "\t\tlayers += [nn.Linear(self.d_filter, 3)]\n",
    "\n",
    "\t\tlayers = nn.ModuleList(layers)\n",
    "\t\treturn layers \n",
    "\t\n",
    "\tdef _query_time(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\t\toriginal_input = x\n",
    "\t\tfor i, layer in enumerate(self.deformation_model):\n",
    "\t\t\tif i in self.skip:\n",
    "\t\t\t\tx = torch.cat([original_input, x], -1)\n",
    "\t\t\t\n",
    "\t\t\tx = layer(x)\n",
    "\t\t\t\n",
    "\t\t\tif i < len(self.deformation_model)-1:\n",
    "\t\t\t\tx = self.act(x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tx: torch.Tensor,\n",
    "\t\ttimesteps: torch.Tensor,\n",
    "\t\tviewdirs: Optional[torch.Tensor] = None,\n",
    "\t) -> torch.Tensor:\n",
    "\t\tr\"\"\"\n",
    "\t\tForward pass through time deformation network and NeRF\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tinputs = torch.cat([x, timesteps], -1)\n",
    "\t\tdx = self._query_time(inputs)\n",
    "\t\tif self.zero_canonical:\n",
    "\t\t\tcond = timesteps[:,0] == 0\n",
    "\t\t\tdx[cond] = 0.0\n",
    "\n",
    "\t\toriginal_points = x[:,:3]\n",
    "\t\tpoints_dx = original_points + dx\n",
    "\t\tpoints_dx = self.encode(points_dx)\n",
    "\n",
    "\t\toutput = self.nerf_model(points_dx, viewdirs)\n",
    "\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNeRF network test\n",
    "X = torch.randn(10,3)#.to(device)\n",
    "V = torch.randn(10,3)#.to(device)\n",
    "T = torch.randn(10,1)#.to(device)\n",
    "\n",
    "T[[0,1,2,3]] = 0\n",
    "\n",
    "# Encoders\n",
    "encoder = PositionalEncoder(3, 10)\n",
    "encode = lambda x: encoder(x)\n",
    "encoder_time = PositionalEncoder(1, 4)\n",
    "encode_time = lambda x: encoder_time(x)\n",
    "\n",
    "\n",
    "X = encode(X)\n",
    "V = encode(V)\n",
    "T = encode_time(T)\n",
    "\n",
    "dnerf = DNeRF(\n",
    "\t\t\td_input = encoder.d_output, \n",
    "\t\t\tn_layers=6, \n",
    "\t\t\td_filter=128, \n",
    "\t\t\tskip=[3],\n",
    "\t\t\td_viewdirs=encoder.d_output,\n",
    "\t\t\td_time=encoder_time.d_output,\n",
    "\t\t\tencode=encode,\n",
    "\t\t\tzero_canonical=True)\n",
    "print(dnerf)\n",
    "dnerf(X,T,V).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volume Rendering\n",
    "\n",
    "From the raw NeRF outputs, we still need to convert these into an image. This is where we apply the volume integration described in Equations 1-3 in Section 4 of the paper. Essentially, we take the weighted sum of all samples along the ray of each pixel to get the estimated color value at that pixel. Each RGB sample is weighted by its alpha value. Higher alpha values indicate higher likelihood that the sampled area is opaque, therefore points further along the ray are likelier to be occluded. The cumulative product ensures that those further points are dampened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumprod_exclusive(\n",
    "\ttensor: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "\tr\"\"\"\n",
    "\t(Courtesy of https://github.com/krrish94/nerf-pytorch)\n",
    "\n",
    "\tMimick functionality of tf.math.cumprod(..., exclusive=True), as it isn't available in PyTorch.\n",
    "\n",
    "\tArgs:\n",
    "\ttensor (torch.Tensor): Tensor whose cumprod (cumulative product, see `torch.cumprod`) along dim=-1\n",
    "\t\tis to be computed.\n",
    "\tReturns:\n",
    "\tcumprod (torch.Tensor): cumprod of Tensor along dim=-1, mimiciking the functionality of\n",
    "\t\ttf.math.cumprod(..., exclusive=True) (see `tf.math.cumprod` for details).\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Compute regular cumprod first (this is equivalent to `tf.math.cumprod(..., exclusive=False)`).\n",
    "\tcumprod = torch.cumprod(tensor, -1)\n",
    "\t# \"Roll\" the elements along dimension 'dim' by 1 element.\n",
    "\tcumprod = torch.roll(cumprod, 1, -1)\n",
    "\t# Replace the first element by \"1\" as this is what tf.cumprod(..., exclusive=True) does.\n",
    "\tcumprod[..., 0] = 1.\n",
    "\n",
    "\treturn cumprod\n",
    "\n",
    "def raw2outputs(\n",
    "\traw: torch.Tensor,\n",
    "\tz_vals: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\traw_noise_std: float = 0.0,\n",
    "\twhite_bkgd: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tConvert the raw NeRF output into RGB and other maps.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Difference between consecutive elements of `z_vals`. [n_rays, n_samples]\n",
    "\tdists = z_vals[..., 1:] - z_vals[..., :-1]\n",
    "\tdists = torch.cat([dists, 1e10 * torch.ones_like(dists[..., :1])], dim=-1)\n",
    "\n",
    "\t# Multiply each distance by the norm of its corresponding direction ray\n",
    "\t# to convert to real world distance (accounts for non-unit directions).\n",
    "\tdists = dists * torch.norm(rays_d[..., None, :], dim=-1)\n",
    "\n",
    "\t# Add noise to model's predictions for density. Can be used to\n",
    "\t# regularize network during training (prevents floater artifacts).\n",
    "\tnoise = 0.\n",
    "\tif raw_noise_std > 0.:\n",
    "\t\tnoise = torch.randn(raw[..., 3].shape) * raw_noise_std\n",
    "\n",
    "\t# Predict density of each sample along each ray. Higher values imply\n",
    "\t# higher likelihood of being absorbed at this point. [n_rays, n_samples]\n",
    "\talpha = 1.0 - torch.exp(-nn.functional.relu(raw[..., 3] + noise) * dists)\n",
    "\n",
    "\t# Compute weight for RGB of each sample along each ray. [n_rays, n_samples]\n",
    "\t# The higher the alpha, the lower subsequent weights are driven.\n",
    "\tweights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n",
    "\n",
    "\t# Compute weighted RGB map.\n",
    "\trgb = torch.sigmoid(raw[..., :3])  # [n_rays, n_samples, 3]\n",
    "\trgb_map = torch.sum(weights[..., None] * rgb, dim=-2)  # [n_rays, 3]\n",
    "\n",
    "\t# Estimated depth map is predicted distance.\n",
    "\tdepth_map = torch.sum(weights * z_vals, dim=-1)\n",
    "\n",
    "\t# Disparity map is inverse depth.\n",
    "\tdisp_map = 1. / torch.max(1e-10 * torch.ones_like(depth_map),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tdepth_map / torch.sum(weights, -1))\n",
    "\n",
    "\t# Sum of weights along each ray. In [0, 1] up to numerical error.\n",
    "\tacc_map = torch.sum(weights, dim=-1)\n",
    "\n",
    "\t# To composite onto a white background, use the accumulated alpha map.\n",
    "\tif white_bkgd:\n",
    "\t\trgb_map = rgb_map + (1. - acc_map[..., None])\n",
    "\n",
    "\treturn rgb_map, depth_map, acc_map, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Volume Sampling\n",
    "\n",
    "The 3D space is in fact very sparse with occlusions and so most points don't contribute much to the rendered image. It is therefore more beneficial to oversample regions with a high likelihood of contributing to the integral. Here we apply learned, normalized weights to the first set of samples to create a PDF across the ray, then apply inverse transform sampling to this PDF to gather a second set of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pdf(\n",
    "\tbins: torch.Tensor,\n",
    "\tweights: torch.Tensor,\n",
    "\tn_samples: int,\n",
    "\tperturb: bool = False\n",
    ") -> torch.Tensor:\n",
    "\tr\"\"\"\n",
    "\tApply inverse transform sampling to a weighted set of points.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Normalize weights to get PDF.\n",
    "\tpdf = (weights + 1e-5) / torch.sum(weights + 1e-5, -1, keepdims=True) # [n_rays, weights.shape[-1]]\n",
    "\n",
    "\t# Convert PDF to CDF.\n",
    "\tcdf = torch.cumsum(pdf, dim=-1) # [n_rays, weights.shape[-1]]\n",
    "\tcdf = torch.concat([torch.zeros_like(cdf[..., :1]), cdf], dim=-1) # [n_rays, weights.shape[-1] + 1]\n",
    "\n",
    "\t# Take sample positions to grab from CDF. Linear when perturb == 0.\n",
    "\tif not perturb:\n",
    "\t\tu = torch.linspace(0., 1., n_samples, device=cdf.device)\n",
    "\t\tu = u.expand(list(cdf.shape[:-1]) + [n_samples]) # [n_rays, n_samples]\n",
    "\telse:\n",
    "\t\tu = torch.rand(list(cdf.shape[:-1]) + [n_samples], device=cdf.device) # [n_rays, n_samples]\n",
    "\n",
    "\t# Find indices along CDF where values in u would be placed.\n",
    "\tu = u.contiguous() # Returns contiguous tensor with same values.\n",
    "\tinds = torch.searchsorted(cdf, u, right=True) # [n_rays, n_samples]\n",
    "\n",
    "\t# Clamp indices that are out of bounds.\n",
    "\tbelow = torch.clamp(inds - 1, min=0)\n",
    "\tabove = torch.clamp(inds, max=cdf.shape[-1] - 1)\n",
    "\tinds_g = torch.stack([below, above], dim=-1) # [n_rays, n_samples, 2]\n",
    "\n",
    "\t# Sample from cdf and the corresponding bin centers.\n",
    "\tmatched_shape = list(inds_g.shape[:-1]) + [cdf.shape[-1]]\n",
    "\tcdf_g = torch.gather(cdf.unsqueeze(-2).expand(matched_shape), dim=-1,\n",
    "\t\t\t\t\t\t\t\t\t\t\t index=inds_g)\n",
    "\tbins_g = torch.gather(bins.unsqueeze(-2).expand(matched_shape), dim=-1,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tindex=inds_g)\n",
    "\n",
    "\t# Convert samples to ray length.\n",
    "\tdenom = (cdf_g[..., 1] - cdf_g[..., 0])\n",
    "\tdenom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n",
    "\tt = (u - cdf_g[..., 0]) / denom\n",
    "\tsamples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n",
    "\n",
    "\treturn samples # [n_rays, n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_hierarchical(\n",
    "\trays_o: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\tz_vals: torch.Tensor,\n",
    "\tweights: torch.Tensor,\n",
    "\tn_samples: int,\n",
    "\tperturb: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tApply hierarchical sampling to the rays.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Draw samples from PDF using z_vals as bins and weights as probabilities.\n",
    "\tz_vals_mid = .5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
    "\tnew_z_samples = sample_pdf(z_vals_mid, weights[..., 1:-1], n_samples,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tperturb=perturb)\n",
    "\tnew_z_samples = new_z_samples.detach()\n",
    "\n",
    "\t# Resample points from ray based on PDF.\n",
    "\tz_vals_combined, _ = torch.sort(torch.cat([z_vals, new_z_samples], dim=-1), dim=-1)\n",
    "\tpts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals_combined[..., :, None]  # [N_rays, N_samples + n_samples, 3]\n",
    "\treturn pts, z_vals_combined, new_z_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Forward Pass\n",
    "\n",
    "Here is where we put everything together to compute a single forward pass through our model.\n",
    "\n",
    "Due to potential memory issues, the forward pass is computed in \"chunks,\" which are then aggregated across a single batch. The gradient propagation is done after the whole batch is processed, hence the distinction between \"chunks\" and \"batches.\" Chunking is especially important for the Google Colab environment, which provides more modest resources than those cited in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(\n",
    "\tinputs: torch.Tensor,\n",
    "\tchunksize: int = 2**15\n",
    ") -> List[torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tDivide an input into chunks.\n",
    "\t\"\"\"\n",
    "\treturn [inputs[i:i + chunksize] for i in range(0, inputs.shape[0], chunksize)]\n",
    "\n",
    "def prepare_chunks(\n",
    "\tpoints: torch.Tensor,\n",
    "\tencoding_function: Callable[[torch.Tensor], torch.Tensor],\n",
    "\tchunksize: int = 2**15\n",
    ") -> List[torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tEncode and chunkify points to prepare for NeRF model.\n",
    "\t\"\"\"\n",
    "\tpoints = points.reshape((-1, 3))\n",
    "\tpoints = encoding_function(points)\n",
    "\tpoints = get_chunks(points, chunksize=chunksize)\n",
    "\treturn points\n",
    "\n",
    "def prepare_viewdirs_chunks(\n",
    "\tpoints: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\tencoding_function: Callable[[torch.Tensor], torch.Tensor],\n",
    "\tchunksize: int = 2**15\n",
    ") -> List[torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tEncode and chunkify viewdirs to prepare for NeRF model.\n",
    "\t\"\"\"\n",
    "\t# Prepare the viewdirs\n",
    "\tviewdirs = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
    "\tviewdirs = viewdirs[:, None, ...].expand(points.shape).reshape((-1, 3))\n",
    "\tviewdirs = encoding_function(viewdirs)\n",
    "\tviewdirs = get_chunks(viewdirs, chunksize=chunksize)\n",
    "\treturn viewdirs\n",
    "\n",
    "def prepare_time(\n",
    "\ttimesteps, \n",
    "\ttime_encoding_fn: Callable[[torch.Tensor], torch.Tensor], \n",
    "\tchunksize: int = 2**15\n",
    ") -> List[torch.Tensor]:\n",
    "\tr\"\"\"\n",
    "\tEncode and chunkify timepoints to prepare for NeRF model.\n",
    "\t\"\"\"\n",
    "\ttimesteps = time_encoding_fn(timesteps)\n",
    "\ttimesteps = get_chunks(timesteps, chunksize=chunksize)\n",
    "\treturn timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nerf_forward(\n",
    "\trays_o: torch.Tensor,\n",
    "\trays_d: torch.Tensor,\n",
    "\ttimesteps: torch.Tensor,\n",
    "\tnear: float,\n",
    "\tfar: float,\n",
    "\tencoding_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "\tcoarse_model: nn.Module,\n",
    "\tkwargs_sample_stratified: dict = None,\n",
    "\tn_samples_hierarchical: int = 0,\n",
    "\tkwargs_sample_hierarchical: dict = None,\n",
    "\tfine_model = None,\n",
    "\tviewdirs_encoding_fn: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n",
    "\ttime_encoding_fn: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n",
    "\tchunksize: int = 2**15\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]:\n",
    "\tr\"\"\"\n",
    "\tCompute forward pass through model(s).\n",
    "\t\"\"\"\n",
    "\t# Set no kwargs if none are given.\n",
    "\tif kwargs_sample_stratified is None:\n",
    "\t\tkwargs_sample_stratified = {}\n",
    "\tif kwargs_sample_hierarchical is None:\n",
    "\t\tkwargs_sample_hierarchical = {}\n",
    "\n",
    "\t# Sample query points along each ray.\n",
    "\tquery_points, z_vals = sample_stratified(\n",
    "\t\t\trays_o, rays_d, near, far, **kwargs_sample_stratified)\n",
    "\t# Prepare batches.\n",
    "\tbatches = prepare_chunks(query_points, encoding_fn, chunksize=chunksize)\n",
    "\tif viewdirs_encoding_fn is not None:\n",
    "\t\tbatches_viewdirs = prepare_viewdirs_chunks(\n",
    "\t\t\tquery_points, rays_d,\n",
    "\t\t\tviewdirs_encoding_fn,\n",
    "\t\t\tchunksize=chunksize)\n",
    "\telse:\n",
    "\t\tbatches_viewdirs = [None] * len(batches)\n",
    "\n",
    "\t# Time\n",
    "\tif timesteps.shape[0] == 1: # only one image and one timestep\n",
    "\t\t# Expanding to match other data\n",
    "\t\tn_points = query_points.reshape(-1,3).shape[0]\n",
    "\t\texpanded_timesteps = timesteps.expand(n_points).reshape(-1,1)\n",
    "\telse: # For each ray repeating timestep along sampling dimention\n",
    "\t\tn_rays = query_points.shape[1]\n",
    "\t\texpanded_timesteps = timesteps.repeat(1,n_rays).reshape(-1,1)\n",
    "\tbatches_times = prepare_time(expanded_timesteps, time_encoding_fn, chunksize)\n",
    "\t\n",
    "\t###\n",
    "\t# Coarse model pass.\n",
    "\t# Split the encoded points into \"chunks\", run the model on all chunks, and\n",
    "\t# concatenate the results (to avoid out-of-memory issues).\n",
    "\tpredictions = []\n",
    "\tfor batch, batch_viewdirs, batch_time in zip(batches, batches_viewdirs, batches_times):\n",
    "\t\tone_batch_prediction = coarse_model(batch, viewdirs=batch_viewdirs, timesteps=batch_time)\n",
    "\t\tpredictions.append(one_batch_prediction)\n",
    "\t\n",
    "\traw = torch.cat(predictions, dim=0)\n",
    "\traw = raw.reshape(list(query_points.shape[:2]) + [raw.shape[-1]])\n",
    "\n",
    "\t# Perform differentiable volume rendering to re-synthesize the RGB image.\n",
    "\trgb_map, depth_map, acc_map, weights = raw2outputs(raw, z_vals, rays_d)\n",
    "\t# rgb_map, depth_map, acc_map, weights = render_volume_density(raw, rays_o, z_vals)\n",
    "\n",
    "\toutputs = {\n",
    "\t\t\t'z_vals_stratified': z_vals\n",
    "\t}\n",
    "\n",
    "\t###\n",
    "\t# Fine model pass.\n",
    "\t\n",
    "\tif n_samples_hierarchical > 0:\n",
    "\t\t# Save previous outputs to return.\n",
    "\t\trgb_map_0, depth_map_0, acc_map_0 = rgb_map, depth_map, acc_map\n",
    "\n",
    "\t\t# Apply hierarchical sampling for fine query points.\n",
    "\t\tquery_points, z_vals_combined, z_hierarch = sample_hierarchical(\n",
    "\t\t\trays_o, rays_d, z_vals, weights, n_samples_hierarchical,\n",
    "\t\t\t**kwargs_sample_hierarchical)\n",
    "\n",
    "\t\t# Prepare inputs as before.\n",
    "\t\tbatches = prepare_chunks(query_points, encoding_fn, chunksize=chunksize)\n",
    "\t\tif viewdirs_encoding_fn is not None:\n",
    "\t\t\tbatches_viewdirs = prepare_viewdirs_chunks(\n",
    "\t\t\t\tquery_points, rays_d,\n",
    "\t\t\t\tviewdirs_encoding_fn,\n",
    "\t\t\t\tchunksize=chunksize)\n",
    "\t\telse:\n",
    "\t\t\tbatches_viewdirs = [None] * len(batches)\n",
    "\n",
    "\t\t# Time\n",
    "\t\tif timesteps.shape[0] == 1: # only one image and one timestep\n",
    "\t\t\t# Expanding to match other data\n",
    "\t\t\tn_points = query_points.reshape(-1,3).shape[0]\n",
    "\t\t\texpanded_timesteps = timesteps.expand(n_points).reshape(-1,1)\n",
    "\t\telse: # For each ray repeating timestep along sampling dimention\n",
    "\t\t\tn_rays = query_points.shape[1]\n",
    "\t\t\texpanded_timesteps = timesteps.repeat(1,n_rays).reshape(-1,1)\n",
    "\t\tbatches_times = prepare_time(expanded_timesteps, time_encoding_fn, chunksize)\n",
    "\n",
    "\t\t# Forward pass new samples through fine model.\n",
    "\t\tfine_model = fine_model if fine_model is not None else coarse_model\n",
    "\t\tpredictions = []\n",
    "\t\tfor batch, batch_viewdirs, batch_time in zip(batches, batches_viewdirs, batches_times):\n",
    "\t\t\tone_batch_predictions = fine_model(batch, viewdirs=batch_viewdirs, timesteps=batch_time)\n",
    "\t\t\tpredictions.append(one_batch_predictions)\n",
    "\t\traw = torch.cat(predictions, dim=0)\n",
    "\t\traw = raw.reshape(list(query_points.shape[:2]) + [raw.shape[-1]])\n",
    "\n",
    "\t\t# Perform differentiable volume rendering to re-synthesize the RGB image.\n",
    "\t\trgb_map, depth_map, acc_map, weights = raw2outputs(raw, z_vals_combined, rays_d)\n",
    "\n",
    "\t\t# Store outputs.\n",
    "\t\toutputs['z_vals_hierarchical'] = z_hierarch\n",
    "\t\toutputs['rgb_map_0'] = rgb_map_0\n",
    "\t\toutputs['depth_map_0'] = depth_map_0\n",
    "\t\toutputs['acc_map_0'] = acc_map_0\n",
    "\n",
    "\t# Store outputs.\n",
    "\toutputs['rgb_map'] = rgb_map\n",
    "\toutputs['depth_map'] = depth_map\n",
    "\toutputs['acc_map'] = acc_map\n",
    "\toutputs['weights'] = weights\n",
    "\treturn outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "At long last, we have (almost) everything we need to train the model. Now we will do some setup for a simple training procedure, creating hyperparameters and helper functions, then train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "All hyperparameters for training are set here. Defaults were taken from the original, unless computational constraints prohibit them. In this case, we apply sensible defaults that are well within the resources provided by Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoders\n",
    "d_input = 3           # Number of input dimensions\n",
    "n_freqs = 10          # Number of encoding functions for samples\n",
    "log_space = True      # If set, frequencies scale in log space\n",
    "use_viewdirs = True   # If set, use view direction as input\n",
    "n_freqs_views = 4    # Number of encoding functions for views\n",
    "n_freqs_time = 4     # Number of encoding functions for time\n",
    "\n",
    "# Stratified sampling\n",
    "n_samples = 64         # Number of spatial samples per ray\n",
    "perturb = True         # If set, applies noise to sample positions\n",
    "inverse_depth = False  # If set, samples points linearly in inverse depth\n",
    "\n",
    "# Model\n",
    "d_filter = 128          # Dimensions of linear layer filters\n",
    "n_layers = 2            # Number of layers in network bottleneck\n",
    "skip = []               # Layers at which to apply input residual\n",
    "use_fine_model = True   # If set, creates a fine model\n",
    "d_filter_fine = 128     # Dimensions of linear layer filters of fine network\n",
    "n_layers_fine = 3#6       # Number of layers in fine network bottleneck \n",
    "skip_fine = []\t\t\t# Layers at which to apply input residual of fine network\n",
    "zero_canonical = True  # Use timestep 0 for a canonical space representation for D-NeRF\n",
    "\n",
    "# Hierarchical sampling\n",
    "n_samples_hierarchical = 64 #64   # Number of samples per ray\n",
    "perturb_hierarchical = False  # If set, applies noise to sample positions\n",
    "\n",
    "# Optimizer\n",
    "lr = 5e-4  \t\t\t\t\t\t\t\t # Learning rate\n",
    "scheduler_start_end_factors = [1.0, 0.9] # Linear decay of learning rate\n",
    "\n",
    "# Training\n",
    "n_iters = 10000\n",
    "batch_size = 2**14          # Number of rays per gradient step (power of 2)\n",
    "one_image_per_step = [True,False][1]   # One image per gradient step (disables batching)\n",
    "chunksize = 2**14           # Modify as needed to fit in GPU memory\n",
    "center_crop = True          # Crop the center of image (one_image_per_)\n",
    "center_crop_iters = 50      # Stop cropping center after this many epochs\n",
    "display_rate = 50           # Display test output every X epochs\n",
    "shuffle_data = True\t\t    # Shuffle rays on every iteration. Mostly for debug. Default: True\n",
    "\n",
    "# Early Stopping\n",
    "warmup_iters = 100          # Number of iterations during warmup phase\n",
    "warmup_min_fitness = 10.0   # Min val PSNR to continue training at warmup_iters\n",
    "n_restarts = 10             # Number of times to restart if training stalls\n",
    "\n",
    "# We bundle the kwargs for various functions to pass all at once.\n",
    "kwargs_sample_stratified = {\n",
    "\t'n_samples': n_samples,\n",
    "\t'perturb': perturb,\n",
    "\t'inverse_depth': inverse_depth\n",
    "}\n",
    "kwargs_sample_hierarchical = {\n",
    "\t'perturb': perturb\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Classes and Functions\n",
    "\n",
    "Here we create some helper functions for training. NeRF can be prone to local minima, in which training will quickly stall and produce blank outputs. `EarlyStopping` is used to restart the training when learning stalls, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(\n",
    "\tz_vals: torch.Tensor,\n",
    "\tz_hierarch: Optional[torch.Tensor] = None,\n",
    "\tax: Optional[np.ndarray] = None):\n",
    "\tr\"\"\"\n",
    "\tPlot stratified and (optional) hierarchical samples.\n",
    "\t\"\"\"\n",
    "\ty_vals = 1 + np.zeros_like(z_vals)\n",
    "\n",
    "\tif ax is None:\n",
    "\t\tax = plt.subplot()\n",
    "\tax.plot(z_vals, y_vals, 'b-o')\n",
    "\tif z_hierarch is not None:\n",
    "\t\ty_hierarch = np.zeros_like(z_hierarch)\n",
    "\t\tax.plot(z_hierarch, y_hierarch, 'r-o')\n",
    "\tax.set_ylim([-1, 2])\n",
    "\tax.set_title('Stratified  Samples (blue) and Hierarchical Samples (red)')\n",
    "\tax.axes.yaxis.set_visible(False)\n",
    "\tax.grid(True)\n",
    "\treturn ax\n",
    "\n",
    "def crop_center(\n",
    "\timg: torch.Tensor,\n",
    "\tfrac: float = 0.5\n",
    ") -> torch.Tensor:\n",
    "\tr\"\"\"\n",
    "\tCrop center square from image.\n",
    "\t\"\"\"\n",
    "\th_offset = round(img.shape[0] * (frac / 2))\n",
    "\tw_offset = round(img.shape[1] * (frac / 2))\n",
    "\treturn img[h_offset:-h_offset, w_offset:-w_offset]\n",
    "\n",
    "class EarlyStopping:\n",
    "\tr\"\"\"\n",
    "\tEarly stopping helper based on fitness criterion.\n",
    "\t\"\"\"\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tpatience: int = 30,\n",
    "\t\tmargin: float = 1e-4\n",
    "\t):\n",
    "\t\tself.best_fitness = 0.0  # In our case PSNR\n",
    "\t\tself.best_iter = 0\n",
    "\t\tself.margin = margin\n",
    "\t\tself.patience = patience or float('inf')  # epochs to wait after fitness stops improving to stop\n",
    "\n",
    "\tdef __call__(\n",
    "\t\tself,\n",
    "\t\titer: int,\n",
    "\t\tfitness: float\n",
    "\t):\n",
    "\t\tr\"\"\"\n",
    "\t\tCheck if criterion for stopping is met.\n",
    "\t\t\"\"\"\n",
    "\t\tif (fitness - self.best_fitness) > self.margin:\n",
    "\t\t\tself.best_iter = iter\n",
    "\t\t\tself.best_fitness = fitness\n",
    "\t\tdelta = iter - self.best_iter\n",
    "\t\tstop = delta >= self.patience  # stop training if patience exceeded\n",
    "\t\treturn stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_models():\n",
    "\tr\"\"\"\n",
    "\tInitialize models, encoders, and optimizer for NeRF training.\n",
    "\t\"\"\"\n",
    "\t# Encoders\n",
    "\tencoder = PositionalEncoder(d_input, n_freqs, log_space=log_space)\n",
    "\tencode = lambda x: encoder(x)\n",
    "\n",
    "\t# View direction encoders\n",
    "\tif use_viewdirs:\n",
    "\t\tencoder_viewdirs = PositionalEncoder(\n",
    "\t\t\td_input, n_freqs_views,log_space=log_space)\n",
    "\t\tencode_viewdirs = lambda x: encoder_viewdirs(x)\n",
    "\t\td_viewdirs = encoder_viewdirs.d_output\n",
    "\telse:\n",
    "\t\tencode_viewdirs = None\n",
    "\t\td_viewdirs = None\n",
    "\n",
    "    # Time encoder\n",
    "\tencoder_time = PositionalEncoder(1, n_freqs_time, log_space=log_space)\n",
    "\tencode_time = lambda x: encoder_time(x)\n",
    "\td_timesteps = encoder_time.d_output\n",
    "\n",
    "\t# Models\n",
    "\tmodel = DNeRF(\n",
    "\t\t\td_input = encoder.d_output, \n",
    "\t\t\tn_layers = n_layers, \n",
    "\t\t\td_filter = d_filter, \n",
    "\t\t\tskip = skip,\n",
    "\t\t\td_viewdirs = d_viewdirs,\n",
    "\t\t\td_time = d_timesteps,\n",
    "\t\t\tencode = encode,\n",
    "\t\t\tzero_canonical=zero_canonical)\n",
    "\t\n",
    "\tmodel.to(device)\n",
    "\tmodel_params = list(model.parameters())\n",
    "\tif use_fine_model:\n",
    "\t\tfine_model = DNeRF(\n",
    "\t\t\t\t\td_input = encoder.d_output, \n",
    "\t\t\t\t\tn_layers=n_layers_fine, \n",
    "\t\t\t\t\td_filter=d_filter_fine, \n",
    "\t\t\t\t\tskip=skip_fine,\n",
    "\t\t\t\t\td_viewdirs=d_viewdirs,\n",
    "\t\t\t\t\td_time=d_timesteps,\n",
    "\t\t\t\t\tencode=encode,\n",
    "\t\t\t\t\tzero_canonical=zero_canonical)\n",
    "\t\t\n",
    "\t\tfine_model.to(device)\n",
    "\t\tmodel_params = model_params + list(fine_model.parameters())\n",
    "\telse:\n",
    "\t\tfine_model = None\n",
    "\n",
    "\t# Optimizer\n",
    "\toptimizer = torch.optim.Adam(model_params, lr=lr)\n",
    "\n",
    "\t# Early Stopping\n",
    "\twarmup_stopper = EarlyStopping(patience=50)\n",
    "\n",
    "\t# Scheduler \n",
    "\tscheduler = torch.optim.lr_scheduler.LinearLR(optimizer, \n",
    "\t\t\tstart_factor=scheduler_start_end_factors[0], \n",
    "\t\t\tend_factor=scheduler_start_end_factors[1], \n",
    "\t\t\ttotal_iters=n_iters)\n",
    "\n",
    "\treturn model, fine_model, encode, encode_viewdirs, encode_time, optimizer, warmup_stopper, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Here we start training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_time(i, i_batch, rays_rgb=None, time_bundle=None):\n",
    "\tmodel.train()\n",
    "\n",
    "\tif one_image_per_step:\n",
    "\t\t# Randomly pick an image as the target.\n",
    "\t\t# Target Image\n",
    "\t\ttarget_img_idx = np.random.randint(images.shape[0])\n",
    "\t\ttarget_img = images[target_img_idx].to(device)\n",
    "\t\tif center_crop and i < center_crop_iters:\n",
    "\t\t\ttarget_img = crop_center(target_img)\n",
    "\t\theight, width = target_img.shape[:2]\n",
    "\t\ttarget_img = target_img.reshape([-1, 3])\n",
    "\t\t# Pose\n",
    "\t\ttarget_pose = poses[target_img_idx].to(device)\n",
    "\t\t# Rays\n",
    "\t\trays_o, rays_d = get_rays(height, width, focal, target_pose)\n",
    "\t\trays_o = rays_o.reshape([-1, 3]).to(device)\n",
    "\t\trays_d = rays_d.reshape([-1, 3]).to(device)\n",
    "\t\t# Handle Time\n",
    "\t\ttimesteps = times[target_img_idx].reshape(1).to(device)\n",
    "\telse:\n",
    "\t\t\n",
    "\t\t# Reset and Shuffle at the beggining of batchify\n",
    "\t\tif i_batch >= rays_rgb.shape[0]:\n",
    "\n",
    "\t\t\t\ti_batch = 0\n",
    "\t\t\t\tif shuffle_data:\n",
    "\t\t\t\t\trand_indexes = torch.randperm(rays_rgb.shape[0])\n",
    "\t\t\t\t\trays_rgb = rays_rgb[rand_indexes]\n",
    "\t\t\t\t\ttime_bundle = time_bundle[rand_indexes]\n",
    "\t\t# Random over all images.\n",
    "\t\tbatch = rays_rgb[i_batch:i_batch + batch_size]\n",
    "\t\tbatch = torch.transpose(batch, 0, 1).to(device)\n",
    "\t\trays_o, rays_d, target_img = batch\n",
    "\t\theight, width = target_img.shape[:2]\n",
    "\t\t\n",
    "\t\t# Handle Time\n",
    "\t\ttimesteps = time_bundle[i_batch:i_batch + batch_size].to(device)\n",
    "\t\t\n",
    "\t\t# This should be last!\n",
    "\t\ti_batch += batch_size\n",
    "\n",
    "\t# Run one iteration of TinyNeRF and get the rendered RGB image.\n",
    "\toutputs = nerf_forward(\n",
    "\t\trays_o, rays_d, timesteps,\n",
    "        near, far, encode, model,\n",
    "        kwargs_sample_stratified=kwargs_sample_stratified,\n",
    "        n_samples_hierarchical=n_samples_hierarchical,\n",
    "        kwargs_sample_hierarchical=kwargs_sample_hierarchical,\n",
    "        fine_model=fine_model,\n",
    "        viewdirs_encoding_fn=encode_viewdirs,\n",
    "\t\ttime_encoding_fn=encode_time,\n",
    "        chunksize=chunksize)\n",
    "\n",
    "\t# Check for any numerical issues.\n",
    "\tfor k, v in outputs.items():\n",
    "\t\tif torch.isnan(v).any():\n",
    "\t\t\tprint(f\"! [Numerical Alert] {k} contains NaN.\")\n",
    "\t\tif torch.isinf(v).any():\n",
    "\t\t\tprint(f\"! [Numerical Alert] {k} contains Inf.\")\n",
    "\n",
    "\t# Backprop!\n",
    "\trgb_predicted = outputs['rgb_map']\n",
    "\tloss = torch.nn.functional.mse_loss(rgb_predicted, target_img)\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\toptimizer.zero_grad()\n",
    "\tpsnr = -10. * torch.log10(loss)\n",
    "\tscheduler.step()\n",
    "\t\n",
    "\treturn psnr.item(), i_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_time(i, iternums, train_psnrs, val_psnrs):\n",
    "\tmodel.eval()\n",
    "\theight, width = testimg.shape[:2]\n",
    "\trays_o, rays_d = get_rays(height, width, focal, testpose)\n",
    "\trays_o = rays_o.reshape([-1, 3]).to(device)\n",
    "\trays_d = rays_d.reshape([-1, 3]).to(device)\n",
    "\toutputs = nerf_forward(rays_o, rays_d, testtime.to(device),\n",
    "\t\t\t\t\t\t\tnear, far, encode, model,\n",
    "\t\t\t\t\t\t\tkwargs_sample_stratified=kwargs_sample_stratified,\n",
    "\t\t\t\t\t\t\tn_samples_hierarchical=n_samples_hierarchical,\n",
    "\t\t\t\t\t\t\tkwargs_sample_hierarchical=kwargs_sample_hierarchical,\n",
    "\t\t\t\t\t\t\tfine_model=fine_model,\n",
    "\t\t\t\t\t\t\tviewdirs_encoding_fn=encode_viewdirs,\n",
    "\t\t\t\t\t\t\ttime_encoding_fn=encode_time,\n",
    "\t\t\t\t\t\t\tchunksize=chunksize)\n",
    "\n",
    "\trgb_predicted = outputs['rgb_map']\n",
    "\ttestimg_flat = testimg.reshape(-1, 3).to(device)\n",
    "\tloss = torch.nn.functional.mse_loss(rgb_predicted, testimg_flat)\n",
    "\tprint(\"Loss:\", loss.item())\n",
    "\tval_psnr = -10. * torch.log10(loss)\n",
    "\tval_psnrs.append(val_psnr.item())\n",
    "\titernums.append(i)\n",
    "\n",
    "\t# Plot example outputs outside\n",
    "\trender(i, rgb_predicted, iternums, outputs,train_psnrs,val_psnrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(i, rgb_predicted, iternums, outputs,train_psnrs,val_psnrs):\n",
    "\tfig, ax = plt.subplots(1, 4, figsize=(24,4), gridspec_kw={'width_ratios': [1, 1, 1, 3]})\n",
    "    # Plot example outputs\n",
    "\t\n",
    "\tax[0].imshow(rgb_predicted.reshape([height, width, 3]).detach().cpu().numpy())\n",
    "\tax[0].set_title(f'Iteration: {i}')\n",
    "\tax[1].imshow(testimg.detach().cpu().numpy())\n",
    "\tax[1].set_title(f'Target\\nTest timestep: {testtime[0].item()}')\n",
    "\tax[2].plot(range(0, i + 1), train_psnrs, 'r')\n",
    "\tax[2].plot(iternums, val_psnrs, 'b')\n",
    "\tax[2].set_title('PSNR (train=red, val=blue')\n",
    "\tz_vals_strat = outputs['z_vals_stratified'].view((-1, n_samples))\n",
    "\tz_sample_strat = z_vals_strat[z_vals_strat.shape[0] // 2].detach().cpu().numpy()\n",
    "\tif 'z_vals_hierarchical' in outputs:\n",
    "\t\tz_vals_hierarch = outputs['z_vals_hierarchical'].view((-1, n_samples_hierarchical))\n",
    "\t\tz_sample_hierarch = z_vals_hierarch[z_vals_hierarch.shape[0] // 2].detach().cpu().numpy()\n",
    "\telse:\n",
    "\t\tz_sample_hierarch = None\n",
    "\t_ = plot_samples(z_sample_strat, z_sample_hierarch, ax=ax[3])\n",
    "\tax[3].margins(0)\n",
    "\tplt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME BUNDLE TEST\n",
    "height, width = images.shape[1:3]\n",
    "all_rays = torch.stack([torch.stack(get_rays(height, width, focal, p), 0)\n",
    "\t\tfor p in poses[:n_training]], 0) # [N, ro+rd, H, W, 3]\n",
    "rays_rgb = torch.cat([all_rays, images[:, None]], 1) # [N, ro+rd+rgb, H, W, 3]\n",
    "rays_rgb = torch.permute(rays_rgb, [0, 2, 3, 1, 4]) # [N, H, W, ro+rd+rgb, 3]\n",
    "rays_rgb = rays_rgb.reshape([-1, 3, 3]) # [(N-1)*H*W, ro+rd+rgb, 3]\n",
    "# rays_rgb = rays_rgb.type(torch.float32)\n",
    "print(rays_rgb.shape, all_rays.shape)\n",
    "\n",
    "# One image is 10000 points\n",
    "A,_,C,D,_ = all_rays.shape\n",
    "time_bundle_shape = (A,C,D,1)\n",
    "time_bundle = torch.zeros(time_bundle_shape)\n",
    "for i,_t in enumerate(times):\n",
    "    time_bundle[i] += _t # fill time bundle \n",
    "time_bundle = time_bundle.reshape(-1, 1) \n",
    "time_bundle.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "\tr\"\"\"\n",
    "\tLaunch training session for NeRF.\n",
    "\t\"\"\"\n",
    "\t# Shuffle rays across all images.\n",
    "\ti_batch = 0\n",
    "\trays_rgb = None\n",
    "\ttime_bundle = None\n",
    "\tif not one_image_per_step:\n",
    "\t\theight, width = images.shape[1:3]\n",
    "\t\tall_rays = torch.stack([torch.stack(get_rays(height, width, focal, p), 0)\n",
    "\t\t\t\tfor p in poses[:n_training]], 0) # [N, ro+rd, H, W, 3]\n",
    "\t\trays_rgb = torch.cat([all_rays, images[:, None]], 1) # [N, ro+rd+rgb, H, W, 3]\n",
    "\t\trays_rgb = torch.permute(rays_rgb, [0, 2, 3, 1, 4]) # [N, H, W, ro+rd+rgb, 3]\n",
    "\t\trays_rgb = rays_rgb.reshape([-1, 3, 3]) # [(N-1)*H*W, ro+rd+rgb, 3]\n",
    "\t\trays_rgb = rays_rgb.type(torch.float32)\n",
    "\t\tif shuffle_data:\n",
    "\t\t\trand_indexes = torch.randperm(rays_rgb.shape[0])\n",
    "\t\t\trays_rgb = rays_rgb[rand_indexes]\n",
    "\n",
    "\t\t# Create batch expended time. \n",
    "\t\t# Each ray has time, corresponede to image timestep \n",
    "\t\tA,_,C,D,_ = all_rays.shape # [N, _, H, W, _]\n",
    "\t\ttime_bundle_shape = (A,C,D,1) # [N, H, W, 1]\n",
    "\t\ttime_bundle = torch.zeros(time_bundle_shape)\n",
    "\t\tfor i,_t in enumerate(times):\n",
    "\t\t\ttime_bundle[i] += _t # fill time bundle \n",
    "\t\ttime_bundle = time_bundle.reshape(-1, 1) # [N*H*W, 1]\n",
    "\t\tif shuffle_data:\n",
    "\t\t\t# same shuffle that we use for rays!\n",
    "\t\t\ttime_bundle = time_bundle[rand_indexes] \n",
    "\t\t\n",
    "\ttrain_psnrs = []\n",
    "\tval_psnrs = []\n",
    "\titernums = []\n",
    "\n",
    "\tfor i in trange(n_iters):\n",
    "\t\t\n",
    "\t\tpsnr, i_batch = train_one_time(i, i_batch, rays_rgb, time_bundle)\n",
    "\t\ttrain_psnrs.append(psnr)\n",
    "\n",
    "\t\t# Evaluate testimg at given display rate.\n",
    "\t\tif i % display_rate == 0:\n",
    "\t\t\teval_one_time(i, iternums, train_psnrs, val_psnrs)\n",
    "\n",
    "\t\t# Check PSNR for issues and stop if any are found.\n",
    "\t\tval_psnr = val_psnrs[-1]\n",
    "\t\tif i == warmup_iters - 1:\n",
    "\t\t\tif val_psnr < warmup_min_fitness:\n",
    "\t\t\t\tprint(f'Val PSNR {val_psnr} below warmup_min_fitness {warmup_min_fitness}. Stopping...')\n",
    "\t\t\t\treturn False, train_psnrs, val_psnrs\n",
    "\t\telif i < warmup_iters:\n",
    "\t\t\tif warmup_stopper is not None and warmup_stopper(i, psnr):\n",
    "\t\t\t\tprint(f'Train PSNR flatlined at {psnr} for {warmup_stopper.patience} iters. Stopping...')\n",
    "\t\t\t\treturn False, train_psnrs, val_psnrs\n",
    "\t\n",
    "\t\t# break # TODO:REMOVE\n",
    "\n",
    "\tprint('val_psnrs',val_psnrs)\n",
    "\treturn True, train_psnrs, val_psnrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training session(s)\n",
    "for _ in range(n_restarts):\n",
    "\tmodel, fine_model, encode, encode_viewdirs, encode_time, optimizer, warmup_stopper, scheduler=init_models()\n",
    "\tsuccess, train_psnrs, val_psnrs = train()\n",
    "\tif success and val_psnrs[-1] >= warmup_min_fitness:\n",
    "\t\tprint('Training successful!')\n",
    "\t\tbreak\n",
    "\t# break # TODO:REMOVE\n",
    "\n",
    "print('')\n",
    "print(f'Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'd-nerf.pt')\n",
    "torch.save(fine_model.state_dict(), 'd-nerf-fine.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnerf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
